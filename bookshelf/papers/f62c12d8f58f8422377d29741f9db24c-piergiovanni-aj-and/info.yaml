abstract: In this paper, we newly introduce the concept of temporal attention filters,
  and describe how they can be used for human activity recognition from videos. Many
  high-level activities are often composed of multiple temporal parts (e.g., sub-events)
  with different duration/speed, and our objective is to make the model explicitly
  learn such temporal structure using multiple attention filters and benefit from
  them. Our temporal filters are designed to be fully differentiable, allowing end-of-end
  training of the temporal filters together with the underlying frame-based or segment-based
  convolutional neural network architectures. This paper presents an approach of learning
  a set of optimal static temporal attention filters to be shared across different
  videos, and extends this approach to dynamically adjust attention filters per testing
  video using recurrent long short-term memory networks (LSTMs). This allows our temporal
  attention filters to learn latent sub-events specific to each activity. We experimentally
  confirm that the proposed concept of temporal attention filters benefits the activity
  recognition, and we visualize the learned latent sub-events.
archiveprefix: arXiv
author: Piergiovanni, AJ and Fan, Chenyou and Ryoo, Michael S.
author_list:
- family: Piergiovanni
  given: AJ
- family: Fan
  given: Chenyou
- family: Ryoo
  given: Michael S.
eprint: 1605.08140v3
file: 1605.08140v3.pdf
files:
- piergiovanni-aj-and-fan-chenyou-and-ryoo-michael-s.learning-latent-sub-events-in-activity-videos-using-temporal-attention-filters2016.pdf
month: May
note: AAAI 2017
primaryclass: cs.CV
ref: 1605.08140v3
time-added: 2020-05-26-16:46:37
title: Learning Latent Sub-events in Activity Videos Using Temporal Attention   Filters
type: article
url: http://arxiv.org/abs/1605.08140v3
year: '2016'

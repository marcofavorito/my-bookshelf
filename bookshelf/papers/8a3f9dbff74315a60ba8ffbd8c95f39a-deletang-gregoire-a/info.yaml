abstract: We extend temporal-difference (TD) learning in order to obtain risk-sensitive,
  model-free reinforcement learning algorithms. This extension can be regarded as
  modification of the Rescorla-Wagner rule, where the (sigmoidal) stimulus is taken
  to be either the event of over- or underestimating the TD target. As a result, one
  obtains a stochastic approximation rule for estimating the free energy from i.i.d.
  samples generated by a Gaussian distribution with unknown mean and variance. Since
  the Gaussian free energy is known to be a certainty-equivalent sensitive to the
  mean and the variance, the learning rule has applications in risk-sensitive decision-making.
archiveprefix: arXiv
author: Delétang, Grégoire and Grau-Moya, Jordi and Kunesch, Markus and Genewein,
  Tim and Brekelmans, Rob and Legg, Shane and Ortega, Pedro A.
author_list:
- family: Delétang
  given: Grégoire
- family: Grau-Moya
  given: Jordi
- family: Kunesch
  given: Markus
- family: Genewein
  given: Tim
- family: Brekelmans
  given: Rob
- family: Legg
  given: Shane
- family: Ortega
  given: Pedro A.
eprint: 2111.02907v1
file: 2111.02907v1.pdf
files:
- deletang-gregoire-and-grau-moya-jordi-and-kunesch-markus-and-genewein-tim-and-brekelmans-rob-and-legg-shane-and-ortega-pedro-a.model-free-risk.pdf
month: Nov
primaryclass: cs.LG
ref: 2111.02907v1
time-added: 2021-11-11-16:32:27
title: Model-Free Risk-Sensitive Reinforcement Learning
type: article
url: http://arxiv.org/abs/2111.02907v1
year: '2021'

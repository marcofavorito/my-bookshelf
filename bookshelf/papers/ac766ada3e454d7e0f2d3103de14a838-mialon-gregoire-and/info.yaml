abstract: 'We introduce GAIA, a benchmark for General AI Assistants that, if solved,
  would represent a milestone in AI research. GAIA proposes real-world questions that
  require a set of fundamental abilities such as reasoning, multi-modality handling,
  web browsing, and generally tool-use proficiency. GAIA questions are conceptually
  simple for humans yet challenging for most advanced AIs: we show that human respondents
  obtain 92\% vs. 15\% for GPT-4 equipped with plugins. This notable performance disparity
  contrasts with the recent trend of LLMs outperforming humans on tasks requiring
  professional skills in e.g. law or chemistry. GAIA''s philosophy departs from the
  current trend in AI benchmarks suggesting to target tasks that are ever more difficult
  for humans. We posit that the advent of Artificial General Intelligence (AGI) hinges
  on a system''s capability to exhibit similar robustness as the average human does
  on such questions. Using GAIA''s methodology, we devise 466 questions and their
  answer. We release our questions while retaining answers to 300 of them to power
  a leader-board available at https://huggingface.co/gaia-benchmark.'
archiveprefix: arXiv
author: Mialon, Grégoire and Fourrier, Clémentine and Swift, Craig and Wolf, Thomas
  and LeCun, Yann and Scialom, Thomas
author_list:
- family: Mialon
  given: Grégoire
- family: Fourrier
  given: Clémentine
- family: Swift
  given: Craig
- family: Wolf
  given: Thomas
- family: LeCun
  given: Yann
- family: Scialom
  given: Thomas
eprint: 2311.12983v1
file: 2311.12983v1.pdf
files:
- mialon-gregoire-and-fourrier-clementine-and-swift-craig-and-wolf-thomas-and-lecun-yann-and-scialom-thomasgaia-a-benchmark-for-general-ai-assist.pdf
month: Nov
primaryclass: cs.CL
ref: 2311.12983v1
time-added: 2023-11-26-20:37:51
title: 'GAIA: a benchmark for General AI Assistants'
type: article
url: http://arxiv.org/abs/2311.12983v1
year: '2023'

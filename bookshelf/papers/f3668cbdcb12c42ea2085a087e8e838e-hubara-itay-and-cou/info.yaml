abstract: We introduce a method to train Quantized Neural Networks (QNNs) --- neural
  networks with extremely low precision (e.g., 1-bit) weights and activations, at
  run-time. At train-time the quantized weights and activations are used for computing
  the parameter gradients. During the forward pass, QNNs drastically reduce memory
  size and accesses, and replace most arithmetic operations with bit-wise operations.
  As a result, power consumption is expected to be drastically reduced. We trained
  QNNs over the MNIST, CIFAR-10, SVHN and ImageNet datasets. The resulting QNNs achieve
  prediction accuracy comparable to their 32-bit counterparts. For example, our quantized
  version of AlexNet with 1-bit weights and 2-bit activations achieves $51\%$ top-1
  accuracy. Moreover, we quantize the parameter gradients to 6-bits as well which
  enables gradients computation using only bit-wise operation. Quantized recurrent
  neural networks were tested over the Penn Treebank dataset, and achieved comparable
  accuracy as their 32-bit counterparts using only 4-bits. Last but not least, we
  programmed a binary matrix multiplication GPU kernel with which it is possible to
  run our MNIST QNN 7 times faster than with an unoptimized GPU kernel, without suffering
  any loss in classification accuracy. The QNN code is available online.
archiveprefix: arXiv
author: Hubara, Itay and Courbariaux, Matthieu and Soudry, Daniel and El-Yaniv, Ran
  and Bengio, Yoshua
author_list:
- family: Hubara
  given: Itay
- family: Courbariaux
  given: Matthieu
- family: Soudry
  given: Daniel
- family: El-Yaniv
  given: Ran
- family: Bengio
  given: Yoshua
eprint: 1609.07061v1
file: 1609.07061v1.pdf
files:
- hubara-itay-and-courbariaux-matthieu-and-soudry-daniel-and-el-yaniv-ran-and-bengio-yoshuaquantized-neural-networks-training-neural-networks-with.pdf
month: Sep
primaryclass: cs.NE
ref: 1609.07061v1
time-added: 2021-02-11-15:46:01
title: 'Quantized Neural Networks: Training Neural Networks with Low Precision   Weights
  and Activations'
type: article
url: http://arxiv.org/abs/1609.07061v1
year: '2016'

abstract: 'Contrastive models like CLIP have been shown to learn robust representations
  of images that capture both semantics and style. To leverage these representations
  for image generation, we propose a two-stage model: a prior that generates a CLIP
  image embedding given a text caption, and a decoder that generates an image conditioned
  on the image embedding. We show that explicitly generating image representations
  improves image diversity with minimal loss in photorealism and caption similarity.
  Our decoders conditioned on image representations can also produce variations of
  an image that preserve both its semantics and style, while varying the non-essential
  details absent from the image representation. Moreover, the joint embedding space
  of CLIP enables language-guided image manipulations in a zero-shot fashion. We use
  diffusion models for the decoder and experiment with both autoregressive and diffusion
  models for the prior, finding that the latter are computationally more efficient
  and produce higher-quality samples.'
archiveprefix: arXiv
author: Ramesh, Aditya and Dhariwal, Prafulla and Nichol, Alex and Chu, Casey and
  Chen, Mark
author_list:
- family: Ramesh
  given: Aditya
- family: Dhariwal
  given: Prafulla
- family: Nichol
  given: Alex
- family: Chu
  given: Casey
- family: Chen
  given: Mark
eprint: 2204.06125v1
file: 2204.06125v1.pdf
files:
- ramesh-aditya-and-dhariwal-prafulla-and-nichol-alex-and-chu-casey-and-chen-markhierarchical-text-conditional-image-generation-with-clip-latents20.pdf
month: Apr
primaryclass: cs.CV
ref: 2204.06125v1
time-added: 2023-04-01-22:45:04
title: Hierarchical Text-Conditional Image Generation with CLIP Latents
type: article
url: http://arxiv.org/abs/2204.06125v1
year: '2022'

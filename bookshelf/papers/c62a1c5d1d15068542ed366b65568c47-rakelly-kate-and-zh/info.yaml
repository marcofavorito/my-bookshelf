abstract: Deep reinforcement learning algorithms require large amounts of experience
  to learn an individual task. While in principle meta-reinforcement learning (meta-RL)
  algorithms enable agents to learn new skills from small amounts of experience, several
  major challenges preclude their practicality. Current methods rely heavily on on-policy
  experience, limiting their sample efficiency. The also lack mechanisms to reason
  about task uncertainty when adapting to new tasks, limiting their effectiveness
  in sparse reward problems. In this paper, we address these challenges by developing
  an off-policy meta-RL algorithm that disentangles task inference and control. In
  our approach, we perform online probabilistic filtering of latent task variables
  to infer how to solve a new task from small amounts of experience. This probabilistic
  interpretation enables posterior sampling for structured and efficient exploration.
  We demonstrate how to integrate these task variables with off-policy RL algorithms
  to achieve both meta-training and adaptation efficiency. Our method outperforms
  prior algorithms in sample efficiency by 20-100X as well as in asymptotic performance
  on several meta-RL benchmarks.
archiveprefix: arXiv
author: Rakelly, Kate and Zhou, Aurick and Quillen, Deirdre and Finn, Chelsea and
  Levine, Sergey
author_list:
- family: Rakelly
  given: Kate
- family: Zhou
  given: Aurick
- family: Quillen
  given: Deirdre
- family: Finn
  given: Chelsea
- family: Levine
  given: Sergey
eprint: 1903.08254v1
file: 1903.08254v1.pdf
files:
- rakelly-kate-and-zhou-aurick-and-quillen-deirdre-and-finn-chelsea-and-levine-sergeyefficient-off-policy-meta-reinforcement-learning-via-probabili.pdf
month: Mar
primaryclass: cs.LG
ref: 1903.08254v1
time-added: 2022-05-03-11:09:29
title: Efficient Off-Policy Meta-Reinforcement Learning via Probabilistic   Context
  Variables
type: article
url: http://arxiv.org/abs/1903.08254v1
year: '2019'

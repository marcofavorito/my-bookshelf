abstract: We show that for several variations of partially observable Markov decision
  processes, polynomial-time algorithms for finding control policies are unlikely
  to or simply don't have guarantees of finding policies within a constant factor
  or a constant summand of optimal. Here "unlikely" means "unless some complexity
  classes collapse," where the collapses considered are P=NP, P=PSPACE, or P=EXP.
  Until or unless these collapses are shown to hold, any control-policy designer must
  choose between such performance guarantees and efficient computation.
archiveprefix: arXiv
author: Goldsmith, J. and Lusena, C. and Mundhenk, M.
author_list:
- family: Goldsmith
  given: J.
- family: Lusena
  given: C.
- family: Mundhenk
  given: M.
doi: 10.1613/jair.714
eprint: 1106.0242v1
file: 1106.0242v1.pdf
files:
- goldsmith-j.-and-lusena-c.-and-mundhenk-m.nonapproximability-results-for-partially-observable-markov-decision-processes2011.pdf
month: Jun
note: Journal Of Artificial Intelligence Research, Volume 14, pages   83-103, 2001
primaryclass: cs.AI
ref: 1106.0242v1
time-added: 2021-03-07-09:55:08
title: Nonapproximability Results for Partially Observable Markov Decision   Processes
type: article
url: http://arxiv.org/abs/1106.0242v1
year: '2011'

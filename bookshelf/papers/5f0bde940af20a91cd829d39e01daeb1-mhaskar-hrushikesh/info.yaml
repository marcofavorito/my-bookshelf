abstract: While the universal approximation property holds both for hierarchical and
  shallow networks, we prove that deep (hierarchical) networks can approximate the
  class of compositional functions with the same accuracy as shallow networks but
  with exponentially lower number of training parameters as well as VC-dimension.
  This theorem settles an old conjecture by Bengio on the role of depth in networks.
  We then define a general class of scalable, shift-invariant algorithms to show a
  simple and natural set of requirements that justify deep convolutional networks.
archiveprefix: arXiv
author: Mhaskar, Hrushikesh and Liao, Qianli and Poggio, Tomaso
author_list:
- family: Mhaskar
  given: Hrushikesh
- family: Liao
  given: Qianli
- family: Poggio
  given: Tomaso
eprint: 1603.00988v4
file: 1603.00988v4.pdf
files:
- mhaskar-hrushikesh-and-liao-qianli-and-poggio-tomasolearning-functions-when-is-deep-better-than-shallow2016.pdf
month: Mar
primaryclass: cs.LG
ref: 1603.00988v4
time-added: 2021-01-21-19:39:41
title: 'Learning Functions: When Is Deep Better Than Shallow'
type: article
url: http://arxiv.org/abs/1603.00988v4
year: '2016'

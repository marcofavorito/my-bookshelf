abstract: 'Instruction tuning large language models (LLMs) using machine-generated
  instruction-following data has improved zero-shot capabilities on new tasks, but
  the idea is less explored in the multimodal field. In this paper, we present the
  first attempt to use language-only GPT-4 to generate multimodal language-image instruction-following
  data. By instruction tuning on such generated data, we introduce LLaVA: Large Language
  and Vision Assistant, an end-to-end trained large multimodal model that connects
  a vision encoder and LLM for general-purpose visual and language understanding.Our
  early experiments show that LLaVA demonstrates impressive multimodel chat abilities,
  sometimes exhibiting the behaviors of multimodal GPT-4 on unseen images/instructions,
  and yields a 85.1% relative score compared with GPT-4 on a synthetic multimodal
  instruction-following dataset. When fine-tuned on Science QA, the synergy of LLaVA
  and GPT-4 achieves a new state-of-the-art accuracy of 92.53%. We make GPT-4 generated
  visual instruction tuning data, our model and code base publicly available.'
archiveprefix: arXiv
author: Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae
author_list:
- family: Liu
  given: Haotian
- family: Li
  given: Chunyuan
- family: Wu
  given: Qingyang
- family: Lee
  given: Yong Jae
eprint: 2304.08485v1
file: 2304.08485v1.pdf
files:
- liu-haotian-and-li-chunyuan-and-wu-qingyang-and-lee-yong-jaevisual-instruction-tuning2023.pdf
month: Apr
primaryclass: cs.CV
ref: 2304.08485v1
time-added: 2023-10-09-15:29:49
title: Visual Instruction Tuning
type: article
url: http://arxiv.org/abs/2304.08485v1
year: '2023'

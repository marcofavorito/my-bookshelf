abstract: Unifying seemingly disparate algorithmic ideas to produce better performing
  algorithms has been a longstanding goal in reinforcement learning. As a primary
  example, TD($\lambda$) elegantly unifies one-step TD prediction with Monte Carlo
  methods through the use of eligibility traces and the trace-decay parameter $\lambda$.
  Currently, there are a multitude of algorithms that can be used to perform TD control,
  including Sarsa, $Q$-learning, and Expected Sarsa. These methods are often studied
  in the one-step case, but they can be extended across multiple time steps to achieve
  better performance. Each of these algorithms is seemingly distinct, and no one dominates
  the others for all problems. In this paper, we study a new multi-step action-value
  algorithm called $Q(\sigma)$ which unifies and generalizes these existing algorithms,
  while subsuming them as special cases. A new parameter, $\sigma$, is introduced
  to allow the degree of sampling performed by the algorithm at each step during its
  backup to be continuously varied, with Sarsa existing at one extreme (full sampling),
  and Expected Sarsa existing at the other (pure expectation). $Q(\sigma)$ is generally
  applicable to both on- and off-policy learning, but in this work we focus on experiments
  in the on-policy case. Our results show that an intermediate value of $\sigma$,
  which results in a mixture of the existing algorithms, performs better than either
  extreme. The mixture can also be varied dynamically which can result in even greater
  performance.
archiveprefix: arXiv
author: Asis, Kristopher De and Hernandez-Garcia, J. Fernando and Holland, G. Zacharias
  and Sutton, Richard S.
author_list:
- family: Asis
  given: Kristopher De
- family: Hernandez-Garcia
  given: J. Fernando
- family: Holland
  given: G. Zacharias
- family: Sutton
  given: Richard S.
eprint: 1703.01327v2
file: 1703.01327v2.pdf
files:
- asis-kristopher-de-and-hernandez-garcia-j.-fernando-and-holland-g.-zacharias-and-sutton-richard-s.multi-step-reinforcement-learning-a-unifying-al.pdf
month: Mar
note: (2018). In AAAI Conference on Artificial Intelligence.   https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16294
primaryclass: cs.AI
ref: 1703.01327v2
time-added: 2022-05-21-17:03:31
title: 'Multi-step Reinforcement Learning: A Unifying Algorithm'
type: article
url: http://arxiv.org/abs/1703.01327v2
year: '2017'

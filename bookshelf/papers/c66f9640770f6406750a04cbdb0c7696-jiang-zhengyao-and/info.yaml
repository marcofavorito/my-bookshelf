abstract: Deep reinforcement learning (DRL) has achieved significant breakthroughs
  in various tasks. However, most DRL algorithms suffer a problem of generalizing
  the learned policy which makes the learning performance largely affected even by
  minor modifications of the training environment. Except that, the use of deep neural
  networks makes the learned policies hard to be interpretable. To address these two
  challenges, we propose a novel algorithm named Neural Logic Reinforcement Learning
  (NLRL) to represent the policies in reinforcement learning by first-order logic.
  NLRL is based on policy gradient methods and differentiable inductive logic programming
  that have demonstrated significant advantages in terms of interpretability and generalisability
  in supervised tasks. Extensive experiments conducted on cliff-walking and blocks
  manipulation tasks demonstrate that NLRL can induce interpretable policies achieving
  near-optimal performance while demonstrating good generalisability to environments
  of different initial states and problem sizes.
archiveprefix: arXiv
author: Jiang, Zhengyao and Luo, Shan
author_list:
- family: Jiang
  given: Zhengyao
- family: Luo
  given: Shan
eprint: 1904.10729v2
file: 1904.10729v2.pdf
files:
- jiang-zhengyao-and-luo-shanneural-logic-reinforcement-learning2019.pdf
month: Apr
primaryclass: cs.LG
ref: 1904.10729v2
time-added: 2020-07-13-00:04:13
title: Neural Logic Reinforcement Learning
type: article
url: http://arxiv.org/abs/1904.10729v2
year: '2019'

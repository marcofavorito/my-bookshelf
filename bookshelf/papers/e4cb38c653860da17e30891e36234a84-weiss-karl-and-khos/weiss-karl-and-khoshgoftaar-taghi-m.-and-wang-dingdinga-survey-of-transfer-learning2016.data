

<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="citation_abstract" content="Machine learning and data mining techniques have been used in numerous real-world applications. An assumption of traditional machine learning methodologies is the training data and testing data are taken from the same domain, such that the input feature space and data distribution characteristics are the same. However, in some real-world machine learning scenarios, this assumption does not hold. There are cases where training data is expensive or difficult to collect. Therefore, there is a need to create high-performance learners trained with more easily obtained data from different domains. This methodology is referred to as transfer learning. This survey paper formally defines transfer learning, presents information on current solutions, and reviews applications applied to transfer learning. Lastly, there is information listed on software downloads for various transfer learning solutions and a discussion of possible future research work. The transfer learning solutions surveyed are independent of data size and can be applied to big data environments."/>

    <meta name="journal_id" content="40537"/>

    <meta name="dc.title" content="A survey of transfer learning"/>

    <meta name="dc.source" content="Journal of Big Data 2016 3:1"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="SpringerOpen"/>

    <meta name="dc.date" content="2016-05-28"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2016 The Author(s)"/>

    <meta name="dc.rightsAgent" content="reprints@biomedcentral.com"/>

    <meta name="dc.description" content="Machine learning and data mining techniques have been used in numerous real-world applications. An assumption of traditional machine learning methodologies is the training data and testing data are taken from the same domain, such that the input feature space and data distribution characteristics are the same. However, in some real-world machine learning scenarios, this assumption does not hold. There are cases where training data is expensive or difficult to collect. Therefore, there is a need to create high-performance learners trained with more easily obtained data from different domains. This methodology is referred to as transfer learning. This survey paper formally defines transfer learning, presents information on current solutions, and reviews applications applied to transfer learning. Lastly, there is information listed on software downloads for various transfer learning solutions and a discussion of possible future research work. The transfer learning solutions surveyed are independent of data size and can be applied to big data environments."/>

    <meta name="prism.issn" content="2196-1115"/>

    <meta name="prism.publicationName" content="Journal of Big Data"/>

    <meta name="prism.publicationDate" content="2016-05-28"/>

    <meta name="prism.volume" content="3"/>

    <meta name="prism.number" content="1"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="1"/>

    <meta name="prism.endingPage" content="40"/>

    <meta name="prism.copyright" content="2016 The Author(s)"/>

    <meta name="prism.rightsAgent" content="reprints@biomedcentral.com"/>

    <meta name="prism.url" content="https://link.springer.com/articles/10.1186/s40537-016-0043-6"/>

    <meta name="prism.doi" content="doi:10.1186/s40537-016-0043-6"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/track/pdf/10.1186/s40537-016-0043-6"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/articles/10.1186/s40537-016-0043-6"/>

    <meta name="citation_journal_title" content="Journal of Big Data"/>

    <meta name="citation_journal_abbrev" content="J Big Data"/>

    <meta name="citation_publisher" content="SpringerOpen"/>

    <meta name="citation_issn" content="2196-1115"/>

    <meta name="citation_title" content="A survey of transfer learning"/>

    <meta name="citation_volume" content="3"/>

    <meta name="citation_issue" content="1"/>

    <meta name="citation_publication_date" content="2016/12"/>

    <meta name="citation_online_date" content="2016/05/28"/>

    <meta name="citation_firstpage" content="1"/>

    <meta name="citation_lastpage" content="40"/>

    <meta name="citation_article_type" content="Survey paper"/>

    <meta name="citation_fulltext_world_readable" content=""/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1186/s40537-016-0043-6"/>

    <meta name="DOI" content="10.1186/s40537-016-0043-6"/>

    <meta name="citation_doi" content="10.1186/s40537-016-0043-6"/>

    <meta name="description" content="Machine learning and data mining techniques have been used in numerous real-world applications. An assumption of traditional machine learning methodologies is the training data and testing data are taken from the same domain, such that the input feature space and data distribution characteristics are the same. However, in some real-world machine learning scenarios, this assumption does not hold. There are cases where training data is expensive or difficult to collect. Therefore, there is a need to create high-performance learners trained with more easily obtained data from different domains. This methodology is referred to as transfer learning. This survey paper formally defines transfer learning, presents information on current solutions, and reviews applications applied to transfer learning. Lastly, there is information listed on software downloads for various transfer learning solutions and a discussion of possible future research work. The transfer learning solutions surveyed are independent of data size and can be applied to big data environments."/>

    <meta name="dc.creator" content="Karl Weiss"/>

    <meta name="dc.creator" content="Taghi M. Khoshgoftaar"/>

    <meta name="dc.creator" content="DingDing Wang"/>

    <meta name="dc.subject" content="Database Management"/>

    <meta name="dc.subject" content="Information Storage and Retrieval"/>

    <meta name="dc.subject" content="Data Mining and Knowledge Discovery"/>

    <meta name="dc.subject" content="Computational Science and Engineering"/>

    <meta name="dc.subject" content="Mathematical Applications in Computer Science"/>

    <meta name="dc.subject" content="Communications Engineering, Networks"/>

    <meta name="citation_reference" content="A literature survey on domain adaptation of statistical classifiers. 
                    http://sifaka.cs.uiuc.edu/jiang4/domain_adaptation/survey/da_survey.html
                    
                  . Accessed 4 Mar 2016."/>

    <meta name="citation_reference" content="citation_journal_title=J Mach Learn Res; citation_title=A framework for learning predictive structures from multiple tasks and unlabeled data; citation_author=RK Ando, T Zhang; citation_volume=6; citation_publication_date=2005; citation_pages=1817-1853; citation_id=CR2"/>

    <meta name="citation_reference" content="citation_journal_title=Comput Vis Image Underst; citation_title=Surf: speeded up robust features; citation_author=H Bay, T Tuytelaars, LV Gool; citation_volume=110; citation_issue=3; citation_publication_date=2006; citation_pages=346-359; citation_doi=10.1016/j.cviu.2007.09.014; citation_id=CR3"/>

    <meta name="citation_reference" content="citation_journal_title=J Mach Learn Res Arch; citation_title=Manifold regularization: a geometric framework for learning from examples; citation_author=M Belkin, P Niyogi, V Sindhwani; citation_volume=7; citation_publication_date=2006; citation_pages=2399-2434; citation_id=CR4"/>

    <meta name="citation_reference" content="Blitzer, J, McDonald R, Pereira F. Domain adaptation with structural correspondence learning. In: Proceedings of the 2006 conference on empirical methods in natural language processing. 2006;120&#8211;8."/>

    <meta name="citation_reference" content="BoChen90 Update TrAdaBoost.m. 
                    https://github.com/BoChen90/machine-learning-matlab/blob/master/TrAdaBoost.m
                    
                  . Accessed 4 Mar 2016."/>

    <meta name="citation_reference" content="Bolt online learning toolbox. 
                    http://pprett.github.com/bolt/
                    
                  . Accessed 4 Mar 2016."/>

    <meta name="citation_reference" content="Bonilla E, Chai KM, Williams C. Multi-task Gaussian process prediction. In: Proceedings of the 20th annual conference of neural information processing systems. 2008. 153&#8211;60."/>

    <meta name="citation_reference" content="Gong B. 
                    http://www-scf.usc.edu/~boqinggo/
                    
                  . Accessed 4 Mar 2016."/>

    <meta name="citation_reference" content="citation_journal_title=Bioinformatics; citation_title=Integrating structured biological data by kernel maximum mean discrepancy; citation_author=KM Borgwardt, A Gretton, MJ Rasch, HP Kriegel, B Sch&#246;lkopf, AJ Smola; citation_volume=22; citation_issue=4; citation_publication_date=2006; citation_pages=49-57; citation_doi=10.1093/bioinformatics/btl242; citation_id=CR10"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Pattern Anal Mach Intell; citation_title=Domain adaptation problems: a DASVM classification technique and a circular validation strategy; citation_author=L Bruzzone, M Marconcini; citation_volume=32; citation_issue=5; citation_publication_date=2010; citation_pages=770-787; citation_doi=10.1109/TPAMI.2009.57; citation_id=CR11"/>

    <meta name="citation_reference" content="Cao B, Liu N, Yang Q. Transfer learning for collective link prediction in multiple heterogeneous domains. In: Proceedings of the 27th international conference on machine learning. 2010. p. 159&#8211;66."/>

    <meta name="citation_reference" content="Cawley G. Leave-one-out cross-validation based model selection criteria for weighted LS-SVMs. In: IEEE 2006 international joint conference on neural network proceedings 2006. p. 1661&#8211;68."/>

    <meta name="citation_reference" content="Chattopadhyay R, Ye J, Panchanathan S, Fan W, Davidson I. Multi-source domain adaptation and its application to early detection of fatigue. ACM Trans Knowl Dis Data (Best of SIGKDD 2011 TKDD Homepage archive) 2011; 6(4) (Article 18)."/>

    <meta name="citation_reference" content="citation_journal_title=Comput Speech Lang; citation_title=Adaptation of maximum entropy classifier: little data can help a lot; citation_author=C Chelba, A Acero; citation_volume=20; citation_issue=4; citation_publication_date=2004; citation_pages=382-399; citation_doi=10.1016/j.csl.2005.05.005; citation_id=CR15"/>

    <meta name="citation_reference" content="Chen M, Xu ZE, Weinberger KQ, Sha F (2012) Marginalized denoising autoencoders for domain adaptation. ICML. arXiv preprintarXiv:1206.4683."/>

    <meta name="citation_reference" content="Chung FRK. Spectral graph theory. In: CBMS regional conference series in mathematics, no. 92. Providence: American Mathematical Society; 1994."/>

    <meta name="citation_reference" content="Computer Vision and Learning Group. 
                    http://vision.cs.uml.edu/adaptation.html
                    
                  . Accessed 4 Mar 2016."/>

    <meta name="citation_reference" content="citation_journal_title=Knowl Inf Syst; citation_title=Transfer learning for activity recognition: a survey; citation_author=DJ Cook, KD Feuz, NC Krishnan; citation_volume=36; citation_issue=3; citation_publication_date=2012; citation_pages=537-556; citation_doi=10.1007/s10115-013-0665-3; citation_id=CR19"/>

    <meta name="citation_reference" content="citation_journal_title=Adv Neural Inform Process Syst; citation_title=Translated learning: transfer learning across different feature spaces; citation_author=W Dai, Y Chen, GR Xue, Q Yang, Y Yu; citation_volume=21; citation_publication_date=2008; citation_pages=353-360; citation_id=CR20"/>

    <meta name="citation_reference" content="Dai W, Yang Q, Xue GR, Yu Y (2007) Boosting for transfer learning. In: Proceedings of the 24th international conference on machine learning. p. 193&#8211;200."/>

    <meta name="citation_reference" content="Daum&#233; H III. Frustratingly easy domain adaptation. In: Proceedings of ACL. 2007. p. 256&#8211;63."/>

    <meta name="citation_reference" content="Davis J, Kulis B, Jain P, Sra S, Dhillon I. Information theoretic metric learning. In: Proceedings of the 24th international conference on machine learning. 2007. p. 209&#8211;16."/>

    <meta name="citation_reference" content="citation_journal_title=J Am Soc Inf Sci; citation_title=Indexing by latent semantic analysis; citation_author=S Deerwester, ST Dumais, GW Furnas, TK Landauer, R Harshman; citation_volume=41; citation_publication_date=1990; citation_pages=391-407; citation_doi=10.1002/(SICI)1097-4571(199009)41:6&lt;391::AID-ASI1&gt;3.0.CO;2-9; citation_id=CR24"/>

    <meta name="citation_reference" content="Deng J, Zhang Z, Marchi E, Schuller B. Sparse autoencoder based feature transfer learning for speech emotion recognition. In: Humaine association conference on affective computing and intelligent interaction. 2013. p. 511&#8211;6."/>

    <meta name="citation_reference" content="Domain adaptation project. 
                    https://www.eecs.berkeley.edu/~jhoffman/domainadapt/
                    
                  . Accessed 4 Mar 2016."/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Pattern Anal Mach Intell; citation_title=Domain transfer multiple kernel learning; citation_author=L Duan, IW Tsang, D Xu; citation_volume=34; citation_issue=3; citation_publication_date=2012; citation_pages=465-479; citation_doi=10.1109/TPAMI.2011.114; citation_id=CR27"/>

    <meta name="citation_reference" content="Duan L, Xu D, Chang SF. Exploiting web images for event recognition in consumer videos: a multiple source domain adaptation approach. In: IEEE 2012 conference on computer vision and pattern recognition. 2012. p. 1338&#8211;45."/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Neural Netw Learn Syst; citation_title=Domain adaptation from multiple sources: a domain-dependent regularization approach; citation_author=L Duan, D Xu, IW Tsang; citation_volume=23; citation_issue=3; citation_publication_date=2012; citation_pages=504-518; citation_doi=10.1109/TNNLS.2011.2178556; citation_id=CR29"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Pattern Anal Mach Intell; citation_title=Learning with augmented features for heterogeneous domain adaptation; citation_author=L Duan, D Xu, IW Tsang; citation_volume=36; citation_issue=6; citation_publication_date=2012; citation_pages=1134-1148; citation_id=CR30"/>

    <meta name="citation_reference" content="citation_journal_title=Proc Mach Learn Knowl Disc Database; citation_title=Modeling transfer relationships between learning tasks for improved inductive transfer; citation_author=E Eaton, M des Jardins, T Lane; citation_volume=5211; citation_publication_date=2008; citation_pages=317-332; citation_doi=10.1007/978-3-540-87479-9_39; citation_id=CR31"/>

    <meta name="citation_reference" content="EasyAdapt.pl.gz (Download). 
                    http://hal3.name/easyadapt.pl.gz
                    
                   Accessed 4 Mar 2016."/>

    <meta name="citation_reference" content="Evgeniou T, Pontil M (2004) Regularized multi-task learning. In: Proceedings of the 10th ACM SIGKDD international conference on knowledge discovery and data mining. p. 109&#8211;17."/>

    <meta name="citation_reference" content="Exploiting web images for event recognition in consumer videos: a multiple source domain adaptation approach. 
                    http://lxduan.info/papers/DuanCVPR2012_poster.pdf
                    
                  . Accessed 4 Mar 2016."/>

    <meta name="citation_reference" content="Farhadi A, Forsyth D, White R. Transfer learning in sign language. In: IEEE 2007 conference on computer vision and pattern recognition. 2007. p. 1&#8211;8."/>

    <meta name="citation_reference" content="Feuz KD, Cook DJ. Transfer learning across feature-rich heterogeneous feature spaces via feature-space remapping (FSR). J ACM Trans Intell Syst Technol. 2014;6(1):1&#8211;27 (Article 3)."/>

    <meta name="citation_reference" content="Gao J, Fan W, Jiang J, Han J (2008) Knowledge transfer via multiple model local structure mapping. In: Proceedings of the 14th ACM SIGKDD international conference on knowledge discovery and data mining. p. 283&#8211;91."/>

    <meta name="citation_reference" content="citation_journal_title=Adv Neural Inf Process Syst; citation_title=Graph based consensus maximization among multiple supervised and unsupervised models; citation_author=J Gao, F Liang, W Fan, Y Sun, J Han; citation_volume=22; citation_publication_date=2009; citation_pages=1-9; citation_id=CR38"/>

    <meta name="citation_reference" content="citation_journal_title=J Softw Pract Exp; citation_title=Choosing software metrics for defect prediction: an investigation on feature selection techniques; citation_author=K Gao, TM Khoshgoftaar, H Wang, N Seliya; citation_volume=41; citation_issue=5; citation_publication_date=2011; citation_pages=579-606; citation_doi=10.1002/spe.1043; citation_id=CR39"/>

    <meta name="citation_reference" content="Ge L, Gao J, Ngo H, Li K, Zhang A. On handling negative transfer and imbalanced distributions in multiple source transfer learning. In: Proceedings of the 2013 SIAM international conference on data mining. 2013. p. 254&#8211;71."/>

    <meta name="citation_reference" content="Glorot X, Bordes A, Bengio Y. Domain adaptation for large-scale sentiment classification: A deep learning approach. In: Proceedings of the twenty-eight international conference on machine learning, vol. 27. 2011. p. 97&#8211;110."/>

    <meta name="citation_reference" content="Gong B, Shi Y, Sha F, Grauman K. Geodesic flow kernel for unsupervised domain adaptation. In: Proceedings of the 2012 IEEE conference on computer vision and pattern recognition. 2012. p. 2066&#8211;73."/>

    <meta name="citation_reference" content="Gopalan R, Li R, Chellappa R. Domain adaptation for object recognition: an unsupervised approach. In: 2011 international conference on computer vision. 2011. p. 999&#8211;1006."/>

    <meta name="citation_reference" content="Guo-Jun Qi&#8217;s publication list. 
                    http://www.eecs.ucf.edu/~gqi/publications.html
                    
                  . Accessed 4 Mar 2016."/>

    <meta name="citation_reference" content="Ham JH, Lee DD, Saul LK. Learning high dimensional correspondences from low dimensional manifolds. In: Proceedings of the twentieth international conference on machine learning. 2003. p. 1&#8211;8."/>

    <meta name="citation_reference" content="Harel M, Mannor S. Learning from multiple outlooks. In: Proceedings of the 28th international conference on machine learning. 2011. p. 401&#8211;8."/>

    <meta name="citation_reference" content="He P, Li B, Ma Y (2014) Towards cross-project defect prediction with imbalanced feature sets. 
                    http://arxiv.org/abs/1411.4228
                    
                  ."/>

    <meta name="citation_reference" content="Heterogeneous defect prediction. 
                    http://www.slideshare.net/hunkim/heterogeneous-defect-prediction-esecfse-2015
                    
                  . Accessed 4 Mar 2016."/>

    <meta name="citation_reference" content="HFA_release_0315.rar (Download). 
                    https://sites.google.com/site/xyzliwen/publications/HFA_release_0315.rar
                    
                  . Accessed 4 Mar 2016."/>

    <meta name="citation_reference" content="Hu M, Liu B. Mining and summarizing customer reviews. In: Proceedings of the 10th ACM SIGKDD international conference on Knowledge discovery and data mining. 2004. p. 168&#8211;77."/>

    <meta name="citation_reference" content="Huang J, Smola A, Gretton A, Borgwardt KM, Sch&#246;lkopf B. Correcting sample selection bias by unlabeled data. In: Proceedings of the 2006 conference. Adv Neural Inf Process Syst. 2006. p. 601&#8211;8."/>

    <meta name="citation_reference" content="Jakob N, Gurevych I. Extracting opinion targets in a single and cross-domain setting with conditional random fields. In: Proceedings of the 2010 conference on empirical methods in NLP. 2010. p. 1035&#8211;45."/>

    <meta name="citation_reference" content="Jiang J, Zhai C. Instance weighting for domain adaptation in NLP. In: Proceedings of the 45th annual meeting of the association of computational linguistics. 2007. p. 264&#8211;71."/>

    <meta name="citation_reference" content="Jiang M, Cui P, Wang F, Yang Q, Zhu W, Yang S. Social recommendation across multiple relational domains. In: Proceedings of the 21st ACM international conference on information and knowledge management. 2012. p. 1422&#8211;31."/>

    <meta name="citation_reference" content="Jiang W, Zavesky E, Chang SF, Loui A. Cross-domain learning methods for high-level visual concept classification. In: IEEE 2008 15th international conference on image processing. 2008. p. 161&#8211;4."/>

    <meta name="citation_reference" content="citation_journal_title=Int J Comput Vis; citation_title=Domain adaptation for face recognition: targetize source domain bridged by common subspace; citation_author=M Kan, J Wu, S Shan, X Chen; citation_volume=109; citation_issue=1&#8211;2; citation_publication_date=2014; citation_pages=94-109; citation_doi=10.1007/s11263-013-0693-1; citation_id=CR56"/>

    <meta name="citation_reference" content="citation_journal_title=J Mach Learn Res; citation_title=Lp-norm multiple kernel learning; citation_author=M Kloft, U Brefeld, S Sonnenburg, A Zien; citation_volume=12; citation_publication_date=2011; citation_pages=953-997; citation_id=CR57"/>

    <meta name="citation_reference" content="Kulis B, Saenko K, Darrell T. What you saw is not what you get: domain adaptation using asymmetric kernel transforms. In: IEEE 2011 conference on computer vision and pattern recognition. 2011. p. 1785&#8211;92."/>

    <meta name="citation_reference" content="LeCun Y, Bottou L, HuangFu J. Learning methods for generic object recognition with invariance to pose and lighting. In: Proceedings of the 2004 IEEE computer society conference on computer vision and pattern recognition, vol. 2. 2004. p. 97&#8211;104."/>

    <meta name="citation_reference" content="Li B, Yang Q, Xue X. Can movies and books collaborate? Cross-domain collaborative filtering for sparsity reduction. In: Proceedings of the 21st international joint conference on artificial intelligence. 2009. p. 2052&#8211;57."/>

    <meta name="citation_reference" content="Li B, Yang Q, Xue X. Transfer learning for collaborative filtering via a rating-matrix generative model. In: Proceedings of the 26th annual international conference on machine learning. 2009. p. 617&#8211;24."/>

    <meta name="citation_reference" content="Li F, Pan SJ, Jin O, Yang Q, Zhu X. Cross-domain co-extraction of sentiment and topic lexicons. In: Proceedings of the 50th annual meeting of the association for computational linguistics long papers, vol. 1. 2012. p. 410&#8211;19."/>

    <meta name="citation_reference" content="Li S, Zong C. Multi-domain adaptation for sentiment classification: Using multiple classifier combining methods. In: Proceedings of the conference on natural language processing and knowledge engineering. 2008. p. 1&#8211;8."/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Pattern Anal Mach Intell; citation_title=Learning with augmented features for supervised and semi-supervised heterogeneous domain adaptation; citation_author=W Li, L Duan, D Xu, IW Tsang; citation_volume=36; citation_issue=6; citation_publication_date=2014; citation_pages=1134-1148; citation_doi=10.1109/TPAMI.2013.167; citation_id=CR64"/>

    <meta name="citation_reference" content="LIBSVM (2016) A library for support vector machines. 
                    http://www.csie.ntu.edu.tw/~cjlin/libsvm
                    
                  . Accessed 4 Mar 2016."/>

    <meta name="citation_reference" content="Ling X, Dai W, Xue GR, Yang Q, Yu Y. Spectral domain-transfer learning. In: Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining. 2008. p. 488&#8211;96."/>

    <meta name="citation_reference" content="Lixin Duan. 
                    http://www.lxduan.info/#sourcecode_hfa
                    
                  . Accessed 4 Mar 2016."/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Knowl Data Eng; citation_title=Adaptation regularization: a general framework for transfer learning; citation_author=M Long, J Wang, G Ding, SJ Pan, PS Yu; citation_volume=26; citation_issue=5; citation_publication_date=2014; citation_pages=1076-1089; citation_doi=10.1109/TKDE.2013.111; citation_id=CR68"/>

    <meta name="citation_reference" content="Long M, Wang J, Ding G, Sun J, Yu PS. Transfer feature learning with joint distribution adaptation. In: Proceedings of the 2013 IEEE international conference on computer vision. 2013. p. 2200&#8211;07."/>

    <meta name="citation_reference" content="citation_journal_title=Int Comput Vis; citation_title=Distinctive image features from scale-invariant keypoints; citation_author=DG Lowe; citation_volume=60; citation_issue=2; citation_publication_date=2004; citation_pages=91-110; citation_doi=10.1023/B:VISI.0000029664.99615.94; citation_id=CR70"/>

    <meta name="citation_reference" content="Luo P, Zhuang F, Xiong H, Xiong Y, He Q. Transfer learning from multiple source domains via consensus regularization. In: Proceedings of the 17th ACM conference on information and knowledge management. 2008. p. 103&#8211;12."/>

    <meta name="citation_reference" content="citation_journal_title=J Quant Spectrosc Radiat Transf; citation_title=Transfer learning used to analyze the dynamic evolution of the dust aerosol; citation_author=Y Ma, W Gong, F Mao; citation_volume=153; citation_publication_date=2015; citation_pages=119-130; citation_doi=10.1016/j.jqsrt.2014.09.025; citation_id=CR72"/>

    <meta name="citation_reference" content="Marszalek M, Schmid C, Harzallah H, Van de Weijer J. Learning object representations for visual object class recognition. In: Visual recognition challenge workshop ICCV. 2007. p. 1&#8211;10."/>

    <meta name="citation_reference" content="Mihalkova L, Mooney RJ. Transfer learning by mapping with minimal target data. In: Proc. assoc. for the advancement of artificial intelligence workshop transfer learning for complex tasks. 2008. p. 31&#8211;6."/>

    <meta name="citation_reference" content="Long M. 
                    http://ise.thss.tsinghua.edu.cn/~mlong/
                    
                  . Accessed 4 Mar 2016."/>

    <meta name="citation_reference" content="Moreno O, Shapira B, Rokach L, Shani G (2012) TALMUD&#8212;transfer learning for multiple domains. In: Proceedings of the 21st ACM international conference on information and knowledge management. 2012. p. 425&#8211;34."/>

    <meta name="citation_reference" content="Nam J, Kim S (2015) Heterogeneous defect prediction. In: Proceedings of the 2015 10th joint meeting on foundations of software engineering. 2015. p. 508&#8211;19."/>

    <meta name="citation_reference" content="Ng MK, Wu Q, Ye Y. Co-transfer learning via joint transition probability graph based method. In: Proceedings of the 1st international workshop on cross domain knowledge discovery in web and social network mining. 2012. p. 1&#8211;9."/>

    <meta name="citation_reference" content="Ngiam J, Khosla A, Kim M, Nam J, Lee H, Ng AY. Multimodal deep learning. In: The 28th international conference on machine learning. 2011. p. 689&#8211;96."/>

    <meta name="citation_reference" content="Ogoe HA, Visweswaran S, Lu X, Gopalakrishnan V. Knowledge transfer via classification rules using functional mapping for integrative modeling of gene expression data. BMC Bioinform. 2015. p. 1&#8211;15."/>

    <meta name="citation_reference" content="Oquab M, Bottou L, Laptev I, Sivic J. Learning and transferring mid-level image representations using convolutional neural networks. In: Proceedings of the 2014 IEEE conference on computer vision and pattern recognition. 2013. p. 1717&#8211;24."/>

    <meta name="citation_reference" content="Pan SJ, Kwok JT, Yang Q. Transfer learning via dimensionality reduction. In: Proceedings of the 23rd national conference on artificial intelligence, vol. 2. 2008. p. 677&#8211;82."/>

    <meta name="citation_reference" content="Pan SJ, Ni X, Sun JT, Yang Q, Chen Z. Cross-domain sentiment classification via spectral feature alignment. In: Proceedings of the 19th international conference on world wide web. 2010. p. 751&#8211;60."/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Knowl Data Eng; citation_title=A survey on transfer learning; citation_author=SJ Pan, Q Yang; citation_volume=22; citation_issue=10; citation_publication_date=2010; citation_pages=1345-1359; citation_doi=10.1109/TKDE.2009.191; citation_id=CR84"/>

    <meta name="citation_reference" content="Pan W, Liu NN, Xiang EW, Yang Q. Transfer learning to predict missing ratings via heterogeneous user feedbacks. In: Proceedings of the 22nd international joint conference on artificial intelligence. 2011. p. 2318&#8211;23."/>

    <meta name="citation_reference" content="Pan W. Xiang EW, Liu NN, Yang Q. Transfer learning in collaborative filtering for sparsity reduction. In: Twenty-fourth AAAI conference on artificial intelligence, vol. 1. 2010. p. 230&#8211;235."/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Neural Netw; citation_title=Domain adaptation via transfer component analysis; citation_author=SJ Pan, IW Tsang, JT Kwok, Q Yang; citation_volume=22; citation_issue=2; citation_publication_date=2009; citation_pages=199-210; citation_id=CR87"/>

    <meta name="citation_reference" content="Papers:oquab-2014. 
                    http://leon.bottou.org/papers/oquab-2014
                    
                  . Accessed 4 Mar 2016."/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Signal Process Mag; citation_title=Visual domain adaptation: a survey of recent advances; citation_author=VM Patel, R Gopalan, R Li, R Chellappa; citation_volume=32; citation_issue=3; citation_publication_date=2014; citation_pages=53-69; citation_doi=10.1109/MSP.2014.2347059; citation_id=CR89"/>

    <meta name="citation_reference" content="citation_journal_title=Mach Learn; citation_title=Machine learning for targeted display advertising: transfer learning in action; citation_author=C Perlich, B Dalessandro, T Raeder, O Stitelman, F Provost; citation_volume=95; citation_publication_date=2014; citation_pages=103-127; citation_doi=10.1007/s10994-013-5375-2; citation_id=CR90"/>

    <meta name="citation_reference" content="Prettenhofer P, Stein B. (2010) Cross-language text classification using structural correspondence learning. In: Proceedings of the 48th annual meeting of the association for computational linguistics. 2010. p. 1118&#8211;27."/>

    <meta name="citation_reference" content="Qi GJ, Aggarwal C, Huang T. Towards semantic knowledge propagation from text corpus to Web images. In: Proceedings of the 20th international conference on world wide web. p. 297&#8211;306."/>

    <meta name="citation_reference" content="Qiu G, Liu B, Bu J, Chen C. Expanding domain sentiment lexicon through double propagation. In: Proceedings of the 21st international joint conference on artificial intelligence. p. 1199&#8211;204."/>

    <meta name="citation_reference" content="Quanz B, Huan J. Large margin transductive transfer learning. In: Proceedings of the 18th ACM conference on information and knowledge management. 2009. p. 1327&#8211;36."/>

    <meta name="citation_reference" content="Raina R, Battle A, Lee H, Packer B, Ng AY. Self-taught learning: transfer learning from unlabeled data. In: Proceedings of the 24th international conference on machine learning. 2007. p. 759&#8211;66."/>

    <meta name="citation_reference" content="citation_journal_title=Int J Comput Vis; citation_title=Exploring transfer learning approaches for head pose classification from multi-view surveillance images; citation_author=AN Rajagopal, R Subramanian, E Ricci, RL Vieriu, O Lanz, KR Ramakrishnan, N Sebe; citation_volume=109; citation_issue=1&#8211;2; citation_publication_date=2014; citation_pages=146-167; citation_doi=10.1007/s11263-013-0692-2; citation_id=CR96"/>

    <meta name="citation_reference" content="Romera-Paredes B, Aung MSH, Pontil M, Bianchi-Berthouze N, Williams AC de C, Watson P. Transfer learning to account for idiosyncrasy in face and body expressions. In: Proceedings of the 10th international conference on automatic face and gesture recognition (FG). 2013. p. 1&#8211;6."/>

    <meta name="citation_reference" content="Rosenstein MT, Marx Z, Kaelbling LP, Dietterich TG. To transfer or not to transfer. In: Proceedings NIPS&#8217;05 workshop, inductive transfer. 10&#160;years later. 2005. p. 1&#8211;4."/>

    <meta name="citation_reference" content="Roy S.D., Mei T., Zeng W., Li S. Social transfer: cross-domain transfer learning from social streams for media applications. In: Proceedings of the 20th ACM international conference on multimedia. p. 649&#8211;58."/>

    <meta name="citation_reference" content="citation_journal_title=Comput Vision ECCV; citation_title=Adapting visual category models to new domains; citation_author=K Saenko, B Kulis, M Fritz, T Darrell; citation_volume=6314; citation_publication_date=2010; citation_pages=213-226; citation_id=CR100"/>

    <meta name="citation_reference" content="citation_journal_title=Adv Neural Inf Process Syst; citation_title=An empirical analysis of domain adaptation algorithms for genomic sequence analysis; citation_author=G Schweikert, C Widmer, B Sch&#246;lkopf, G R&#228;tsch; citation_volume=21; citation_publication_date=2009; citation_pages=1433-1440; citation_id=CR101"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Cybern; citation_title=Combating negative transfer from predictive distribution differences; citation_author=CW Seah, YS Ong, IW Tsang; citation_volume=43; citation_issue=4; citation_publication_date=2013; citation_pages=1153-1165; citation_doi=10.1109/TSMCB.2012.2225102; citation_id=CR102"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Neural Netw Learn Syst; citation_title=Transfer learning for visual categorization: a survey; citation_author=L Shao, F Zhu, X Li; citation_volume=26; citation_issue=5; citation_publication_date=2014; citation_pages=1019-1034; citation_doi=10.1109/TNNLS.2014.2330900; citation_id=CR103"/>

    <meta name="citation_reference" content="Shawe-Taylor J, Cristianini N. Kernel methods for pattern analysis. Cambridge: Cambridge University Press; 2004."/>

    <meta name="citation_reference" content="Shi X, Liu Q, Fan W, Yu PS, Zhu R. Transfer learning on heterogeneous feature spaces via spectral transformation. In: 2010 IEEE international conference on data mining. 2010. p. 1049&#8211;1054."/>

    <meta name="citation_reference" content="Shi Y, Sha F. Information-theoretical learning of discriminative clusters for unsupervised domain adaptation. In: Proceedings of the 29th international conference on machine learning. 2012. p. 1&#8211;8."/>

    <meta name="citation_reference" content="citation_journal_title=J Stat Plan Inf; citation_title=Improving predictive inference under covariate shift by weighting the log-likelihood function; citation_author=H Shimodaira; citation_volume=90; citation_issue=2; citation_publication_date=2000; citation_pages=227-244; citation_doi=10.1016/S0378-3758(00)00115-4; citation_id=CR107"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Softw Eng; citation_title=Reducing features to improve code change-based bug prediction; citation_author=S Shivaji, EJ Whitehead, R Akella, S Kim; citation_volume=39; citation_issue=4; citation_publication_date=2013; citation_pages=552-569; citation_doi=10.1109/TSE.2012.43; citation_id=CR108"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Knowl Data Eng; citation_title=Bregman divergence-based regularization for transfer subspace learning; citation_author=S Si, D Tao, B Geng; citation_volume=22; citation_issue=7; citation_publication_date=2010; citation_pages=929-942; citation_doi=10.1109/TKDE.2009.126; citation_id=CR109"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Pattern Anal Mach Intell; citation_title=Contextualizing object detection and classification; citation_author=Z Song, Q Chen, Z Huang, Y Hua, S Yan; citation_volume=37; citation_issue=1; citation_publication_date=2011; citation_pages=13-27; citation_id=CR110"/>

    <meta name="citation_reference" content="citation_journal_title=JMLR; citation_title=On the influence of the kernel on the consistency of support vector machines; citation_author=I Steinwart; citation_volume=2; citation_publication_date=2001; citation_pages=67-93; citation_id=CR111"/>

    <meta name="citation_reference" content="citation_journal_title=JMLR; citation_title=Transfer learning for reinforcement learning domains: a survey; citation_author=ME Taylor, P Stone; citation_volume=10; citation_publication_date=2009; citation_pages=1633-1685; citation_id=CR112"/>

    <meta name="citation_reference" content="Tommasi T, Caputo B. The more you know, the less you learn: from knowledge transfer to one-shot learning of object categories. BMVC. 2009;1&#8211;11."/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Conf Comput Vision Pattern Recog; citation_title=Safety in numbers: learning categories from few examples with multi model knowledge transfer; citation_author=T Tommasi, F Orabona, B Caputo; citation_volume=2010; citation_publication_date=2010; citation_pages=3081-3088; citation_id=CR114"/>

    <meta name="citation_reference" content="Transfer learning resources. 
                    http://www.cse.ust.hk/TL/
                    
                  . Accessed 4 Mar 2016."/>

    <meta name="citation_reference" content="Tutorial on domain adaptation and transfer learning. 
                    http://tommasit.wix.com/datl14tutorial
                    
                  . Accessed 4 Mar 2016."/>

    <meta name="citation_reference" content="citation_journal_title=Adv Neural Inf Process Syst; citation_title=Principles of risk minimization for learning theory; citation_author=V Vapnik; citation_volume=4; citation_publication_date=1992; citation_pages=831-838; citation_id=CR117"/>

    <meta name="citation_reference" content="Vedaldi A, Gulshan V, Varma M, Zisserman A. Multiple kernels for object detection. In: 2009 IEEE 12th international conference on computer vision. 2009. p. 606&#8211;13."/>

    <meta name="citation_reference" content="Vincent P, Larochelle H, Bengio Y, Manzagol PA. Extracting and composing robust features with denoising autoencoders. In: Proceedings of the 25th international conference on machine learning. 2008. p. 1096&#8211;103."/>

    <meta name="citation_reference" content="citation_journal_title=Adv Neural Inf Proces Syst; citation_title=Inferring a semantic representation of text via crosslanguage correlation analysis; citation_author=A Vinokourov, J Shawe-Taylor, N Cristianini; citation_volume=15; citation_publication_date=2002; citation_pages=1473-1480; citation_id=CR120"/>

    <meta name="citation_reference" content="Wang C, Mahadevan S. Heterogeneous domain adaptation using manifold alignment. In: Proceedings of the 22nd international joint conference on artificial intelligence, vol. 2. 2011. p. 541&#8211;46."/>

    <meta name="citation_reference" content="Wang G, Hoiem D, Forsyth DA. Building text Features for object image classification. In: 2009 IEEE conference on computer vision and pattern recognition. 2009. p. 1367&#8211;74."/>

    <meta name="citation_reference" content="Wang H, Klaser A, Schmid C, Liu CL (2011) Action recognition by dense trajectories. In: IEEE 2011 conference on computer vision and pattern recognition. 2011. p. 3169&#8211;76."/>

    <meta name="citation_reference" content="Wei B, Pal C (2010) Cross-lingual adaptation: an experiment on sentiment classifications. In: Proceedings of the ACL 2010 conference short papers. 2010. p. 258&#8211;62."/>

    <meta name="citation_reference" content="Wei B, Pal C (2011) Heterogeneous transfer learning with RBMs. In: Proceedings of the twenty-fifth AAAI conference on artificial intelligence. 2011. p. 531&#8211;36."/>

    <meta name="citation_reference" content="citation_journal_title=JMLR; citation_title=Distance metric learning for large margin nearest neighbor classification; citation_author=KQ Weinberger, LK Saul; citation_volume=10; citation_publication_date=2009; citation_pages=207-244; citation_id=CR126"/>

    <meta name="citation_reference" content="citation_journal_title=JMLR; citation_title=Multitask learning in computational biology; citation_author=C Widmer, G Ratsch; citation_volume=27; citation_publication_date=2012; citation_pages=207-216; citation_id=CR127"/>

    <meta name="citation_reference" content="citation_journal_title=J Am Med Inform Assoc; citation_title=A study in transfer learning: leveraging data from multiple hospitals to enhance hospital-specific predictions; citation_author=J Wiens, J Guttag, EJ Horvitz; citation_volume=21; citation_issue=4; citation_publication_date=2013; citation_pages=699-706; citation_doi=10.1136/amiajnl-2013-002162; citation_id=CR128"/>

    <meta name="citation_reference" content="citation_title=Data mining, practical machine learning tools and techniques; citation_publication_date=2011; citation_id=CR129; citation_author=IH Witten; citation_author=E Frank; citation_publisher=Morgan Kaufmann Publishers"/>

    <meta name="citation_reference" content="Wu X, Xu D, Duan L, Luo J (2011) Action recognition using context and appearance distribution features. In: IEEE 2011 conference on computer vision and pattern recognition. 2011. p. 489&#8211;96."/>

    <meta name="citation_reference" content="Xia R, Zong C. A POS-based ensemble model for cross-domain sentiment classification. In: Proceedings of the 5th international joint conference on natural language processing. 2011. p. 614&#8211;22."/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Intell Syst; citation_title=Feature ensemble plus sample selection: domain adaptation for sentiment classification; citation_author=R Xia, C Zong, X Hu, E Cambria; citation_volume=28; citation_issue=3; citation_publication_date=2013; citation_pages=10-18; citation_doi=10.1109/MIS.2013.27; citation_id=CR132"/>

    <meta name="citation_reference" content="Xiao M, Guo Y. Semi-supervised kernel matching for domain adaptation. In: Proceedings of the twenty-sixth AAAI conference on artificial intelligence. 2012. p. 1183&#8211;89."/>

    <meta name="citation_reference" content="Xie M, Jean N, Burke M, Lobell D, Ermon S. Transfer learning from deep features for remote sensing and poverty mapping. In: Proceedings 30th AAAI conference on artificial intelligence. 2015. p. 1&#8211;10."/>

    <meta name="citation_reference" content="Yang J, Yan R, Hauptmann AG. Cross-domain video concept detection using adaptive SVMs. In: Proceedings of the 15th ACM international conference on multimedia. 2007. p. 188&#8211;97."/>

    <meta name="citation_reference" content="Yang L, Jing L, Yu J, Ng MK. Learning transferred weights from co-occurrence data for heterogeneous transfer learning. IEEE Trans Neural Netw Learn Syst. 2015;PP(99):1&#8211;14."/>

    <meta name="citation_reference" content="Yang Q, Chen Y, Xue GR, Dai W, Yu Y. Heterogeneous transfer learning for image clustering via the social web. In: Proceedings of the joint conference of the 47th annual meeting of the ACL, vol. 1. 2009. p. 1&#8211;9."/>

    <meta name="citation_reference" content="Yao Y, Doretto G. Boosting for transfer learning with multiple sources. In: Proceedings of the IEEE computer society conference on computer vision and pattern recognition. 2010. p. 1855&#8211;62."/>

    <meta name="citation_reference" content="Yin Z. 
                    http://www.cse.ust.hk/~yinz/
                    
                  . Accessed 4 Mar 2016."/>

    <meta name="citation_reference" content="Zhang Y, Cao B, Yeung D. Multi-domain collaborative filtering. In: Proceedings of the 26th conference on uncertainty in artificial intelligence. 2010. p. 725&#8211;32."/>

    <meta name="citation_reference" content="Zhang Y, Yeung DY. Transfer metric learning by learning task relationships. In: Proceedings of the 16th ACM SIGKDD international conference on knowledge discovery and data mining. 2010. p. 1199&#8211;208."/>

    <meta name="citation_reference" content="Zhao L, Pan SJ, Xiang EW, Zhong E, Lu Z, Yang Q. Active transfer learning for cross-system recommendation. In: Proceedings of the 27th AAAI conference on artificial intelligence. 2013. p. 1205&#8211;11."/>

    <meta name="citation_reference" content="Zhong E, Fan W, Peng J, Zhang K, Ren J, Turaga D, Verscheure O. Cross domain distribution adaptation via kernel mapping. In: Proceedings of the 15th ACM SIGKDD. 2009. p. 1027&#8211;36."/>

    <meta name="citation_reference" content="Zhou JT, Pan S, Tsang IW, Yan Y. Hybrid heterogeneous transfer learning through deep learning. In: Proceedings of the national conference on artificial intelligence, vol. 3. 2014. p. 2213&#8211;20."/>

    <meta name="citation_reference" content="Zhou JT, Tsang IW, Pan SJ Tan M. Heterogeneous domain adaptation for multiple classes. In: International conference on artificial intelligence and statistics. 2014. p. 1095&#8211;103."/>

    <meta name="citation_reference" content="Zhu Y, Chen Y, Lu Z, Pan S, Xue G, Yu Y, Yang Q. Heterogeneous transfer learning for image classification. In: Proceedings of the national conference on artificial intelligence, vol. 2. 2011. p. 1304&#8211;9."/>

    <meta name="citation_author" content="Karl Weiss"/>

    <meta name="citation_author_institution" content="Florida Atlantic University, Boca Raton, USA"/>

    <meta name="citation_author" content="Taghi M. Khoshgoftaar"/>

    <meta name="citation_author_institution" content="Florida Atlantic University, Boca Raton, USA"/>

    <meta name="citation_author" content="DingDing Wang"/>

    <meta name="citation_author_institution" content="Florida Atlantic University, Boca Raton, USA"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1186/s40537-016-0043-6"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Journal of Big Data"/>
        <meta property="og:title" content="A survey of transfer learning"/>
        <meta property="og:description" content="Machine learning and data mining techniques have been used in numerous real-world applications. An assumption of traditional machine learning methodologies is the training data and testing data are taken from the same domain, such that the input feature space and data distribution characteristics are the same. However, in some real-world machine learning scenarios, this assumption does not hold. There are cases where training data is expensive or difficult to collect. Therefore, there is a need to create high-performance learners trained with more easily obtained data from different domains. This methodology is referred to as transfer learning. This survey paper formally defines transfer learning, presents information on current solutions, and reviews applications applied to transfer learning. Lastly, there is information listed on software downloads for various transfer learning solutions and a discussion of possible future research work. The transfer learning solutions surveyed are independent of data size and can be applied to big data environments."/>
        <meta property="og:image" content="https://media.springernature.com/w200/springer-static/cover/journal/40537.jpg"/>
    

    <title>A survey of transfer learning | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>
    
    <style>button{line-height:inherit}html,label{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif}html{height:100%;overflow-y:scroll;box-sizing:border-box;color:#333;line-height:1.61803;-webkit-font-smoothing:subpixel-antialiased;font-size:62.5%}*{box-sizing:inherit}body{max-width:100%;min-height:100%;background-color:#fcfcfc;background-position:initial initial;background-repeat:initial initial;margin:0}button,div,form,input{margin:0;padding:0}body,p{padding:0}a{color:#004b83;text-decoration:underline}h1,h2{margin-top:0}h1{font-size:3.2rem}h2{font-size:2.8rem}h1,h2{font-style:normal;margin-bottom:1em;line-height:1.4;font-family:Georgia,Palatino,serif;font-weight:400}p{margin:0}ul{margin-top:0}p{margin-bottom:1.5em}.c-ad{display:none;padding:8px;text-align:center}@media only screen and (min-width:768px){.js .c-ad{display:block}}.c-ad--728x90{background-color:#ccc}.c-ad--728x90 .c-ad__inner{min-height:calc(1.5em + 94px)}.c-ad--300x250{background-color:#f2f2f2}.c-ad--300x250 .c-ad__inner{min-height:calc(1.5em + 254px)}.c-ad__label,.c-header{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif}.c-ad__label{font-size:1.4rem;font-weight:400;margin-bottom:4px;color:#333;line-height:1.5}.c-header{font-size:1.6rem}.c-header{background-color:#fff;padding:16px 0;border-bottom:4px solid #00285a}.c-header__container{margin:0 auto;max-width:1280px;padding:0 16px;display:-webkit-flex;-webkit-box-align:center;-webkit-align-items:center;-webkit-box-pack:justify;-webkit-justify-content:space-between}.c-header__brand{margin-right:32px}.c-header__brand a{text-decoration:none}.c-header__menu,.c-header__navigation{display:-webkit-flex}.c-header__navigation{-webkit-box-align:center;-webkit-align-items:center}.c-header__menu{list-style:none;margin:0;padding:0}.c-header__item{color:inherit;margin-right:24px}.c-header__item:last-child{margin-right:0}.c-header__link{text-decoration:none;color:inherit}.js .c-popup{position:absolute;font-family:Georgia,Palatino,serif;z-index:100;padding:16px;border:1px solid rgba(151,191,216,.298039);-webkit-box-shadow:hsla(0,0%,50.2%,.0980392) 0 0 5px 0;box-shadow:0 0 5px 0 hsla(0,0%,50.2%,.0980392);width:auto;border-top-left-radius:2px;border-top-right-radius:2px;border-bottom-right-radius:2px;border-bottom-left-radius:2px;background-color:#fff}.js .c-popup__close{position:absolute;display:block;top:16px;right:16px;height:16px;background-image:url("data:image/svg+xml,%0A%3Csvg height='16' viewBox='0 0 16 16' width='16' xmlns='http://www.w3.org/2000/svg'%3E%3Cpath d='m2.29679575 12.2772478c-.39658757.3965876-.39438847 1.0328109-.00062148 1.4265779.39651227.3965123 1.03246768.3934888 1.42657791-.0006214l4.27724782-4.27724787 4.2772478 4.27724787c.3965876.3965875 1.0328109.3943884 1.4265779.0006214.3965123-.3965122.3934888-1.0324677-.0006214-1.4265779l-4.27724787-4.2772478 4.27724787-4.27724782c.3965875-.39658757.3943884-1.03281091.0006214-1.42657791-.3965122-.39651226-1.0324677-.39348875-1.4265779.00062148l-4.2772478 4.27724782-4.27724782-4.27724782c-.39658757-.39658757-1.03281091-.39438847-1.42657791-.00062148-.39651226.39651227-.39348875 1.03246768.00062148 1.42657791l4.27724782 4.27724782z' fill='%23666' fill-rule='evenodd'/%3E%3C/svg%3E");border:0;padding-right:16px;background-position:initial initial;background-repeat:no-repeat}.js .c-popup__close-text{border:0;clip:rect(0 0 0 0);height:1px;margin:-100%;overflow:hidden;padding:0;width:1px;position:absolute!important}.js .c-popup__arrow{content:"";position:absolute;width:20px;height:20px;background-color:#fff;border-top:1px solid rgba(151,191,216,.298039);border-left:1px solid rgba(151,191,216,.298039)}body{font-size:1.8em}.c-article-header{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;margin-bottom:40px}.c-article-identifiers{list-style:none;font-size:1.6rem;line-height:1.3;display:-webkit-flex;-webkit-flex-wrap:wrap;color:#6f6f6f;padding:0;margin:0 0 8px}.c-article-identifiers__item{border-right:1px solid #6f6f6f;margin-right:8px;padding-right:8px;list-style:none}.c-article-identifiers__item a{color:#069;text-decoration:none}.c-article-identifiers__item:last-child{margin-right:0;padding-right:0;border-right-width:0}@media only screen and (min-width:768px){.c-author-popup .c-article-identifiers{-webkit-box-pack:end;-webkit-justify-content:flex-end}}.c-article-title{font-size:2.4rem;line-height:1.25;margin-bottom:16px}@media only screen and (min-width:768px){.c-article-title{font-size:3rem;line-height:1.2}}.c-author-list{font-size:1.6rem;list-style:none;margin-bottom:0;padding:0;width:100%}.c-author-list__item{margin-left:0}.c-author-list__item,.c-author-list li{display:inline;padding-right:0}.c-author-list__item svg{margin-left:4px}.c-article-info-details{font-size:1.6rem;margin-bottom:8px;margin-top:16px}.c-article-info-details__cite-as{border-left:1px solid #6f6f6f;margin-left:8px;padding-left:8px}.c-article-metrics-bar{display:-webkit-flex;-webkit-flex-wrap:wrap;line-height:1.3;font-size:1.6rem}.c-article-metrics-bar__wrapper{margin:0 0 16px}.c-article-metrics-bar__item{-webkit-box-align:baseline;-webkit-align-items:baseline;border-right:1px solid #6f6f6f;margin-right:8px}.c-article-metrics-bar__item:last-child{border-right-width:0}.c-article-metrics-bar__count{font-weight:700;margin:0}.c-article-metrics-bar__label{color:#626262;font-weight:400;font-style:normal;margin:0 10px 0 5px}.c-article-metrics-bar__details{margin:0}.c-article-main-column{margin-right:8.6%;width:60.2%}@media only screen and (max-width:1023px){.c-article-main-column{margin-right:0;width:100%}}.c-author-popup__subheading{font-weight:700;float:left;padding-right:8px;margin-bottom:8px;margin-top:4px}.c-author-popup .c-article-button{font-size:1.6rem;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif}.c-author-popup__author-list{list-style:none;font-size:1.6rem;padding:0;margin-top:0;clear:both;margin-bottom:16px}.c-author-popup__author-list>li{margin-bottom:8px}.c-author-popup__link{font-weight:700;vertical-align:baseline;color:#069;text-decoration:none}.c-author-popup .c-article-button{color:#fff;background-image:linear-gradient(#4d78af,#3365a0);border:1px solid transparent;border-top-left-radius:2px;border-top-right-radius:2px;border-bottom-right-radius:2px;border-bottom-left-radius:2px;text-decoration:none;display:block;width:100%;padding-top:8px;padding-bottom:8px;text-align:center;background-position:initial initial;background-repeat:initial initial}.c-article-section{clear:both}.c-article-section__title{border-bottom:2px solid #d5d5d5;font-size:2rem;line-height:1.3;padding-bottom:8px}@media only screen and (min-width:768px){.c-article-section__title{font-size:2.4rem;line-height:1.24}}.c-article-section__content{margin-bottom:40px}@media only screen and (max-width:1023px){.c-article-section__content{padding-left:0}}.c-article-authors-search{margin-top:0;margin-bottom:24px}.c-article-authors-search__item,.c-article-authors-search__title{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif}.c-article-authors-search__title{color:#626262;font-weight:700;margin:0;padding:0;font-size:1.7rem}.c-article-authors-search__item{font-size:1.6rem}.c-article-authors-search__text{margin:0}@media only screen and (min-width:768px){.c-author-popup .c-article-authors-search__list{display:-webkit-flex;-webkit-flex-wrap:wrap;-webkit-box-pack:justify;-webkit-justify-content:space-between;-webkit-box-align:center;-webkit-align-items:center}.c-author-popup .c-article-authors-search__list-item--left{-webkit-flex-basis:40%}}.c-author-popup .c-article-authors-search__list-item--right{margin-top:16px}@media only screen and (min-width:768px){.c-author-popup .c-article-authors-search__list-item--right{text-align:right;-webkit-box-flex:1;-webkit-flex:1 1 0px;margin-top:0}}.c-article-share-box__no-sharelink-info{font-size:1.3rem;font-weight:700;padding-top:4px;margin-bottom:24px}.c-article-share-box__only-read-input{border:1px solid #d5d5d5;display:inline-block;margin-bottom:8px;font-size:1.4rem;font-weight:700;padding:8px 10px}.c-article-share-box__button--link-like{background-color:transparent;border:0;color:#069;cursor:pointer;font-size:1.4rem;margin-bottom:8px;margin-left:10px}.c-article-body{clear:both}.c-article-body p{word-wrap:break-word}.c-pdf-download{display:-webkit-flex;margin-bottom:24px;max-height:48px}@media only screen and (min-width:540px){.c-pdf-download{max-height:none}}@media only screen and (min-width:1024px){.c-pdf-download{max-height:48px}}@media only screen and (min-width:1024px){.c-pdf-button__container{display:none}}.c-context-bar{position:relative;width:100%;-webkit-box-shadow:rgba(51,51,51,.2) 0 0 10px 0;box-shadow:0 0 10px 0 rgba(51,51,51,.2)}.c-context-bar__title{display:none}.c-article-extras{float:left;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;width:31.2%}@media only screen and (max-width:1023px){.c-article-extras{display:none}}.c-pdf-download__link{display:-webkit-flex;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;-webkit-box-pack:justify;-webkit-justify-content:space-between;color:#fff;background-image:linear-gradient(#4d78af,#3365a0);border:1px solid transparent;border-top-left-radius:2px;border-top-right-radius:2px;border-bottom-right-radius:2px;border-bottom-left-radius:2px;text-decoration:none;font-size:1.6rem;line-height:1.3;-webkit-box-flex:1;-webkit-flex:1 1 0px;padding:13px 24px;background-position:initial initial;background-repeat:initial initial}.c-reading-companion{clear:both}.c-reading-companion__sticky{max-width:582px}.c-reading-companion__scroll-pane{overflow-x:hidden;overflow-y:auto;margin:0 0 16px}.c-reading-companion__tabs{font-size:1.6rem;list-style:none;display:-webkit-flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-webkit-flex-flow:row nowrap;margin:0 0 8px;padding:0}.c-reading-companion__tabs>li{-webkit-box-flex:1;-webkit-flex-grow:1}.c-reading-companion__tab{color:#069;border:1px solid #d5d5d5;border-left-width:0;background-color:#eee;padding:8px 8px 8px 15px;text-align:left;font-size:1.6rem;width:100%}.c-reading-companion__tabs li:first-child .c-reading-companion__tab{border-left-width:1px}.c-reading-companion__tab--active{color:#222;background-color:#fcfcfc;border-bottom:1px solid #fcfcfc;font-weight:700}.c-reading-companion__references-list,.c-reading-companion__sections-list{list-style:none;padding:0}.c-reading-companion__references-list--numeric{list-style:decimal inside}.c-reading-companion__sections-list{margin:0 0 8px;min-height:50px}.c-reading-companion__section-item{font-size:1.6rem;padding:0}.c-reading-companion__section-item a{display:block;padding:8px 0 8px 16px;line-height:1em;overflow:hidden;white-space:nowrap;text-overflow:ellipsis}.c-reading-companion__reference-item{padding:8px 8px 8px 0;border-top:1px solid #d5d5d5;font-size:1.6rem}.c-reading-companion__reference-item:first-child{border-top-style:none}.c-reading-companion__reference-citation{display:inline}.c-reading-companion__reference-links{list-style:none;text-align:right;margin:8px 0 0;padding:0;font-weight:700;font-size:1.3rem}.c-reading-companion__reference-links>a{display:inline-block;padding-left:8px}.c-reading-companion__reference-links>a:first-child{display:inline-block;padding-left:0}.c-reading-companion__panel{display:none;border-top:1px solid #d5d5d5;margin-top:-9px;padding-top:9px}.c-reading-companion__panel--active{display:block}.c-popup-search{position:relative;z-index:10;background-color:#eee;padding:16px 0;-webkit-box-shadow:rgba(0,0,0,.207843) 0 3px 3px -3px;box-shadow:0 3px 3px -3px rgba(0,0,0,.207843)}@media only screen and (min-width:1024px){.js .c-popup-search{position:absolute;width:100%;top:100%}.c-popup-search__container{margin:auto;max-width:70%}}.app-search__content{display:-webkit-flex}.app-search__label{font-size:1.4rem;display:inline-block;color:#666;margin-bottom:8px}.app-search__input{font-size:1.4rem;border:1px solid #b3b3b3;border-top-left-radius:3px;border-bottom-left-radius:3px;vertical-align:middle;line-height:1.2;-webkit-box-shadow:rgba(0,0,0,.207843) 0 1px 3px 0 inset;box-shadow:inset 0 1px 3px 0 rgba(0,0,0,.207843);padding:.75em 1em;width:100%;-webkit-box-flex:0;-webkit-flex:0 1 auto}.app-search__button{-webkit-box-align:center;-webkit-align-items:center;cursor:pointer;display:-webkit-inline-flex;margin:0;position:relative;text-decoration:none;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:16px;line-height:1.3;-webkit-box-pack:center;-webkit-justify-content:center;padding:8px;transition:.25s ease,color .25s ease,border-color .25s ease;-webkit-transition:.25s ease,color .25s ease,border-color .25s ease;color:#fff;background-color:#33629d;background-image:linear-gradient(#4d76a9,#33629d);border:1px solid rgba(0,59,132,.498039);width:50px;text-align:center;border-top-left-radius:0;border-bottom-left-radius:0}.u-clearfix:after,.u-clearfix:before{content:"";display:table}.u-clearfix:after{clear:both}.u-display-block{display:block}.u-display-flex{display:-webkit-flex;width:100%}.u-align-items-center{-webkit-box-align:center;-webkit-align-items:center}.u-flex-static{-webkit-box-flex:0;-webkit-flex:0 1 auto;-webkit-flex:0 0 auto}.u-display-none{display:none}.js .u-js-hide{display:none;visibility:hidden}@media print{.u-hide-print{display:none}}.u-icon{fill:currentColor;-webkit-transform:translate(0);display:inline-block;vertical-align:text-top}.u-list-reset{list-style:none;margin:0;padding:0}.u-button-reset{background-color:transparent;border:0;padding:0}.u-h3{font-size:1.8rem}.u-container{margin:0 auto;max-width:1280px;padding:0 16px}.u-position-relative{position:relative}.u-mt-32{margin-top:32px}.u-mr-24{margin-right:24px}.u-mb-16{margin-bottom:16px}.u-mb-24{margin-bottom:24px}.u-mb-32{margin-bottom:32px}.u-ml-8{margin-left:8px}.u-hide{display:none;visibility:hidden}.u-visually-hidden{border:0;clip:rect(0 0 0 0);height:1px;margin:-100%;overflow:hidden;padding:0;width:1px;position:absolute!important}.hide,.js .js-hide{display:none;visibility:hidden}.c-article-section__content p{line-height:1.8}.c-reading-companion__section-item a{text-decoration:none}.c-reading-companion__sections-list{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif}.c-article-main-column .c-pdf-button__container .c-pdf-download{display:none}@media only screen and (max-width:1023px){.c-article-main-column .c-pdf-button__container .c-pdf-download{display:block}}</style>



    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-f182b8db52.css media="screen">
    <link rel="stylesheet" data-inline-css-source="critical-css" id="js-mustard" href="/oscar-static/app-springerlink/css/enhanced-article-ccb94c9114.css" media="print" onload="this.media='only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)';this.onload=null">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"IT","doi":"10.1186-s40537-016-0043-6","Journal Title":"Journal of Big Data","Journal Id":40537,"Keywords":"Transfer learning, Survey, Domain adaptation, Machine learning, Data mining","kwrd":["Transfer_learning","Survey","Domain_adaptation","Machine_learning","Data_mining"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":[],"Open Access":"Y","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":[],"businessPartnerIDString":""}},"Access Type":"open","Bpids":"","Bpnames":"","BPID":["1"],"VG Wort Identifier":"vgzm.415900-10.1186-s40537-016-0043-6","Full HTML":"Y","Subject Codes":["SCI","SCI18024","SCI18032","SCI18030","SCM14026","SCM13110","SCT24035"],"pmc":["I","I18024","I18032","I18030","M14026","M13110","T24035"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"2196-1115"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Database Management","2":"Information Storage and Retrieval","3":"Data Mining and Knowledge Discovery","4":"Computational Science and Engineering","5":"Mathematical Applications in Computer Science","6":"Communications Engineering, Networks"},"secondarySubjectCodes":{"1":"I18024","2":"I18032","3":"I18030","4":"M14026","5":"M13110","6":"T24035"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1186/s40537-016-0043-6","Page":"article","page":{"attributes":{"environment":"live"}}}];
    </script>


    
    
        
            <script src=/oscar-static/js/jquery-220afd743d.js></script>
        
    

    <script data-test="onetrust-control">
        
            (function(w,d,t) {
                var assetPath = '/oscar-static/js/cookie-consent-es5-bundle-0ea0aa3601.js';
                function cc() {
                    var h = w.location.hostname,
                        e = d.createElement(t),
                        s = d.getElementsByTagName(t)[0];

                    if (h === "link.springer.com") {
                        e.src = "https://cdn.cookielaw.org/scripttemplates/otSDKStub.js";
                        e.setAttribute("data-domain-script", "4f53bc14-4ee3-45bd-9935-e3d2b6b2a543");
                    } else {
                        e.src = assetPath;
                        e.setAttribute("data-consent", h);
                    }
                    s.parentNode.insertBefore(e, s);
                }
                w.google_tag_manager ? cc() : window.addEventListener("gtm_loaded", cc);
            })(window,document,"script");
        
    </script>
    <script>
        function OptanonWrapper() {
            var elementInside = function(candidate, element) {
                if (candidate === element) {
                    return true;
                } else if (candidate.nodeName.toLowerCase() === 'body') {
                    return false;
                } else {
                    return elementInside(candidate.parentNode, element);
                }
            };

            var disclaimer = document.querySelector('.c-disclaimer[aria-hidden="false"]');
            window.dataLayer.push({event:'OneTrustGroupsUpdated'});
            if (disclaimer) {
                if (!elementInside(document.activeElement, disclaimer)) {
                    disclaimer.querySelector('button').focus();
                }
            } else {
                document.activeElement.blur();
            }
        }
    </script>

    <script>
    window.config = window.config || {};
    window.config.mustardcut = false;

    
    if (window.matchMedia && window.matchMedia('only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)').matches) {
        window.config.mustardcut = true;
    }
</script>

    <!--Polyfills CustomEvent constructor in IE. Allows us to use events to manage race conditions in client side js-->
<script>
    (function () {
        if ( typeof window.CustomEvent === "function" ) return false;
        function CustomEvent ( event, params ) {
            params = params || { bubbles: false, cancelable: false, detail: null };
            var evt = document.createEvent( 'CustomEvent' );
            evt.initCustomEvent( event, params.bubbles, params.cancelable, params.detail );
            return evt;
        }

        CustomEvent.prototype = window.Event.prototype;

        window.CustomEvent = CustomEvent;
    })();
</script>

    
    
        
            <!-- Google Tag Manager -->
            <script data-test="gtm-head">
                if (window.config.mustardcut) {
                    (function (w, d, s, l, i) {
                        w[l] = w[l] || [];
                        w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                        var f = d.getElementsByTagName(s)[0],
                                j = d.createElement(s),
                                dl = l != 'dataLayer' ? '&l=' + l : '';
                        j.async = true;
                        j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                        
                        j.addEventListener('load', function() {
                            var _ge = new CustomEvent('gtm_loaded', { bubbles: true });
                            d.dispatchEvent(_ge);
                        });
                        f.parentNode.insertBefore(j, f);
                    })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
                }
            </script>
            <!-- End Google Tag Manager -->
        
    


    <script class="js-entry">
    if (window.config.mustardcut) {
        (function(w, d) {
            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                {'src': '/oscar-static/js/polyfill-es5-bundle-974eb189f7.js', 'async': false},
                {'src': '/oscar-static/js/airbrake-es5-bundle-90aefb6023.js', 'async': false},
            ];

            var bodyScripts = [
                {'src': '/oscar-static/js/app-es5-bundle-05e3d0b21b.js', 'async': false, 'module': false},
                {'src': '/oscar-static/js/app-es6-bundle-8d5be091e0.js', 'async': false, 'module': true}
                
                
                    , {'src': '/oscar-static/js/global-article-es5-bundle-20559bc353.js', 'async': false, 'module': false},
                    {'src': '/oscar-static/js/global-article-es6-bundle-644b2aa152.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i = 0; i < headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i = 0; i < bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        })(window, document);
    }
</script>

    
    
    <link rel="canonical" href="//journalofbigdata.springeropen.com/article/10.1186/s40537-016-0043-6"/>
    

</head>
<body class="shared-article-renderer">
    
    
    
        
            <!-- Google Tag Manager (noscript) -->
            <noscript data-test="gtm-body">
                <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
                height="0" width="0" style="display:none;visibility:hidden"></iframe>
            </noscript>
            <!-- End Google Tag Manager (noscript) -->
        
    


    <div class="u-vh-full">
        
    <div class="u-hide u-show-following-ad"></div>
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
                <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/40537/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=43;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="u-icon u-flex-static u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a
                        data-test="login-link"
                        class="c-header__link"
                        href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1186%2Fs40537-016-0043-6"
                        data-track="click"
                        data-track-category="header"
                        data-track-action="login header"
                        data-track-label="link">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="u-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            A survey of transfer learning
                        </div>
                        
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1186/s40537-016-0043-6.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button" data-track-external  download>
            
                <span>Download PDF</span>
                <svg aria-hidden="true" focusable="false" width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>

                    </div>
                </div>
            

            <div class="c-pdf-button__container">
                
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1186/s40537-016-0043-6.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button" data-track-external  download>
            
                <span>Download PDF</span>
                <svg aria-hidden="true" focusable="false" width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>

            </div>

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
        <li class="c-article-identifiers__item" data-test="article-category">Survey paper</li>
    
    
        <li class="c-article-identifiers__item">
            <span class="c-article-identifiers__open" data-test="open-access">Open Access</span>
        </li>
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2016-05-28" itemprop="datePublished">28 May 2016</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">A survey of transfer learning</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Karl-Weiss" data-author-popup="auth-Karl-Weiss" data-corresp-id="c1">Karl Weiss<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Florida Atlantic University" /><meta itemprop="address" content="grid.255951.f, 0000000406350263, Florida Atlantic University, 777 Glades Road, Boca Raton, FL, 33431, USA" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Taghi_M_-Khoshgoftaar" data-author-popup="auth-Taghi_M_-Khoshgoftaar">Taghi M. Khoshgoftaar</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Florida Atlantic University" /><meta itemprop="address" content="grid.255951.f, 0000000406350263, Florida Atlantic University, 777 Glades Road, Boca Raton, FL, 33431, USA" /></span></sup> &amp; </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-DingDing-Wang" data-author-popup="auth-DingDing-Wang">DingDing Wang</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Florida Atlantic University" /><meta itemprop="address" content="grid.255951.f, 0000000406350263, Florida Atlantic University, 777 Glades Road, Boca Raton, FL, 33431, USA" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/40537"><i data-test="journal-title">Journal of Big Data</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 3</b>, Article number: <span data-test="article-number">9</span> (<span data-test="article-publication-year">2016</span>)
            <a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    <div class="c-article-metrics-bar__wrapper u-clear-both">
        <ul class="c-article-metrics-bar u-list-reset">
            
                <li class=" c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__count">82k <span class="c-article-metrics-bar__label">Accesses</span></p>
                </li>
            
            
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__count">637 <span class="c-article-metrics-bar__label">Citations</span></p>
                </li>
            
            
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">41 <span class="c-article-metrics-bar__label">Altmetric</span></p>
                    </li>
                
            
            <li class="c-article-metrics-bar__item">
                <p class="c-article-metrics-bar__details"><a href="/article/10.1186%2Fs40537-016-0043-6/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
            </li>
        </ul>
    </div>
</div>

                        </div>
                            
    

    

                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" data-title="Abstract" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>Machine learning and data mining techniques have been used in numerous real-world applications. An assumption of traditional machine learning methodologies is the training data and testing data are taken from the same domain, such that the input feature space and data distribution characteristics are the same. However, in some real-world machine learning scenarios, this assumption does not hold. There are cases where training data is expensive or difficult to collect. Therefore, there is a need to create high-performance learners trained with more easily obtained data from different domains. This methodology is referred to as transfer learning. This survey paper formally defines transfer learning, presents information on current solutions, and reviews applications applied to transfer learning. Lastly, there is information listed on software downloads for various transfer learning solutions and a discussion of possible future research work. The transfer learning solutions surveyed are independent of data size and can be applied to big data environments.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec1" data-title="Background"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Background</h2><div class="c-article-section__content" id="Sec1-content"><p>The field of data mining and machine learning has been widely and successfully used in many applications where patterns from past information (training data) can be extracted in order to predict future outcomes [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 129" title="Witten IH, Frank E. Data mining, practical machine learning tools and techniques. 3rd ed. San Francisco: Morgan Kaufmann Publishers; 2011." href="/article/10.1186/s40537-016-0043-6#ref-CR129" id="ref-link-section-d43522e323">129</a>]. Traditional machine learning is characterized by training data and testing data having the same input feature space and the same data distribution. When there is a difference in data distribution between the training data and test data, the results of a predictive learner can be degraded [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 107" title="Shimodaira H. Improving predictive inference under covariate shift by weighting the log-likelihood function. J Stat Plan Inf. 2000;90(2):227–44." href="/article/10.1186/s40537-016-0043-6#ref-CR107" id="ref-link-section-d43522e326">107</a>]. In certain scenarios, obtaining training data that matches the feature space and predicted data distribution characteristics of the test data can be difficult and expensive. Therefore, there is a need to create a high-performance learner for a target domain trained from a related source domain. This is the motivation for transfer learning.</p><p>Transfer learning is used to improve a learner from one domain by transferring information from a related domain. We can draw from real-world non-technical experiences to understand why transfer learning is possible. Consider an example of two people who want to learn to play the piano. One person has no previous experience playing music, and the other person has extensive music knowledge through playing the guitar. The person with an extensive music background will be able to learn the piano in a more efficient manner by transferring previously learned music knowledge to the task of learning to play the piano [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 84" title="Pan SJ, Yang Q. A survey on transfer learning. IEEE Trans Knowl Data Eng. 2010;22(10):1345–59." href="/article/10.1186/s40537-016-0043-6#ref-CR84" id="ref-link-section-d43522e332">84</a>]. One person is able to take information from a previously learned task and use it in a beneficial way to learn a related task.</p><p>Looking at a concrete example from the domain of machine learning, consider the task of predicting text sentiment of product reviews where there exists an abundance of labeled data from digital camera reviews. If the training data and the target data are both derived from digital camera reviews, then traditional machine learning techniques are used to achieve good prediction results. However, in the case where the training data is from digital camera reviews and the target data is from food reviews, then the prediction results are likely to degrade due to the differences in domain data. Digital camera reviews and food reviews still have a number of characteristics in common, if not exactly the same. They both are written in textual form using the same language, and they both express views about a purchased product. Because these two domains are related, transfer learning can be used to potentially improve the results of a target learner [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 84" title="Pan SJ, Yang Q. A survey on transfer learning. IEEE Trans Knowl Data Eng. 2010;22(10):1345–59." href="/article/10.1186/s40537-016-0043-6#ref-CR84" id="ref-link-section-d43522e338">84</a>]. An alternative way to view the data domains in a transfer learning environment is that the training data and the target data exist in different sub-domains linked by a high-level common domain. For example, a piano player and a guitar player are subdomains of a musician domain. Further, a digital camera review and a food review are subdomains of a review domain. The high-level common domain determines how the subdomains are related.</p><p>As previously mentioned, the need for transfer learning occurs when there is a limited supply of target training data. This could be due to the data being rare, the data being expensive to collect and label, or the data being inaccessible. With big data repositories becoming more prevalent, using existing datasets that are related to, but not exactly the same as, a target domain of interest makes transfer learning solutions an attractive approach. There are many machine learning applications that transfer learning has been successfully applied to including text sentiment classification [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 121" title="Wang C, Mahadevan S. Heterogeneous domain adaptation using manifold alignment. In: Proceedings of the 22nd international joint conference on artificial intelligence, vol. 2. 2011. p. 541–46." href="/article/10.1186/s40537-016-0043-6#ref-CR121" id="ref-link-section-d43522e344">121</a>], image classification [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 30" title="Duan L, Xu D, Tsang IW. Learning with augmented features for heterogeneous domain adaptation. IEEE Trans Pattern Anal Mach Intell. 2012;36(6):1134–48." href="/article/10.1186/s40537-016-0043-6#ref-CR30" id="ref-link-section-d43522e347">30</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 58" title="Kulis B, Saenko K, Darrell T. What you saw is not what you get: domain adaptation using asymmetric kernel transforms. In: IEEE 2011 conference on computer vision and pattern recognition. 2011. p. 1785–92." href="/article/10.1186/s40537-016-0043-6#ref-CR58" id="ref-link-section-d43522e350">58</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 146" title="Zhu Y, Chen Y, Lu Z, Pan S, Xue G, Yu Y, Yang Q. Heterogeneous transfer learning for image classification. In: Proceedings of the national conference on artificial intelligence, vol. 2. 2011. p. 1304–9." href="/article/10.1186/s40537-016-0043-6#ref-CR146" id="ref-link-section-d43522e353">146</a>], human activity classification [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 46" title="Harel M, Mannor S. Learning from multiple outlooks. In: Proceedings of the 28th international conference on machine learning. 2011. p. 401–8." href="/article/10.1186/s40537-016-0043-6#ref-CR46" id="ref-link-section-d43522e356">46</a>], software defect classification [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 77" title="Nam J, Kim S (2015) Heterogeneous defect prediction. In: Proceedings of the 2015 10th joint meeting on foundations of software engineering. 2015. p. 508–19." href="/article/10.1186/s40537-016-0043-6#ref-CR77" id="ref-link-section-d43522e360">77</a>], and multi-language text classification [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 145" title="Zhou JT, Tsang IW, Pan SJ Tan M. Heterogeneous domain adaptation for multiple classes. In: International conference on artificial intelligence and statistics. 2014. p. 1095–103." href="/article/10.1186/s40537-016-0043-6#ref-CR145" id="ref-link-section-d43522e363">145</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 91" title="Prettenhofer P, Stein B. (2010) Cross-language text classification using structural correspondence learning. In: Proceedings of the 48th annual meeting of the association for computational linguistics. 2010. p. 1118–27." href="/article/10.1186/s40537-016-0043-6#ref-CR91" id="ref-link-section-d43522e366">91</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 144" title="Zhou JT, Pan S, Tsang IW, Yan Y. Hybrid heterogeneous transfer learning through deep learning. In: Proceedings of the national conference on artificial intelligence, vol. 3. 2014. p. 2213–20." href="/article/10.1186/s40537-016-0043-6#ref-CR144" id="ref-link-section-d43522e369">144</a>].</p><p>This survey paper aims to provide a researcher interested in transfer learning with an overview of related works, examples of applications that are addressed by transfer learning, and issues and solutions that are relevant to the field of transfer learning. This survey paper provides an overview of current methods being used in the field of transfer learning as it pertains to data mining tasks for classification, regression, and clustering problems; however, it does not focus on transfer learning for reinforcement learning (for more information on reinforcement learning see Taylor [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 112" title="Taylor ME, Stone P. Transfer learning for reinforcement learning domains: a survey. JMLR. 2009;10:1633–85." href="/article/10.1186/s40537-016-0043-6#ref-CR112" id="ref-link-section-d43522e376">112</a>]). Information pertaining to the history and taxonomy of transfer learning is not provided in this survey paper, but can be found in the paper by Pan [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 84" title="Pan SJ, Yang Q. A survey on transfer learning. IEEE Trans Knowl Data Eng. 2010;22(10):1345–59." href="/article/10.1186/s40537-016-0043-6#ref-CR84" id="ref-link-section-d43522e379">84</a>]. Since the publication of the transfer learning survey paper by Pan [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 84" title="Pan SJ, Yang Q. A survey on transfer learning. IEEE Trans Knowl Data Eng. 2010;22(10):1345–59." href="/article/10.1186/s40537-016-0043-6#ref-CR84" id="ref-link-section-d43522e382">84</a>] in 2010, there have been over 700 academic papers written addressing advancements and innovations on the subject of transfer learning. These works broadly cover the areas of new algorithm development, improvements to existing transfer learning algorithms, and algorithm deployment in new application domains. The selected surveyed works in this paper are meant to be diverse and representative of transfer learning solutions in the past 5 years. Most of the surveyed papers provide a generic transfer learning solution; however, some surveyed papers provide solutions that are specific to individual applications. This paper is written with the assumption the reader has a working knowledge of machine learning. For more information on machine learning see Witten [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 129" title="Witten IH, Frank E. Data mining, practical machine learning tools and techniques. 3rd ed. San Francisco: Morgan Kaufmann Publishers; 2011." href="/article/10.1186/s40537-016-0043-6#ref-CR129" id="ref-link-section-d43522e385">129</a>]. The surveyed works in this paper are intended to present a high-level description of proposed solutions with unique and salient points being highlighted. Experiments from the surveyed papers are described with respect to applied applications, other competing solutions tested, and overall relative results of the experiments. This survey paper provides a section on heterogeneous transfer learning which, to the best of our knowledge, is unique. Additionally, a list of software downloads for various surveyed papers is provided, which is unique to this paper.</p><p>The remainder of this paper is organized as follows. “<a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1186/s40537-016-0043-6#Sec2">Definitions of transfer learning</a>” section provides definitions and notations of transfer learning. “<a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1186/s40537-016-0043-6#Sec3">Homogeneous transfer learning</a>” and “<a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1186/s40537-016-0043-6#Sec7">Heterogeneous transfer learning</a>” sections provide solutions on homogeneous and heterogeneous transfer learning, “<a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1186/s40537-016-0043-6#Sec17">Negative transfer</a>” section provides information on negative transfer as it pertains to transfer learning. “Transfer learning application” section provides examples of transfer learning applications. “<a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1186/s40537-016-0043-6#Sec19">Conclusion and discussion</a>” section summarizes and discusses potential future research work. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1186/s40537-016-0043-6#Sec1">Appendix</a> provides information on software downloads for transfer learning.</p></div></div></section><section aria-labelledby="Sec2" data-title="Definitions of transfer learning"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">Definitions of transfer learning</h2><div class="c-article-section__content" id="Sec2-content"><p>The following section lists the notation and definitions used for the remainder of this paper. The notation and definitions in this section match those from the survey paper by Pan [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 84" title="Pan SJ, Yang Q. A survey on transfer learning. IEEE Trans Knowl Data Eng. 2010;22(10):1345–59." href="/article/10.1186/s40537-016-0043-6#ref-CR84" id="ref-link-section-d43522e418">84</a>], if present in both papers, to maintain consistency across both surveys. To provide illustrative examples of the definitions listed below, a machine learning application of software module defect classification is used where a learner is trained to predict whether a software module is defect prone or not.</p><p>A domain <span class="mathjax-tex">\({\mathcal{D}}\)</span> is defined by two parts, a feature space <span class="mathjax-tex">\({\mathcal{X}}\)</span> and a marginal probability distribution P(X), where <span class="mathjax-tex">\({\text{X}} = \left\{ {{\text{x}}_{1} \, ,\ldots ,\,{\text{x}}_{\text{n}} } \right\} \in {\mathcal{X}}\)</span>. For example, if the machine learning application is software module defect classification and each software metric is taken as a feature, then x<sub>i</sub> is the i-th feature vector (instance) corresponding to the i-th software module, n is the number of feature vectors in X, <span class="mathjax-tex">\({\mathcal{X}}\)</span> is the space of all possible feature vectors, and X is a particular learning sample. For a given domain <span class="mathjax-tex">\({\mathcal{D}}\)</span>, a task <span class="mathjax-tex">\({\mathcal{T}}\)</span> is defined by two parts, a label space <span class="mathjax-tex">\({\mathcal{Y}}\)</span>, and a predictive function f(·), which is learned from the feature vector and label pairs {x<sub>i</sub>, y<sub>i</sub>} where <span class="mathjax-tex">\({\text{x}}_{\text{i}} \in {\text{X}}\)</span> and <span class="mathjax-tex">\({\text{y}}_{\text{i}} \in {\mathcal{Y}}\)</span>. Referring to the software module defect classification application, <span class="mathjax-tex">\({\mathcal{Y}}\)</span> is the set of labels and in this case contains true and false, y<sub>i</sub> takes on a value of true or false, and f(x) is the learner that predicts the label value for the software module x. From the definitions above, a domain <span class="mathjax-tex">\({\mathcal{D}} = \left\{ {{\mathcal{X}},\,{\text{P}}\left( {\text{X}} \right)} \right\}\)</span> and a task <span class="mathjax-tex">\({\mathcal{T}} = \left\{ {{\mathcal{Y}},f( \cdot )} \right\}\)</span>. Now, D<sub>S</sub> is defined as the source domain data where <span class="mathjax-tex">\({\text{D}}_{\text{S}} = \left\{ {\left( {{\text{x}}_{\text{S1}} ,{\text{y}}_{\text{S1}} } \right) \ldots ,\left( {{\text{x}}_{\text{Sn}} ,{\text{y}}_{\text{Sn}} } \right)} \right\}\)</span>, where <span class="mathjax-tex">\({\text{x}}_{\text{Si}} \in {\mathcal{X}}_{S}\)</span> is the ith data instance of D<sub>S</sub> and <span class="mathjax-tex">\({\text{y}}_{\text{Si}} \in {\mathcal{Y}}_{S}\)</span> is the corresponding class label for x<sub>Si</sub>. In the same way, D<sub>T</sub> is defined as the target domain data where <span class="mathjax-tex">\({\text{D}}_{\text{T}} = \left\{ {\left( {{\text{x}}_{\text{T1}} ,{\text{y}}_{\text{T1}} } \right) \ldots ,\left( {{\text{x}}_{\text{Tn}} ,{\text{y}}_{\text{Tn}} } \right)} \right\}\)</span>, where <span class="mathjax-tex">\({\text{x}}_{\text{Ti}} \in {\mathcal{X}}_{{\mathcal{T}}}\)</span> is the ith data instance of D<sub>T</sub> and <span class="mathjax-tex">\({\text{y}}_{\text{Ti}} , \in {\mathcal{Y}}_{{\mathcal{T}}}\)</span> is the corresponding class label for x<sub>Ti</sub>. Further, the source task is notated as <span class="mathjax-tex">\({\mathcal{T}}_{{\mathcal{S}}}\)</span>, the target task as <span class="mathjax-tex">\({\mathcal{T}}_{{\mathcal{T}}}\)</span>, the source predictive function as f<sub>S</sub>(·), and the target predictive function as f<sub>T</sub>(·).</p><p>Transfer learning is now formally defined. Given a source domain <span class="mathjax-tex">\({\mathcal{D}}_{\text{S}}\)</span> with a corresponding source task <span class="mathjax-tex">\({\mathcal{T}}_{{\mathcal{S}}}\)</span> and a target domain <span class="mathjax-tex">\({\mathcal{D}}_{\text{T}}\)</span> with a corresponding task <span class="mathjax-tex">\({\mathcal{T}}_{{\mathcal{T}}}\)</span>, transfer learning is the process of improving the target predictive function f<sub>T</sub>(∙) by using the related information from <span class="mathjax-tex">\({\mathcal{D}}_{\text{S}}\)</span> and <span class="mathjax-tex">\({\mathcal{T}}_{{\mathcal{S}}}\)</span>, where <span class="mathjax-tex">\({\mathcal{D}}_{\text{S}} \, \ne {\mathcal{D}}_{\text{T}}\)</span> or <span class="mathjax-tex">\({\mathcal{T}}_{{\mathcal{S}}} \, \ne \,{\mathcal{T}}_{{\mathcal{T}}}\)</span>. The single source domain defined here can be extended to multiple source domains. Given the definition of transfer learning, since <span class="mathjax-tex">\({\mathcal{D}}_{\text{S}} = \left\{ {{\mathcal{X}}_{{\mathcal{S}}} ,\,{\text{P}}\left( {{\text{X}}_{\text{S}} } \right)} \right\}\)</span> and <span class="mathjax-tex">\({\mathcal{D}}_{\text{T}} = \left\{ {{\mathcal{X}}_{{\mathcal{T}}} ,\,{\text{P}}\left( {{\text{X}}_{\text{T}} } \right)} \right\}\)</span>, the condition where <span class="mathjax-tex">\({\mathcal{D}}_{\text{S}} \, \ne {\mathcal{D}}_{\text{T}}\)</span> means that <span class="mathjax-tex">\({\mathcal{X}}_{S} \, \ne {\mathcal{X}}_{{\mathcal{T}}}\)</span> and/or <span class="mathjax-tex">\({\text{P}}({\text{X}}_{\text{S}} )\,\, \ne \,{\text{P}}\left( {{\text{X}}_{\text{T}} } \right)\text{ }\)</span>. The case where <span class="mathjax-tex">\({\mathcal{X}}_{{\mathcal{S}}} \, \ne {\mathcal{X}}_{{\mathcal{T}}}\)</span> with respect to transfer learning is defined as heterogeneous transfer learning. The case where <span class="mathjax-tex">\({\mathcal{X}}_{{\mathcal{S}}} \, = {\mathcal{X}}_{{\mathcal{T}}}\)</span> with respect to transfer learning is defined as homogeneous transfer learning. Going back to the example of software module defect classification, heterogeneous transfer learning is the case where the source software project has different metrics (features) than the target software project. Alternatively, homogeneous transfer learning is when the software metrics are the same for both the source and the target software projects. Continuing with the definition of transfer learning, the case where <span class="mathjax-tex">\({\text{P}}({\text{X}}_{\text{S}} )\, \ne \,{\text{P}}\left( {{\text{X}}_{\text{T}} } \right)\text{ }\)</span> means the marginal distributions in the input spaces are different between the source and the target domains. Shimodaira [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 107" title="Shimodaira H. Improving predictive inference under covariate shift by weighting the log-likelihood function. J Stat Plan Inf. 2000;90(2):227–44." href="/article/10.1186/s40537-016-0043-6#ref-CR107" id="ref-link-section-d43522e1830">107</a>] demonstrated that a learner trained with a given source domain will not perform optimally on a target domain when the marginal distributions of the input domains are different. Referring to the software module defect classification application, an example of marginal distribution differences is when the source software program is written for a user interface system and the target software program is written for DSP signaling decoder algorithm. Another possible condition of transfer learning (from the definition above) is <span class="mathjax-tex">\({\mathcal{T}}_{{\mathcal{S}}} \, \ne \,{\mathcal{T}}_{{\mathcal{T}}}\)</span>, and it was stated that <span class="mathjax-tex">\({\mathcal{T}} = \left\{ {{\mathcal{Y}},f( \cdot )} \right\}\)</span> or to rewrite this, <span class="mathjax-tex">\({\mathcal{T}} = \left\{ {{\mathcal{Y}},{\text{P}}\left( {{\text{Y}}\left| {\text{X}} \right.} \right)} \right\}\)</span>. Therefore, in a transfer learning environment, it is possible that <span class="mathjax-tex">\({\mathcal{Y}}_{{\mathcal{S}}} \, \ne \,{\mathcal{Y}}_{{\mathcal{T}}}\)</span> and/or <span class="mathjax-tex">\({\text{P}}(\left. {{\text{Y}}_{\text{S}} } \right|{\text{X}}_{\text{S}} )\, \ne \,{\text{P}}\left( {{\text{Y}}_{\text{T}} \left| {{\text{X}}_{\text{T}} } \right.} \right)\text{ }\)</span>. The case where <span class="mathjax-tex">\({\text{P}}(\left. {{\text{Y}}_{\text{S}} } \right|{\text{X}}_{\text{S}} )\, \ne \,{\text{P}}\left( {{\text{Y}}_{\text{T}} \left| {{\text{X}}_{\text{T}} } \right.} \right)\text{ }\)</span> means the conditional probability distributions between the source and target domains are different. An example of a conditional distribution mismatch is when a particular software module yields different fault prone results in the source and target domains. The case of <span class="mathjax-tex">\({\mathcal{Y}}_{{\mathcal{S}}} \, \ne \,{\mathcal{Y}}_{{\mathcal{T}}}\)</span> refers to a mismatch in the class space. An example of this case is when the source software project has a binary label space of true for defect prone and false for not defect prone, and the target domain has a label space that defines five levels of fault prone modules. Another case that can cause discriminative classifier degradation is when <span class="mathjax-tex">\({\text{P}}({\text{Y}}_{\text{s}} )\, \ne \,{\text{P}}\left( {{\text{Y}}_{\text{T}} } \right)\)</span>, which is caused by an unbalanced labeled data set between the source and target domains. The case of traditional machine learning is <span class="mathjax-tex">\({\mathcal{D}}_{\text{S}} = {\mathcal{D}}_{\text{T}}\)</span> and <span class="mathjax-tex">\({\mathcal{T}}_{{\mathcal{S}}} \,\, = {\mathcal{T}}_{{\mathcal{T}}}\)</span>. The common notation used in this paper is summarized in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1186/s40537-016-0043-6#Tab1">1</a>.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-1"><figure><figcaption class="c-article-table__figcaption"><b id="Tab1" data-test="table-caption">Table 1 Summary of commonly used notation</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1186/s40537-016-0043-6/tables/1"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                     <p>To elaborate on the distribution issues that can occur between the source and target domains, the application of natural language processing is used to illustrate. In natural language processing, text instances are often modeled as a bag-of-words where a unique word represents a feature. Consider the example of review text where the source covers movie reviews and the target covers book reviews. Words that are generic and domain independent should occur at a similar rate in both domains. However, words that are domain specific are used more frequently in one domain because of the strong relationship with that domain topic. This is referred to as frequency feature bias and will cause the marginal distribution between the source and target domains to be different <span class="mathjax-tex">\(\left( {{\text{P}}({\text{X}}_{\text{S}} )\, \ne \,{\text{P}}\left( {{\text{X}}_{\text{T}} } \right)} \right)\text{ }\)</span>. Another form of bias is referred to as context feature bias and this will cause the conditional distributions to be different between the source and target domains <span class="mathjax-tex">\({\text{P}}(\left. {{\text{Y}}_{\text{S}} } \right|{\text{X}}_{\text{S}} )\, \ne \,{\text{P}}\left( {{\text{Y}}_{\text{T}} \left| {{\text{X}}_{\text{T}} } \right.} \right)\text{ }\)</span>. An example of context feature bias is when a word can have different meanings in two domains. A specific example is the word “monitor” where in one domain it is used as a noun and in another domain it is used as a verb. Another example of context feature bias is with sentiment classification when a word has a positive meaning in one domain and a negative meaning in another domain. The word “small” can have a good meaning if describing a cell phone but a bad meaning if describing a hotel room. A further example of context feature bias is demonstrated in the case of document sentiment classification of reviews where the source domain contains reviews of one product written in German and the target domain contains reviews of a different product written in English. The translated words from the source document may not accurately represent the actual words used in the target documents. An example is the case of the German word “betonen”, which translates to the English word “emphasize” by Google translator. However, in the target documents the corresponding English word used is “highlight” (Zhou [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 144" title="Zhou JT, Pan S, Tsang IW, Yan Y. Hybrid heterogeneous transfer learning through deep learning. In: Proceedings of the national conference on artificial intelligence, vol. 3. 2014. p. 2213–20." href="/article/10.1186/s40537-016-0043-6#ref-CR144" id="ref-link-section-d43522e2759">144</a>]).</p><p>Negative transfer, with regards to transfer learning, occurs when the information learned from a source domain has a detrimental effect on a target learner. More formally, given a source domain <span class="mathjax-tex">\({\mathcal{D}}_{{\mathcal{S}}}\)</span>, a source task <span class="mathjax-tex">\({\mathcal{T}}_{{\mathcal{S}}}\)</span>, a target domain <span class="mathjax-tex">\({\mathcal{D}}_{{\mathcal{T}}}\)</span>, a target task <span class="mathjax-tex">\({\mathcal{T}}_{{\mathcal{T}}}\)</span>, a predictive learner f<sub>T1</sub>(·) trained only with <span class="mathjax-tex">\({\mathcal{D}}_{{\mathcal{T}}}\)</span>, and a predictive learner f<sub>T2</sub>(·) trained with a transfer learning process combining <span class="mathjax-tex">\({\mathcal{D}}_{{\mathcal{T}}}\)</span> and <span class="mathjax-tex">\({\mathcal{D}}_{{\mathcal{S}}}\)</span>, negative transfer occurs when the performance of f<sub>T1</sub>(·) is greater than the performance of f<sub>T2</sub>(·). The topic of negative transfer addresses the need to quantify the amount of relatedness between the source domain and the target domain and whether an attempt to transfer knowledge from the source domain should be made. Extending the definition above, positive transfer occurs when the performance of f<sub>T2</sub>(·) is greater than the performance of f<sub>T1</sub>(·).</p><p>Throughout the literature on transfer learning, there are a number of terminology inconsistencies. Phrases such as transfer learning and domain adaptation are used to refer to similar processes. The following definitions will be used in this paper. Domain adaptation, as it pertains to transfer learning, is the process of adapting one or more source domains for the means of transferring information to improve the performance of a target learner. The domain adaptation process attempts to alter a source domain in an attempt to bring the distribution of the source closer to that of the target. Another area of literature inconsistencies is in characterizing the transfer learning process with respect to the availability of labeled and unlabeled data. For example, Daumé [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 22" title="Daumé H III. Frustratingly easy domain adaptation. In: Proceedings of ACL. 2007. p. 256–63." href="/article/10.1186/s40537-016-0043-6#ref-CR22" id="ref-link-section-d43522e2965">22</a>] and Chattopadhyay [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="Chattopadhyay R, Ye J, Panchanathan S, Fan W, Davidson I. Multi-source domain adaptation and its application to early detection of fatigue. ACM Trans Knowl Dis Data (Best of SIGKDD 2011 TKDD Homepage archive) 2011; 6(4) (Article 18)." href="/article/10.1186/s40537-016-0043-6#ref-CR14" id="ref-link-section-d43522e2968">14</a>] define supervised transfer learning as the case of having abundant labeled source data and limited labeled target data, and semi-supervised transfer learning as the case of abundant labeled source data and no labeled target data. In Gong [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 42" title="Gong B, Shi Y, Sha F, Grauman K. Geodesic flow kernel for unsupervised domain adaptation. In: Proceedings of the 2012 IEEE conference on computer vision and pattern recognition. 2012. p. 2066–73." href="/article/10.1186/s40537-016-0043-6#ref-CR42" id="ref-link-section-d43522e2971">42</a>] and Blitzer [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 5" title="Blitzer, J, McDonald R, Pereira F. Domain adaptation with structural correspondence learning. In: Proceedings of the 2006 conference on empirical methods in natural language processing. 2006;120–8." href="/article/10.1186/s40537-016-0043-6#ref-CR5" id="ref-link-section-d43522e2974">5</a>], semi-supervised transfer learning is the case of having abundant labeled source data and limited labeled target data, and unsupervised transfer learning is the case of abundant labeled source data and no labeled target data. Cook [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 19" title="Cook DJ, Feuz KD, Krishnan NC. Transfer learning for activity recognition: a survey. Knowl Inf Syst. 2012;36(3):537–56." href="/article/10.1186/s40537-016-0043-6#ref-CR19" id="ref-link-section-d43522e2977">19</a>] and Feuz [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 36" title="Feuz KD, Cook DJ. Transfer learning across feature-rich heterogeneous feature spaces via feature-space remapping (FSR). J ACM Trans Intell Syst Technol. 2014;6(1):1–27 (Article 3)." href="/article/10.1186/s40537-016-0043-6#ref-CR36" id="ref-link-section-d43522e2981">36</a>] provide a different variation where the definition of supervised or unsupervised refers to the presence or absence of labeled data in the source domain and informed or uninformed refers to the presence or absence of labeled data in the target domain. With this definition, a labeled source and limited labeled target domain is referred to as informed supervised transfer learning. Pan [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 84" title="Pan SJ, Yang Q. A survey on transfer learning. IEEE Trans Knowl Data Eng. 2010;22(10):1345–59." href="/article/10.1186/s40537-016-0043-6#ref-CR84" id="ref-link-section-d43522e2984">84</a>] refers to inductive transfer learning as the case of having available labeled target domain data, transductive transfer learning as the case of having labeled source and no labeled target domain data, and unsupervised transfer learning as the case of having no labeled source and no labeled target domain data. This paper will explicitly state when labeled and unlabeled data are being used in the source and target domains.</p><p>There are different strategies and implementations for solving a transfer learning problem. The majority of the homogeneous transfer learning solutions employ one of three general strategies which include trying to correct for the marginal distribution difference in the source, trying to correct for the conditional distribution difference in the source, or trying to correct both the marginal and conditional distribution differences in the source. The majority of the heterogeneous transfer learning solutions are focused on aligning the input spaces of the source and target domains with the assumption that the domain distributions are the same. If the domain distributions are not equal, then further domain adaptation steps are needed. Another important aspect of a transfer learning solution is the form of information transfer (or what is being transferred). The form of information transfer is categorized into four general Transfer Categories [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 84" title="Pan SJ, Yang Q. A survey on transfer learning. IEEE Trans Knowl Data Eng. 2010;22(10):1345–59." href="/article/10.1186/s40537-016-0043-6#ref-CR84" id="ref-link-section-d43522e2990">84</a>]. The first Transfer Category is transfer learning through instances. A common method used in this case is for instances from the source domain to be reweighted in an attempt to correct for marginal distribution differences. These reweighted instances are then directly used in the target domain for training (examples in Huang [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 51" title="Huang J, Smola A, Gretton A, Borgwardt KM, Schölkopf B. Correcting sample selection bias by unlabeled data. In: Proceedings of the 2006 conference. Adv Neural Inf Process Syst. 2006. p. 601–8." href="/article/10.1186/s40537-016-0043-6#ref-CR51" id="ref-link-section-d43522e2993">51</a>], Jiang [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 53" title="Jiang J, Zhai C. Instance weighting for domain adaptation in NLP. In: Proceedings of the 45th annual meeting of the association of computational linguistics. 2007. p. 264–71." href="/article/10.1186/s40537-016-0043-6#ref-CR53" id="ref-link-section-d43522e2996">53</a>]). These reweighting algorithms work best when the conditional distribution is the same in both domains. The second Transfer Category is transfer learning through features. Feature-based transfer learning approaches are categorized in two ways. The first approach transforms the features of the source through reweighting to more closely match the target domain (e.g. Pan [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 82" title="Pan SJ, Kwok JT, Yang Q. Transfer learning via dimensionality reduction. In: Proceedings of the 23rd national conference on artificial intelligence, vol. 2. 2008. p. 677–82." href="/article/10.1186/s40537-016-0043-6#ref-CR82" id="ref-link-section-d43522e2999">82</a>]). This is referred to as asymmetric feature transformation and is depicted in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1186/s40537-016-0043-6#Fig1">1</a>b. The second approach discovers underlying meaningful structures between the domains to find a common latent feature space that has predictive qualities while reducing the marginal distribution between the domains (e.g. Blitzer [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 5" title="Blitzer, J, McDonald R, Pereira F. Domain adaptation with structural correspondence learning. In: Proceedings of the 2006 conference on empirical methods in natural language processing. 2006;120–8." href="/article/10.1186/s40537-016-0043-6#ref-CR5" id="ref-link-section-d43522e3006">5</a>]). This is referred to as symmetric feature transformation and is depicted in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1186/s40537-016-0043-6#Fig1">1</a>a. The third transfer category is to transfer knowledge through shared parameters of source and target domain learner models or by creating multiple source learner models and optimally combining the reweighted learners (ensemble learners) to form an improved target learner (examples in Gao [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 37" title="Gao J, Fan W, Jiang J, Han J (2008) Knowledge transfer via multiple model local structure mapping. In: Proceedings of the 14th ACM SIGKDD international conference on knowledge discovery and data mining. p. 283–91." href="/article/10.1186/s40537-016-0043-6#ref-CR37" id="ref-link-section-d43522e3012">37</a>], Bonilla [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 8" title="Bonilla E, Chai KM, Williams C. Multi-task Gaussian process prediction. In: Proceedings of the 20th annual conference of neural information processing systems. 2008. 153–60." href="/article/10.1186/s40537-016-0043-6#ref-CR8" id="ref-link-section-d43522e3015">8</a>], and Evgeniou [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 33" title="Evgeniou T, Pontil M (2004) Regularized multi-task learning. In: Proceedings of the 10th ACM SIGKDD international conference on knowledge discovery and data mining. p. 109–17." href="/article/10.1186/s40537-016-0043-6#ref-CR33" id="ref-link-section-d43522e3018">33</a>]). The last transfer category (and the least used approach) is to transfer knowledge based on some defined relationship between the source and target domains (examples in Mihalkova [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 74" title="Mihalkova L, Mooney RJ. Transfer learning by mapping with minimal target data. In: Proc. assoc. for the advancement of artificial intelligence workshop transfer learning for complex tasks. 2008. p. 31–6." href="/article/10.1186/s40537-016-0043-6#ref-CR74" id="ref-link-section-d43522e3021">74</a>] and Li [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 62" title="Li F, Pan SJ, Jin O, Yang Q, Zhu X. Cross-domain co-extraction of sentiment and topic lexicons. In: Proceedings of the 50th annual meeting of the association for computational linguistics long papers, vol. 1. 2012. p. 410–19." href="/article/10.1186/s40537-016-0043-6#ref-CR62" id="ref-link-section-d43522e3025">62</a>]).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1" data-title="Fig. 1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1186/s40537-016-0043-6/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1186%2Fs40537-016-0043-6/MediaObjects/40537_2016_43_Fig1_HTML.gif?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1186%2Fs40537-016-0043-6/MediaObjects/40537_2016_43_Fig1_HTML.gif" alt="figure1" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>
                                    <b>a</b> The symmetric transformation mapping (T<sub>S</sub> and T<sub>T</sub>) of the source (X<sub>S</sub>) and target (X<sub>T</sub>) domains into a common latent feature space. <b>b</b> The asymmetric transformation (T<sub>T</sub>) of the source domain (X<sub>S</sub>) to the target domain (X<sub>T</sub>)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1186/s40537-016-0043-6/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                     <p>Detailed information on specific transfer learning solutions are presented in “<a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1186/s40537-016-0043-6#Sec3">Homogeneous transfer learning</a>” “<a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1186/s40537-016-0043-6#Sec7">Heterogeneous transfer learning</a>” and “<a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1186/s40537-016-0043-6#Sec17">Negative transfer</a>” sections. These sections represent the majority of the works surveyed in this paper. “<a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1186/s40537-016-0043-6#Sec3">Homogeneous transfer learning</a>” “<a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1186/s40537-016-0043-6#Sec7">Heterogeneous transfer learning</a>” and “<a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1186/s40537-016-0043-6#Sec17">Negative transfer</a>” sections cover homogeneous transfer learning solutions, heterogeneous transfer learning solutions, and solutions addressing negative transfer, respectively. The section covering transfer learning applications focuses on the general applications that transfer learning is applied to, but does not describe the solution details.</p></div></div></section><section aria-labelledby="Sec3" data-title="Homogeneous transfer learning"><div class="c-article-section" id="Sec3-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec3">Homogeneous transfer learning</h2><div class="c-article-section__content" id="Sec3-content"><p>This section presents surveyed papers covering homogeneous transfer learning solutions and is divided into subsections that correspond to the transfer categories of instance-based, feature-based (both asymmetric and symmetric), parameter-based, and relational-based. Recall that homogeneous transfer learning is the case where <span class="mathjax-tex">\({\mathcal{X}}_{{\mathcal{S}}} = {\mathcal{X}}_{{\mathcal{T}}}\)</span>. The algorithms surveyed are summarized in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1186/s40537-016-0043-6#Tab2">2</a>.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-2"><figure><figcaption class="c-article-table__figcaption"><b id="Tab2" data-test="table-caption">Table 2 Homogeneous transfer learning approaches surveyed in “<a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1186/s40537-016-0043-6#Sec3">Homogeneous transfer learning</a>” section listing different characteristics of each approach</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1186/s40537-016-0043-6/tables/2"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                     <p>The methodology of homogeneous transfer learning is directly applicable to a big data environment. As repositories of big data become more available, there is a desire to use this abundant resource for machine learning tasks, avoiding the timely and potentially costly collection of new data. If there is an available dataset that is drawn from a domain that is related to, but does not an exactly match a target domain of interest, then homogeneous transfer learning can be used to build a predictive model for the target domain as long as the input feature space is the same.</p><h3 class="c-article__sub-heading" id="Sec4">Instance-based transfer learning</h3><p>The paper by Chattopadhyay [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="Chattopadhyay R, Ye J, Panchanathan S, Fan W, Davidson I. Multi-source domain adaptation and its application to early detection of fatigue. ACM Trans Knowl Dis Data (Best of SIGKDD 2011 TKDD Homepage archive) 2011; 6(4) (Article 18)." href="/article/10.1186/s40537-016-0043-6#ref-CR14" id="ref-link-section-d43522e3930">14</a>] proposes two separate solutions both using multiple labeled source domains. The first solution is the conditional probability based multi-source domain adaptation (CP-MDA) approach, which is a domain adaptation process based on correcting the conditional distribution differences between the source and target domains. The CP-MDA approach assumes a limited amount of labeled target data is available. The main idea is to use a combination of source domain classifiers to label the unlabeled target data. This is accomplished by first building a classifier for each separate source domain. Then a weight value is found for each classifier as a function of the closeness in conditional distribution between each source and the target domain. The weighted source classifiers are summed together to create a learning task that will find the pseudo labels (estimated labels later used for training) for the unlabeled target data. Finally, the target learner is built from the labeled and pseudo labeled target data. The second proposed solution is the two stage weighting framework for multi-source domain adaptation (2SW-MDA) which addresses both marginal and conditional distribution differences between the source and target domains. Labeled target data is not required for the 2SW-MDA approach; however, it can be used if available. In this approach, a weight for each source domain is computed based on the marginal distribution differences between the source and target domains. In the second step, the source domain weights are modified as a function of the difference in the conditional distribution as performed in the CP-MDA approach previously described. Finally, a target classifier is learned based on the reweighted source instances and any labeled target instances that are available. The work presented in Chattopadhyay [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="Chattopadhyay R, Ye J, Panchanathan S, Fan W, Davidson I. Multi-source domain adaptation and its application to early detection of fatigue. ACM Trans Knowl Dis Data (Best of SIGKDD 2011 TKDD Homepage archive) 2011; 6(4) (Article 18)." href="/article/10.1186/s40537-016-0043-6#ref-CR14" id="ref-link-section-d43522e3933">14</a>] is an extension of Duan [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 29" title="Duan L, Xu D, Tsang IW. Domain adaptation from multiple sources: a domain-dependent regularization approach. IEEE Trans Neural Netw Learn Syst. 2012;23(3):504–18." href="/article/10.1186/s40537-016-0043-6#ref-CR29" id="ref-link-section-d43522e3936">29</a>] where the novelty is in calculating the source weights as a function of conditional probability. Note, the 2SW-MDA approach is an example of an instance-based Transfer Category, but the CP-MDA approach is more appropriately classified as a parameter-based Transfer Category (see “<a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1186/s40537-016-0043-6#Sec3">Parameter-based transfer learning</a>” section). Experiments are performed for muscle fatigue classification using surface electromyography data where classification accuracy is measured as the performance metric. Each source domain represents one person’s surface electromyography measurements. A baseline approach is constructed using a support vector machine (SVM) classifier trained on the combination of seven sources used for this test. The transfer learning approaches that are tested against include an approach proposed by Huang [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 51" title="Huang J, Smola A, Gretton A, Borgwardt KM, Schölkopf B. Correcting sample selection bias by unlabeled data. In: Proceedings of the 2006 conference. Adv Neural Inf Process Syst. 2006. p. 601–8." href="/article/10.1186/s40537-016-0043-6#ref-CR51" id="ref-link-section-d43522e3942">51</a>], Pan [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 87" title="Pan SJ, Tsang IW, Kwok JT, Yang Q. Domain adaptation via transfer component analysis. IEEE Trans Neural Netw. 2009;22(2):199–210." href="/article/10.1186/s40537-016-0043-6#ref-CR87" id="ref-link-section-d43522e3946">87</a>], Zhong [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 143" title="Zhong E, Fan W, Peng J, Zhang K, Ren J, Turaga D, Verscheure O. Cross domain distribution adaptation via kernel mapping. In: Proceedings of the 15th ACM SIGKDD. 2009. p. 1027–36." href="/article/10.1186/s40537-016-0043-6#ref-CR143" id="ref-link-section-d43522e3949">143</a>], Gao [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 37" title="Gao J, Fan W, Jiang J, Han J (2008) Knowledge transfer via multiple model local structure mapping. In: Proceedings of the 14th ACM SIGKDD international conference on knowledge discovery and data mining. p. 283–91." href="/article/10.1186/s40537-016-0043-6#ref-CR37" id="ref-link-section-d43522e3952">37</a>], and Duan [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 29" title="Duan L, Xu D, Tsang IW. Domain adaptation from multiple sources: a domain-dependent regularization approach. IEEE Trans Neural Netw Learn Syst. 2012;23(3):504–18." href="/article/10.1186/s40537-016-0043-6#ref-CR29" id="ref-link-section-d43522e3955">29</a>]. The order of performance from best to worst is 2SW-MDA, CP-MDA, Duan [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 29" title="Duan L, Xu D, Tsang IW. Domain adaptation from multiple sources: a domain-dependent regularization approach. IEEE Trans Neural Netw Learn Syst. 2012;23(3):504–18." href="/article/10.1186/s40537-016-0043-6#ref-CR29" id="ref-link-section-d43522e3958">29</a>], Zhong [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 143" title="Zhong E, Fan W, Peng J, Zhang K, Ren J, Turaga D, Verscheure O. Cross domain distribution adaptation via kernel mapping. In: Proceedings of the 15th ACM SIGKDD. 2009. p. 1027–36." href="/article/10.1186/s40537-016-0043-6#ref-CR143" id="ref-link-section-d43522e3961">143</a>], Gao [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 37" title="Gao J, Fan W, Jiang J, Han J (2008) Knowledge transfer via multiple model local structure mapping. In: Proceedings of the 14th ACM SIGKDD international conference on knowledge discovery and data mining. p. 283–91." href="/article/10.1186/s40537-016-0043-6#ref-CR37" id="ref-link-section-d43522e3965">37</a>], Pan [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 87" title="Pan SJ, Tsang IW, Kwok JT, Yang Q. Domain adaptation via transfer component analysis. IEEE Trans Neural Netw. 2009;22(2):199–210." href="/article/10.1186/s40537-016-0043-6#ref-CR87" id="ref-link-section-d43522e3968">87</a>], Huang [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 51" title="Huang J, Smola A, Gretton A, Borgwardt KM, Schölkopf B. Correcting sample selection bias by unlabeled data. In: Proceedings of the 2006 conference. Adv Neural Inf Process Syst. 2006. p. 601–8." href="/article/10.1186/s40537-016-0043-6#ref-CR51" id="ref-link-section-d43522e3971">51</a>], and the baseline approach. All the transfer learning approaches performed better than the baseline approach.</p><h3 class="c-article__sub-heading" id="Sec5">Asymmetric feature-based transfer learning</h3><p>In an early and often cited work, Daumé [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 22" title="Daumé H III. Frustratingly easy domain adaptation. In: Proceedings of ACL. 2007. p. 256–63." href="/article/10.1186/s40537-016-0043-6#ref-CR22" id="ref-link-section-d43522e3982">22</a>] proposes a simple domain adaptation algorithm, referred to as the feature augmentation method (FAM), requiring only ten lines of Perl script that uses labeled source data and limited labeled target data. In a transfer learning environment, there are scenarios where a feature in the source domain may have a different meaning in the target domain. The issue is referred to as context feature bias, which causes the conditional distributions between the source and target domains to be different. To resolve context feature bias, a method to augment the source and target feature space with three duplicate copies of the original feature set is proposed. More specifically, the three duplicate copies of the original feature set in the augmented source feature space represent a common feature set, a source specific feature set, and a target specific feature set which is always set to zero. In a similar way, the three duplicate copies of the original feature set in the augmented target feature space represent a common feature set, a source specific feature set which is always set to zero, and a target specific feature set. By performing this feature augmentation, the feature space is duplicated three times. From the feature augmentation structure, a classifier learns the individual feature weights for the augmented feature set, which will help correct for any feature bias issues. Using a text document example where features are modeled as a bag-of-words, a common word like “the” would be assigned (through the learning process) a high weight for the common feature set, and a word that is different between the source and target like “monitor” would be assigned a high weight for the corresponding domain feature set. The duplication of features creates feature separation between the source and target domains, and allows the final classifier to learn the optimal feature weights. For the experiments, a number of different natural language processing applications are tested and in each case the classification error rate is measured as the performance metric. An SVM learner is used to implement the Daumé [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 22" title="Daumé H III. Frustratingly easy domain adaptation. In: Proceedings of ACL. 2007. p. 256–63." href="/article/10.1186/s40537-016-0043-6#ref-CR22" id="ref-link-section-d43522e3985">22</a>] approach. A number of baseline approaches with no transfer learning techniques are measured along with a method by Chelba [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 15" title="Chelba C, Acero A. Adaptation of maximum entropy classifier: little data can help a lot. Comput Speech Lang. 2004;20(4):382–99." href="/article/10.1186/s40537-016-0043-6#ref-CR15" id="ref-link-section-d43522e3988">15</a>]. The test results show the Daumé [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 22" title="Daumé H III. Frustratingly easy domain adaptation. In: Proceedings of ACL. 2007. p. 256–63." href="/article/10.1186/s40537-016-0043-6#ref-CR22" id="ref-link-section-d43522e3991">22</a>] method is able to outperform the other methods tested. However, when the source and target domains are very similar, the Daumé [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 22" title="Daumé H III. Frustratingly easy domain adaptation. In: Proceedings of ACL. 2007. p. 256–63." href="/article/10.1186/s40537-016-0043-6#ref-CR22" id="ref-link-section-d43522e3994">22</a>] approach tends to underperform. The reason for the underperformance is the duplication of feature sets represents irrelevant and noisy information when the source and target domains are very similar.</p><p>Multiple kernel learning is a technique used in traditional machine learning algorithms as demonstrated in the works of Wu [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 130" title="Wu X, Xu D, Duan L, Luo J (2011) Action recognition using context and appearance distribution features. In: IEEE 2011 conference on computer vision and pattern recognition. 2011. p. 489–96." href="/article/10.1186/s40537-016-0043-6#ref-CR130" id="ref-link-section-d43522e4000">130</a>] and Vedaldi [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 118" title="Vedaldi A, Gulshan V, Varma M, Zisserman A. Multiple kernels for object detection. In: 2009 IEEE 12th international conference on computer vision. 2009. p. 606–13." href="/article/10.1186/s40537-016-0043-6#ref-CR118" id="ref-link-section-d43522e4003">118</a>]. Multiple kernel learning allows for an optimal kernel function to be learned in a computationally efficient manner. The paper by Duan [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 27" title="Duan L, Tsang IW, Xu D. Domain transfer multiple kernel learning. IEEE Trans Pattern Anal Mach Intell. 2012;34(3):465–79." href="/article/10.1186/s40537-016-0043-6#ref-CR27" id="ref-link-section-d43522e4006">27</a>] proposes to implement a multiple kernel learning framework for a transfer learning environment called the domain transfer multiple kernel learning (DTMKL). Instead of learning one kernel, multiple kernel learning assumes the kernel is comprised of a linear combination of multiple predefined base kernels. The final classifier and the kernel function are learned simultaneously which has the advantage of using labeled data during the kernel learning process. This is an improvement over Pan [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 82" title="Pan SJ, Kwok JT, Yang Q. Transfer learning via dimensionality reduction. In: Proceedings of the 23rd national conference on artificial intelligence, vol. 2. 2008. p. 677–82." href="/article/10.1186/s40537-016-0043-6#ref-CR82" id="ref-link-section-d43522e4009">82</a>] and Huang [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 51" title="Huang J, Smola A, Gretton A, Borgwardt KM, Schölkopf B. Correcting sample selection bias by unlabeled data. In: Proceedings of the 2006 conference. Adv Neural Inf Process Syst. 2006. p. 601–8." href="/article/10.1186/s40537-016-0043-6#ref-CR51" id="ref-link-section-d43522e4012">51</a>] where a two-stage approach is used. The final classifier learning process minimizes the structural risk functional [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 117" title="Vapnik V. Principles of risk minimization for learning theory. Adv Neural Inf Process Syst. 1992;4:831–8." href="/article/10.1186/s40537-016-0043-6#ref-CR117" id="ref-link-section-d43522e4016">117</a>] and the marginal distribution between domains using the maximum mean discrepancy measure [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 10" title="Borgwardt KM, Gretton A, Rasch MJ, Kriegel HP, Schölkopf B, Smola AJ. Integrating structured biological data by kernel maximum mean discrepancy. Bioinformatics. 2006;22(4):49–57." href="/article/10.1186/s40537-016-0043-6#ref-CR10" id="ref-link-section-d43522e4019">10</a>]. Pseudo labels are found for the unlabeled target data to take advantage of this information during the learning process. The pseudo labels are found as a weighted combination of base classifiers (one for each feature) trained from the labeled source data. A regularization term is added to the optimization problem to ensure the predicted values from the final target classifier and the base classifiers are similar for the unlabeled target data. Experiments are performed on the applications of video concept detection, text classification, and email spam detection. The methods tested against include a baseline approach using an SVM classifier trained on the labeled source data, the feature replication method from Daumé [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 22" title="Daumé H III. Frustratingly easy domain adaptation. In: Proceedings of ACL. 2007. p. 256–63." href="/article/10.1186/s40537-016-0043-6#ref-CR22" id="ref-link-section-d43522e4022">22</a>], an adaptive SVM method from Yang [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 135" title="Yang J, Yan R, Hauptmann AG. Cross-domain video concept detection using adaptive SVMs. In: Proceedings of the 15th ACM international conference on multimedia. 2007. p. 188–97." href="/article/10.1186/s40537-016-0043-6#ref-CR135" id="ref-link-section-d43522e4025">135</a>], a cross-domain SVM method proposed by Jiang [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 55" title="Jiang W, Zavesky E, Chang SF, Loui A. Cross-domain learning methods for high-level visual concept classification. In: IEEE 2008 15th international conference on image processing. 2008. p. 161–4." href="/article/10.1186/s40537-016-0043-6#ref-CR55" id="ref-link-section-d43522e4028">55</a>], and a kernel mean matching method by Huang [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 51" title="Huang J, Smola A, Gretton A, Borgwardt KM, Schölkopf B. Correcting sample selection bias by unlabeled data. In: Proceedings of the 2006 conference. Adv Neural Inf Process Syst. 2006. p. 601–8." href="/article/10.1186/s40537-016-0043-6#ref-CR51" id="ref-link-section-d43522e4031">51</a>]. The DTMKL approach uses an SVM learner for the experiments. Average precision and classification accuracy are measured as the performance metrics. The DTMKL method performed the best for all applications, and the baseline approach is consistently the worst performing. The other methods showed better performance over the baseline which demonstrated a positive transfer learning effect.</p><p>The work by Long [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 69" title="Long M, Wang J, Ding G, Sun J, Yu PS. Transfer feature learning with joint distribution adaptation. In: Proceedings of the 2013 IEEE international conference on computer vision. 2013. p. 2200–07." href="/article/10.1186/s40537-016-0043-6#ref-CR69" id="ref-link-section-d43522e4037">69</a>] is a joint domain adaptation (JDA) solution that aims to simultaneously correct for the marginal and conditional distribution differences between the labeled source domain and the unlabeled target domain. Principal component analysis (PCA) is used for optimization and dimensionality reduction. To address the difference in marginal distribution between the domains, the maximum mean discrepancy distance measure [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 10" title="Borgwardt KM, Gretton A, Rasch MJ, Kriegel HP, Schölkopf B, Smola AJ. Integrating structured biological data by kernel maximum mean discrepancy. Bioinformatics. 2006;22(4):49–57." href="/article/10.1186/s40537-016-0043-6#ref-CR10" id="ref-link-section-d43522e4040">10</a>] is used to compute the marginal distribution differences and is integrated into the PCA optimization algorithm. The next part of the solution requires a process to correct the conditional distribution differences, which requires labeled target data. Since the target data is unlabeled, pseudo labels (estimated target labels) are found by learning a classifier from the labeled source data. The maximum mean discrepancy distance measure is modified to measure the distance between the conditional distributions and is integrated into the PCA optimization algorithm to minimize the conditional distributions. Finally, the features identified by the modified PCA algorithm are used to train the final target classifier. Experiments are performed for the application of image recognition and classification accuracy is measured as the performance metric. Two baseline approaches of a 1-nearest neighbor classifier and a PCA approach trained on the source data are tested. Transfer learning approaches tested for this experiment include the approach by Pan [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 87" title="Pan SJ, Tsang IW, Kwok JT, Yang Q. Domain adaptation via transfer component analysis. IEEE Trans Neural Netw. 2009;22(2):199–210." href="/article/10.1186/s40537-016-0043-6#ref-CR87" id="ref-link-section-d43522e4043">87</a>], Gong [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 42" title="Gong B, Shi Y, Sha F, Grauman K. Geodesic flow kernel for unsupervised domain adaptation. In: Proceedings of the 2012 IEEE conference on computer vision and pattern recognition. 2012. p. 2066–73." href="/article/10.1186/s40537-016-0043-6#ref-CR42" id="ref-link-section-d43522e4046">42</a>], and Si [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 109" title="Si S, Tao D, Geng B. Bregman divergence-based regularization for transfer subspace learning. IEEE Trans Knowl Data Eng. 2010;22(7):929–42." href="/article/10.1186/s40537-016-0043-6#ref-CR109" id="ref-link-section-d43522e4049">109</a>]. These transfer learning approaches only attempt to correct for marginal distribution differences between domains. The Long [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 69" title="Long M, Wang J, Ding G, Sun J, Yu PS. Transfer feature learning with joint distribution adaptation. In: Proceedings of the 2013 IEEE international conference on computer vision. 2013. p. 2200–07." href="/article/10.1186/s40537-016-0043-6#ref-CR69" id="ref-link-section-d43522e4053">69</a>] approach is the best performing, followed by the Pan [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 87" title="Pan SJ, Tsang IW, Kwok JT, Yang Q. Domain adaptation via transfer component analysis. IEEE Trans Neural Netw. 2009;22(2):199–210." href="/article/10.1186/s40537-016-0043-6#ref-CR87" id="ref-link-section-d43522e4056">87</a>] and Si [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 109" title="Si S, Tao D, Geng B. Bregman divergence-based regularization for transfer subspace learning. IEEE Trans Knowl Data Eng. 2010;22(7):929–42." href="/article/10.1186/s40537-016-0043-6#ref-CR109" id="ref-link-section-d43522e4059">109</a>] approaches (a tie), then the Gong [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 42" title="Gong B, Shi Y, Sha F, Grauman K. Geodesic flow kernel for unsupervised domain adaptation. In: Proceedings of the 2012 IEEE conference on computer vision and pattern recognition. 2012. p. 2066–73." href="/article/10.1186/s40537-016-0043-6#ref-CR42" id="ref-link-section-d43522e4062">42</a>] approach, and finally the baseline approaches. All transfer learning approaches perform better than the baseline approaches. The possible reason behind the underperformance of the Gong [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 42" title="Gong B, Shi Y, Sha F, Grauman K. Geodesic flow kernel for unsupervised domain adaptation. In: Proceedings of the 2012 IEEE conference on computer vision and pattern recognition. 2012. p. 2066–73." href="/article/10.1186/s40537-016-0043-6#ref-CR42" id="ref-link-section-d43522e4065">42</a>] approach is the data smoothness assumption that is made for the Gong [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 42" title="Gong B, Shi Y, Sha F, Grauman K. Geodesic flow kernel for unsupervised domain adaptation. In: Proceedings of the 2012 IEEE conference on computer vision and pattern recognition. 2012. p. 2066–73." href="/article/10.1186/s40537-016-0043-6#ref-CR42" id="ref-link-section-d43522e4068">42</a>] solution may not be intact for the data sets tested.</p><p>The paper by Long [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 68" title="Long M, Wang J, Ding G, Pan SJ, Yu PS. Adaptation regularization: a general framework for transfer learning. IEEE Trans Knowl Data Eng. 2014;26(5):1076–89." href="/article/10.1186/s40537-016-0043-6#ref-CR68" id="ref-link-section-d43522e4074">68</a>] proposes an Adaptation Regularization based transfer learning (ARTL) framework for scenarios of labeled source data and unlabeled target data. This transfer learning framework proposes to correct the difference in marginal distribution between the source and target domains, correct the difference in conditional distribution between the domains, and improve classification performance through a manifold regularization [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 4" title="Belkin M, Niyogi P, Sindhwani V. Manifold regularization: a geometric framework for learning from examples. J Mach Learn Res Arch. 2006;7:2399–434." href="/article/10.1186/s40537-016-0043-6#ref-CR4" id="ref-link-section-d43522e4077">4</a>] process (which optimally shifts the hyperplane of an SVM learner). This complete framework process is depicted in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1186/s40537-016-0043-6#Fig2">2</a>. The proposed ARTL framework will learn a classifier by simultaneously performing structural risk minimization [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 117" title="Vapnik V. Principles of risk minimization for learning theory. Adv Neural Inf Process Syst. 1992;4:831–8." href="/article/10.1186/s40537-016-0043-6#ref-CR117" id="ref-link-section-d43522e4083">117</a>], reducing the marginal and conditional distributions between the domains, and optimizing the manifold consistency of the marginal distribution. To resolve the conditional distribution differences, pseudo labels are found for the target data in the same way as proposed by Long [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 69" title="Long M, Wang J, Ding G, Sun J, Yu PS. Transfer feature learning with joint distribution adaptation. In: Proceedings of the 2013 IEEE international conference on computer vision. 2013. p. 2200–07." href="/article/10.1186/s40537-016-0043-6#ref-CR69" id="ref-link-section-d43522e4086">69</a>]. A difference between the ARTL approach and Long [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 69" title="Long M, Wang J, Ding G, Sun J, Yu PS. Transfer feature learning with joint distribution adaptation. In: Proceedings of the 2013 IEEE international conference on computer vision. 2013. p. 2200–07." href="/article/10.1186/s40537-016-0043-6#ref-CR69" id="ref-link-section-d43522e4090">69</a>] is ARTL learns the final classifier simultaneously while minimizing the domain distribution differences, which is claimed by Long [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 68" title="Long M, Wang J, Ding G, Pan SJ, Yu PS. Adaptation regularization: a general framework for transfer learning. IEEE Trans Knowl Data Eng. 2014;26(5):1076–89." href="/article/10.1186/s40537-016-0043-6#ref-CR68" id="ref-link-section-d43522e4093">68</a>] to be a more optimal solution. Unfortunately, the solution by Long [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 69" title="Long M, Wang J, Ding G, Sun J, Yu PS. Transfer feature learning with joint distribution adaptation. In: Proceedings of the 2013 IEEE international conference on computer vision. 2013. p. 2200–07." href="/article/10.1186/s40537-016-0043-6#ref-CR69" id="ref-link-section-d43522e4096">69</a>] is not included in the experiments. Experiments are performed on the applications of text classification and image classification where classification accuracy is measured as the performance metric. There are three baseline methods tested where different classifiers are trained with the labeled source data. There are five transfer learning methods tested against, which include methods by Ling [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 66" title="Ling X, Dai W, Xue GR, Yang Q, Yu Y. Spectral domain-transfer learning. In: Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining. 2008. p. 488–96." href="/article/10.1186/s40537-016-0043-6#ref-CR66" id="ref-link-section-d43522e4099">66</a>], Pan [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 83" title="Pan SJ, Ni X, Sun JT, Yang Q, Chen Z. Cross-domain sentiment classification via spectral feature alignment. In: Proceedings of the 19th international conference on world wide web. 2010. p. 751–60." href="/article/10.1186/s40537-016-0043-6#ref-CR83" id="ref-link-section-d43522e4102">83</a>], Pan [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 87" title="Pan SJ, Tsang IW, Kwok JT, Yang Q. Domain adaptation via transfer component analysis. IEEE Trans Neural Netw. 2009;22(2):199–210." href="/article/10.1186/s40537-016-0043-6#ref-CR87" id="ref-link-section-d43522e4105">87</a>], Quanz [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 94" title="Quanz B, Huan J. Large margin transductive transfer learning. In: Proceedings of the 18th ACM conference on information and knowledge management. 2009. p. 1327–36." href="/article/10.1186/s40537-016-0043-6#ref-CR94" id="ref-link-section-d43522e4109">94</a>], and Xiao [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 133" title="Xiao M, Guo Y. Semi-supervised kernel matching for domain adaptation. In: Proceedings of the twenty-sixth AAAI conference on artificial intelligence. 2012. p. 1183–89." href="/article/10.1186/s40537-016-0043-6#ref-CR133" id="ref-link-section-d43522e4112">133</a>]. The order of performance from best to worst is ARTL, Xiao [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 133" title="Xiao M, Guo Y. Semi-supervised kernel matching for domain adaptation. In: Proceedings of the twenty-sixth AAAI conference on artificial intelligence. 2012. p. 1183–89." href="/article/10.1186/s40537-016-0043-6#ref-CR133" id="ref-link-section-d43522e4115">133</a>], Pan [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 87" title="Pan SJ, Tsang IW, Kwok JT, Yang Q. Domain adaptation via transfer component analysis. IEEE Trans Neural Netw. 2009;22(2):199–210." href="/article/10.1186/s40537-016-0043-6#ref-CR87" id="ref-link-section-d43522e4118">87</a>], Pan [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 83" title="Pan SJ, Ni X, Sun JT, Yang Q, Chen Z. Cross-domain sentiment classification via spectral feature alignment. In: Proceedings of the 19th international conference on world wide web. 2010. p. 751–60." href="/article/10.1186/s40537-016-0043-6#ref-CR83" id="ref-link-section-d43522e4121">83</a>], Quanz [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 94" title="Quanz B, Huan J. Large margin transductive transfer learning. In: Proceedings of the 18th ACM conference on information and knowledge management. 2009. p. 1327–36." href="/article/10.1186/s40537-016-0043-6#ref-CR94" id="ref-link-section-d43522e4124">94</a>] and Ling [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 66" title="Ling X, Dai W, Xue GR, Yang Q, Yu Y. Spectral domain-transfer learning. In: Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining. 2008. p. 488–96." href="/article/10.1186/s40537-016-0043-6#ref-CR66" id="ref-link-section-d43522e4128">66</a>] (tie), and the baseline approaches. The baseline methods underperformed all other transfer learning approaches tested.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2" data-title="Fig. 2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1186/s40537-016-0043-6/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1186%2Fs40537-016-0043-6/MediaObjects/40537_2016_43_Fig2_HTML.gif?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1186%2Fs40537-016-0043-6/MediaObjects/40537_2016_43_Fig2_HTML.gif" alt="figure2" loading="lazy" /></picture></a><p class="c-article-section__figure-credit text-right c-article-section__figure-credit-right" data-test="figure-credit">Diagram adapted from Long [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 68" title="Long M, Wang J, Ding G, Pan SJ, Yu PS. Adaptation regularization: a general framework for transfer learning. IEEE Trans Knowl Data Eng. 2014;26(5):1076–89." href="/article/10.1186/s40537-016-0043-6#ref-CR68" id="ref-link-section-d43522e4155">68</a>]</p></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>ARTL overview showing <i>MDA</i> marginal distribution adaptation, <i>CDA</i> conditional distribution adaptation, and <i>MR</i> manifold regularization</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1186/s40537-016-0043-6/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <h3 class="c-article__sub-heading" id="Sec6">Symmetric feature-based transfer learning</h3><p>The paper by Pan [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 87" title="Pan SJ, Tsang IW, Kwok JT, Yang Q. Domain adaptation via transfer component analysis. IEEE Trans Neural Netw. 2009;22(2):199–210." href="/article/10.1186/s40537-016-0043-6#ref-CR87" id="ref-link-section-d43522e4177">87</a>] proposes a feature transformation approach for domain adaptation called transfer component analysis (TCA), which does not require labeled target data. The goal is to discover common latent features that have the same marginal distribution across the source and target domains while maintaining the intrinsic structure of the original domain data. The latent features are learned between the source and target domains in a reproducing kernel hilbert space [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 111" title="Steinwart I. On the influence of the kernel on the consistency of support vector machines. JMLR. 2001;2:67–93." href="/article/10.1186/s40537-016-0043-6#ref-CR111" id="ref-link-section-d43522e4180">111</a>] using the maximum mean discrepancy [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 10" title="Borgwardt KM, Gretton A, Rasch MJ, Kriegel HP, Schölkopf B, Smola AJ. Integrating structured biological data by kernel maximum mean discrepancy. Bioinformatics. 2006;22(4):49–57." href="/article/10.1186/s40537-016-0043-6#ref-CR10" id="ref-link-section-d43522e4183">10</a>] as a marginal distribution measurement criteria. Once the latent features are found, traditional machine learning is used to train the final target classifier. The TCA approach extends the work of Pan [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 82" title="Pan SJ, Kwok JT, Yang Q. Transfer learning via dimensionality reduction. In: Proceedings of the 23rd national conference on artificial intelligence, vol. 2. 2008. p. 677–82." href="/article/10.1186/s40537-016-0043-6#ref-CR82" id="ref-link-section-d43522e4186">82</a>] by improving computational efficiency. Experiments are conducted for the application of WiFi localization where the location of a particular device is being predicted. The source domain is comprised of data measured from different room and building topologies. The performance metric measured is the average error distance of the position of a device. The transfer learning methods tested against are from Blitzer [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 5" title="Blitzer, J, McDonald R, Pereira F. Domain adaptation with structural correspondence learning. In: Proceedings of the 2006 conference on empirical methods in natural language processing. 2006;120–8." href="/article/10.1186/s40537-016-0043-6#ref-CR5" id="ref-link-section-d43522e4189">5</a>] and Huang [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 51" title="Huang J, Smola A, Gretton A, Borgwardt KM, Schölkopf B. Correcting sample selection bias by unlabeled data. In: Proceedings of the 2006 conference. Adv Neural Inf Process Syst. 2006. p. 601–8." href="/article/10.1186/s40537-016-0043-6#ref-CR51" id="ref-link-section-d43522e4193">51</a>]. The TCA method performed the best followed by the Huang [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 51" title="Huang J, Smola A, Gretton A, Borgwardt KM, Schölkopf B. Correcting sample selection bias by unlabeled data. In: Proceedings of the 2006 conference. Adv Neural Inf Process Syst. 2006. p. 601–8." href="/article/10.1186/s40537-016-0043-6#ref-CR51" id="ref-link-section-d43522e4196">51</a>] approach and the Blitzer [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 5" title="Blitzer, J, McDonald R, Pereira F. Domain adaptation with structural correspondence learning. In: Proceedings of the 2006 conference on empirical methods in natural language processing. 2006;120–8." href="/article/10.1186/s40537-016-0043-6#ref-CR5" id="ref-link-section-d43522e4199">5</a>] approach. For the Blitzer [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 5" title="Blitzer, J, McDonald R, Pereira F. Domain adaptation with structural correspondence learning. In: Proceedings of the 2006 conference on empirical methods in natural language processing. 2006;120–8." href="/article/10.1186/s40537-016-0043-6#ref-CR5" id="ref-link-section-d43522e4202">5</a>] approach, the manual definition of the pivot functions (functions that define the correspondence) is important to performance and specific to the end application. There is no mention as to how the pivot functions are defined for WiFi localization.</p><p>The work by Pan [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 83" title="Pan SJ, Ni X, Sun JT, Yang Q, Chen Z. Cross-domain sentiment classification via spectral feature alignment. In: Proceedings of the 19th international conference on world wide web. 2010. p. 751–60." href="/article/10.1186/s40537-016-0043-6#ref-CR83" id="ref-link-section-d43522e4208">83</a>] proposes a spectral feature alignment (SFA) transfer learning algorithm that discovers a new feature representation for the source and target domain to resolve the marginal distribution differences. The SFA method assumes an abundance of labeled source data and a limited amount of labeled target data. The SFA approach identifies domain-specific and domain-independent features and uses the domain-independent features as a bridge to build a bipartite graph modeling the co-occurrence relationship between the domain-independent and domain-specific features. If the graph shows two domain-specific features having connections to common domain-independent feature, then there is a higher chance the domain-specific features are aligned. A spectral clustering algorithm based on graph spectral theory [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 17" title="Chung FRK. Spectral graph theory. In: CBMS regional conference series in mathematics, no. 92. Providence: American Mathematical Society; 1994." href="/article/10.1186/s40537-016-0043-6#ref-CR17" id="ref-link-section-d43522e4211">17</a>] is used on the bipartite graph to align domain-specific features and domain-independent features into a set of clusters representing new features. These clusters are used to reduce the difference between domain-specific features in the source and the target domains. All the data instances are projected into this new feature space and a final target classifier is trained using the new feature representation. The SFA algorithm is a type of correspondence learning where the domain-independent features act as pivot features (see Blitzer [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 5" title="Blitzer, J, McDonald R, Pereira F. Domain adaptation with structural correspondence learning. In: Proceedings of the 2006 conference on empirical methods in natural language processing. 2006;120–8." href="/article/10.1186/s40537-016-0043-6#ref-CR5" id="ref-link-section-d43522e4214">5</a>] and Prettenhofer [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 91" title="Prettenhofer P, Stein B. (2010) Cross-language text classification using structural correspondence learning. In: Proceedings of the 48th annual meeting of the association for computational linguistics. 2010. p. 1118–27." href="/article/10.1186/s40537-016-0043-6#ref-CR91" id="ref-link-section-d43522e4217">91</a>] for further information on correspondence learning). The SFA method is well-suited for the application of text document classification where a bag-of-words model is used to define features. For this application there are domain-independent words that will appear often in both domains and domain-specific words that will appear often only in a specific domain. This is referred to as frequency feature bias, which causes marginal distribution differences between the domains. An example of domain-specific features being combined is the word “sharp” appearing often in the source domain but not in the target domain, and the word “hooked” appearing often in the target but not in the source domain. These words are both connected to the same domain-independent words (for example “good” and “exciting”). Further, when the words “sharp” or “hooked” appear in text instances, the labels are the same. The idea is to combine (or align) these two features (in this case “sharp” and “hooked”) to form a new single invariant feature. The experiments are performed on sentiment classification where classification accuracy is measured as the performance metric. A baseline approach is tested where a classifier is trained only on source data. An upper limit approach is also tested where a classifier is trained on a large amount of labeled target data. The competing transfer learning approach tested against is by Blitzer [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 5" title="Blitzer, J, McDonald R, Pereira F. Domain adaptation with structural correspondence learning. In: Proceedings of the 2006 conference on empirical methods in natural language processing. 2006;120–8." href="/article/10.1186/s40537-016-0043-6#ref-CR5" id="ref-link-section-d43522e4220">5</a>]. The order of performance for the tests from best to worst is the upper limit approach, SFA, Blitzer [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 5" title="Blitzer, J, McDonald R, Pereira F. Domain adaptation with structural correspondence learning. In: Proceedings of the 2006 conference on empirical methods in natural language processing. 2006;120–8." href="/article/10.1186/s40537-016-0043-6#ref-CR5" id="ref-link-section-d43522e4224">5</a>], and baseline approach. Not only does the SFA approach demonstrate better performance than Blitzer [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 5" title="Blitzer, J, McDonald R, Pereira F. Domain adaptation with structural correspondence learning. In: Proceedings of the 2006 conference on empirical methods in natural language processing. 2006;120–8." href="/article/10.1186/s40537-016-0043-6#ref-CR5" id="ref-link-section-d43522e4227">5</a>], the SFA approach does not need to manually define pivot functions as in the Blitzer [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 5" title="Blitzer, J, McDonald R, Pereira F. Domain adaptation with structural correspondence learning. In: Proceedings of the 2006 conference on empirical methods in natural language processing. 2006;120–8." href="/article/10.1186/s40537-016-0043-6#ref-CR5" id="ref-link-section-d43522e4230">5</a>] approach. The SFA approach only addresses the issue of marginal distribution differences and does not address any context feature bias issues, which would represent conditional distribution differences.</p><p>The work by Glorot [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 41" title="Glorot X, Bordes A, Bengio Y. Domain adaptation for large-scale sentiment classification: A deep learning approach. In: Proceedings of the twenty-eight international conference on machine learning, vol. 27. 2011. p. 97–110." href="/article/10.1186/s40537-016-0043-6#ref-CR41" id="ref-link-section-d43522e4236">41</a>] proposes a deep learning algorithm for transfer learning called a stacked denoising autoencoder (SDA) to resolve the marginal distribution differences between a labeled source domain and an unlabeled target domain. Deep learning algorithms learn intermediate invariant concepts between two data sources, which are used to find a common latent feature set. The first step in this process is to train the stacked denoising autoencoders [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 119" title="Vincent P, Larochelle H, Bengio Y, Manzagol PA. Extracting and composing robust features with denoising autoencoders. In: Proceedings of the 25th international conference on machine learning. 2008. p. 1096–103." href="/article/10.1186/s40537-016-0043-6#ref-CR119" id="ref-link-section-d43522e4239">119</a>] with unlabeled data from the source and target domains. This transforms the input space to discover the common invariant latent feature space. The next step is to train a classifier using the transformed latent features with the labeled source data. Experiments are performed on text review sentiment classification where transfer loss is measured as the performance metric. Transfer loss is defined as the classification error rate using a learner only trained on the source domain and tested on the target minus the classification error rate using a learner only trained on the target domain and tested on the target. There are 12 different source and target domain pairs that are created from four unique review topics. A baseline method is tested where an SVM classifier is trained on the source domain. The transfer learning approaches that are tested include an approach by Blitzer [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 5" title="Blitzer, J, McDonald R, Pereira F. Domain adaptation with structural correspondence learning. In: Proceedings of the 2006 conference on empirical methods in natural language processing. 2006;120–8." href="/article/10.1186/s40537-016-0043-6#ref-CR5" id="ref-link-section-d43522e4242">5</a>], Li [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 63" title="Li S, Zong C. Multi-domain adaptation for sentiment classification: Using multiple classifier combining methods. In: Proceedings of the conference on natural language processing and knowledge engineering. 2008. p. 1–8." href="/article/10.1186/s40537-016-0043-6#ref-CR63" id="ref-link-section-d43522e4245">63</a>], and Pan [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 83" title="Pan SJ, Ni X, Sun JT, Yang Q, Chen Z. Cross-domain sentiment classification via spectral feature alignment. In: Proceedings of the 19th international conference on world wide web. 2010. p. 751–60." href="/article/10.1186/s40537-016-0043-6#ref-CR83" id="ref-link-section-d43522e4248">83</a>]. The Glorot [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 41" title="Glorot X, Bordes A, Bengio Y. Domain adaptation for large-scale sentiment classification: A deep learning approach. In: Proceedings of the twenty-eight international conference on machine learning, vol. 27. 2011. p. 97–110." href="/article/10.1186/s40537-016-0043-6#ref-CR41" id="ref-link-section-d43522e4252">41</a>] approach performed the best with the Blitzer [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 5" title="Blitzer, J, McDonald R, Pereira F. Domain adaptation with structural correspondence learning. In: Proceedings of the 2006 conference on empirical methods in natural language processing. 2006;120–8." href="/article/10.1186/s40537-016-0043-6#ref-CR5" id="ref-link-section-d43522e4255">5</a>], Li [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 63" title="Li S, Zong C. Multi-domain adaptation for sentiment classification: Using multiple classifier combining methods. In: Proceedings of the conference on natural language processing and knowledge engineering. 2008. p. 1–8." href="/article/10.1186/s40537-016-0043-6#ref-CR63" id="ref-link-section-d43522e4258">63</a>], and Pan [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 83" title="Pan SJ, Ni X, Sun JT, Yang Q, Chen Z. Cross-domain sentiment classification via spectral feature alignment. In: Proceedings of the 19th international conference on world wide web. 2010. p. 751–60." href="/article/10.1186/s40537-016-0043-6#ref-CR83" id="ref-link-section-d43522e4261">83</a>] methods all having similar performance and all outperforming the baseline approach.</p><p>In the paper by Gong [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 42" title="Gong B, Shi Y, Sha F, Grauman K. Geodesic flow kernel for unsupervised domain adaptation. In: Proceedings of the 2012 IEEE conference on computer vision and pattern recognition. 2012. p. 2066–73." href="/article/10.1186/s40537-016-0043-6#ref-CR42" id="ref-link-section-d43522e4267">42</a>], a domain adaptation technique called the geodesic flow kernel (GFK) is proposed that finds a low-dimensional feature space, which reduces the marginal distribution differences between the labeled source and unlabeled target domains. To accomplish this, a geodesic flow kernel is constructed using the source and target input feature data, which projects a large number of subspaces that lie on the geodesic flow curve. The geodesic flow curve represents incremental differences in geometric and statistical properties between the source and target domain spaces. A classifier is then learned from the geodesic flow kernel by selecting the features from the geodesic flow curve that are domain invariant. The work of Gong [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 42" title="Gong B, Shi Y, Sha F, Grauman K. Geodesic flow kernel for unsupervised domain adaptation. In: Proceedings of the 2012 IEEE conference on computer vision and pattern recognition. 2012. p. 2066–73." href="/article/10.1186/s40537-016-0043-6#ref-CR42" id="ref-link-section-d43522e4270">42</a>] directly enhances the work of Gopalan [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 43" title="Gopalan R, Li R, Chellappa R. Domain adaptation for object recognition: an unsupervised approach. In: 2011 international conference on computer vision. 2011. p. 999–1006." href="/article/10.1186/s40537-016-0043-6#ref-CR43" id="ref-link-section-d43522e4273">43</a>] by eliminating tuning parameters and improving computational efficiency. In addition, a rank of domain (ROD) metric is developed to evaluate which of many source domains is the best match for the target domain. The ROD metric is a function of the geometric alignment between the domains and the Kullback–Leibler divergence in data distributions between the projected source and target subspaces. Experiments are performed for the application of image classification where classification accuracy is measured as the performance metric. The tests use pairs of source and target data sets from four available data sets. A baseline approach is defined that does not use transfer learning, along with the approach defined by Gopalan [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 43" title="Gopalan R, Li R, Chellappa R. Domain adaptation for object recognition: an unsupervised approach. In: 2011 international conference on computer vision. 2011. p. 999–1006." href="/article/10.1186/s40537-016-0043-6#ref-CR43" id="ref-link-section-d43522e4276">43</a>]. Additionally, the Gong [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 42" title="Gong B, Shi Y, Sha F, Grauman K. Geodesic flow kernel for unsupervised domain adaptation. In: Proceedings of the 2012 IEEE conference on computer vision and pattern recognition. 2012. p. 2066–73." href="/article/10.1186/s40537-016-0043-6#ref-CR42" id="ref-link-section-d43522e4279">42</a>] approach uses a 1-nearest neighbor classifier. The results in order from best to worst performance are Gong [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 42" title="Gong B, Shi Y, Sha F, Grauman K. Geodesic flow kernel for unsupervised domain adaptation. In: Proceedings of the 2012 IEEE conference on computer vision and pattern recognition. 2012. p. 2066–73." href="/article/10.1186/s40537-016-0043-6#ref-CR42" id="ref-link-section-d43522e4283">42</a>], Gopalan [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 43" title="Gopalan R, Li R, Chellappa R. Domain adaptation for object recognition: an unsupervised approach. In: 2011 international conference on computer vision. 2011. p. 999–1006." href="/article/10.1186/s40537-016-0043-6#ref-CR43" id="ref-link-section-d43522e4286">43</a>], and the baseline approach. The ROD measurements between the different source and target domain pairs tested have a high correlation to the actual test results, meaning the domains that are found to be more related with respect to the ROD measurement had higher classification accuracies.</p><p>The solution by Shi [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 106" title="Shi Y, Sha F. Information-theoretical learning of discriminative clusters for unsupervised domain adaptation. In: Proceedings of the 29th international conference on machine learning. 2012. p. 1–8." href="/article/10.1186/s40537-016-0043-6#ref-CR106" id="ref-link-section-d43522e4293">106</a>], referred to as the discriminative clustering process (DCP), proposes to equalize the marginal distribution of the labeled source and unlabeled target domains. A discriminative clustering process is used to discover a common latent feature space that is domain invariant while simultaneously learning the final target classifier. The motivating assumptions for this solution are the data in both domains form well-defined clusters which correspond to unique class labels, and the clusters from the source domain are geometrically close to the target clusters if they share the same label. Through clustering, the source domain labels can be used to estimate the target labels. A one-stage solution is formulated that minimizes the marginal distribution differences while minimizing the predicted classification error in the target domain using a nearest neighbor classifier. Experiments are performed for object recognition and sentiment classification where classification accuracy is measured as the performance metric. The approach described above is tested against a baseline approach taken from Weinberger [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 126" title="Weinberger KQ, Saul LK. Distance metric learning for large margin nearest neighbor classification. JMLR. 2009;10:207–44." href="/article/10.1186/s40537-016-0043-6#ref-CR126" id="ref-link-section-d43522e4296">126</a>] with no transfer learning. Other transfer learning approaches tested include an approach from Pan [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 87" title="Pan SJ, Tsang IW, Kwok JT, Yang Q. Domain adaptation via transfer component analysis. IEEE Trans Neural Netw. 2009;22(2):199–210." href="/article/10.1186/s40537-016-0043-6#ref-CR87" id="ref-link-section-d43522e4299">87</a>], Blitzer [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 5" title="Blitzer, J, McDonald R, Pereira F. Domain adaptation with structural correspondence learning. In: Proceedings of the 2006 conference on empirical methods in natural language processing. 2006;120–8." href="/article/10.1186/s40537-016-0043-6#ref-CR5" id="ref-link-section-d43522e4302">5</a>], and Gopalan [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 43" title="Gopalan R, Li R, Chellappa R. Domain adaptation for object recognition: an unsupervised approach. In: 2011 international conference on computer vision. 2011. p. 999–1006." href="/article/10.1186/s40537-016-0043-6#ref-CR43" id="ref-link-section-d43522e4305">43</a>]. The Blitzer [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 5" title="Blitzer, J, McDonald R, Pereira F. Domain adaptation with structural correspondence learning. In: Proceedings of the 2006 conference on empirical methods in natural language processing. 2006;120–8." href="/article/10.1186/s40537-016-0043-6#ref-CR5" id="ref-link-section-d43522e4309">5</a>] approach is not tested for the object recognition application because the pivot functions are not easily defined for this application. For the object recognition tests, the Shi [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 106" title="Shi Y, Sha F. Information-theoretical learning of discriminative clusters for unsupervised domain adaptation. In: Proceedings of the 29th international conference on machine learning. 2012. p. 1–8." href="/article/10.1186/s40537-016-0043-6#ref-CR106" id="ref-link-section-d43522e4312">106</a>] method is best in five out of six comparison tests. For the text classification tests, the Shi [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 106" title="Shi Y, Sha F. Information-theoretical learning of discriminative clusters for unsupervised domain adaptation. In: Proceedings of the 29th international conference on machine learning. 2012. p. 1–8." href="/article/10.1186/s40537-016-0043-6#ref-CR106" id="ref-link-section-d43522e4315">106</a>] approach is the best performing overall, with the Blitzer [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 5" title="Blitzer, J, McDonald R, Pereira F. Domain adaptation with structural correspondence learning. In: Proceedings of the 2006 conference on empirical methods in natural language processing. 2006;120–8." href="/article/10.1186/s40537-016-0043-6#ref-CR5" id="ref-link-section-d43522e4318">5</a>] approach a close second. An important point to note is the baseline method outperformed the Pan [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 87" title="Pan SJ, Tsang IW, Kwok JT, Yang Q. Domain adaptation via transfer component analysis. IEEE Trans Neural Netw. 2009;22(2):199–210." href="/article/10.1186/s40537-016-0043-6#ref-CR87" id="ref-link-section-d43522e4321">87</a>] and Gopalan [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 43" title="Gopalan R, Li R, Chellappa R. Domain adaptation for object recognition: an unsupervised approach. In: 2011 international conference on computer vision. 2011. p. 999–1006." href="/article/10.1186/s40537-016-0043-6#ref-CR43" id="ref-link-section-d43522e4324">43</a>] methods in both tests. Both the Pan [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 87" title="Pan SJ, Tsang IW, Kwok JT, Yang Q. Domain adaptation via transfer component analysis. IEEE Trans Neural Netw. 2009;22(2):199–210." href="/article/10.1186/s40537-016-0043-6#ref-CR87" id="ref-link-section-d43522e4328">87</a>] and Gopalan [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 43" title="Gopalan R, Li R, Chellappa R. Domain adaptation for object recognition: an unsupervised approach. In: 2011 international conference on computer vision. 2011. p. 999–1006." href="/article/10.1186/s40537-016-0043-6#ref-CR43" id="ref-link-section-d43522e4331">43</a>] methods are two-stage domain adaptation processes where the first stage reduces the marginal distributions between the domains and the second stage trains a classifier with the adapted domain data. This paper offers a hypothesis that two-stage processes are actually detrimental to transfer learning (causes negative transfer). The one-stage learning process is a novel idea presented by this paper. The hypothesis that the two-stage transfer learning process creates low performing learners does not agree with the results presented in the individual papers by Gopalan [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 43" title="Gopalan R, Li R, Chellappa R. Domain adaptation for object recognition: an unsupervised approach. In: 2011 international conference on computer vision. 2011. p. 999–1006." href="/article/10.1186/s40537-016-0043-6#ref-CR43" id="ref-link-section-d43522e4334">43</a>] and Pan [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 87" title="Pan SJ, Tsang IW, Kwok JT, Yang Q. Domain adaptation via transfer component analysis. IEEE Trans Neural Netw. 2009;22(2):199–210." href="/article/10.1186/s40537-016-0043-6#ref-CR87" id="ref-link-section-d43522e4337">87</a>] and other previously surveyed works.</p><p>Convolutional neural networks (CNN) have been successfully used in traditional data mining environments [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 59" title="LeCun Y, Bottou L, HuangFu J. Learning methods for generic object recognition with invariance to pose and lighting. In: Proceedings of the 2004 IEEE computer society conference on computer vision and pattern recognition, vol. 2. 2004. p. 97–104." href="/article/10.1186/s40537-016-0043-6#ref-CR59" id="ref-link-section-d43522e4343">59</a>]. However, a CNN requires a large amount of labeled training data to be effective, which may not be available. The paper by Oquab [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 81" title="Oquab M, Bottou L, Laptev I, Sivic J. Learning and transferring mid-level image representations using convolutional neural networks. In: Proceedings of the 2014 IEEE conference on computer vision and pattern recognition. 2013. p. 1717–24." href="/article/10.1186/s40537-016-0043-6#ref-CR81" id="ref-link-section-d43522e4346">81</a>] proposes a transfer learning method of training a CNN with available labeled source data (a source learner) and then extracting the CNN internal layers (which represent a generic mid-level feature representation) to a target CNN learner. This method is referred to as the transfer convolutional neural network (TCNN). To correct for any further distribution differences between the source and the target domains, an adaptation layer is added to the target CNN learner, which is trained from the limited labeled target data. The experiments are run on the application of object image classification where average precision is measured as the performance metric. The Oquab [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 81" title="Oquab M, Bottou L, Laptev I, Sivic J. Learning and transferring mid-level image representations using convolutional neural networks. In: Proceedings of the 2014 IEEE conference on computer vision and pattern recognition. 2013. p. 1717–24." href="/article/10.1186/s40537-016-0043-6#ref-CR81" id="ref-link-section-d43522e4349">81</a>] method is tested against a method proposed by Marszalek [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 73" title="Marszalek M, Schmid C, Harzallah H, Van de Weijer J. Learning object representations for visual object class recognition. In: Visual recognition challenge workshop ICCV. 2007. p. 1–10." href="/article/10.1186/s40537-016-0043-6#ref-CR73" id="ref-link-section-d43522e4352">73</a>] and a method proposed by Song [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 110" title="Song Z, Chen Q, Huang Z, Hua Y, Yan S. Contextualizing object detection and classification. IEEE Trans Pattern Anal Mach Intell. 2011;37(1):13–27." href="/article/10.1186/s40537-016-0043-6#ref-CR110" id="ref-link-section-d43522e4355">110</a>]. Both the Marszalek [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 73" title="Marszalek M, Schmid C, Harzallah H, Van de Weijer J. Learning object representations for visual object class recognition. In: Visual recognition challenge workshop ICCV. 2007. p. 1–10." href="/article/10.1186/s40537-016-0043-6#ref-CR73" id="ref-link-section-d43522e4359">73</a>] and Song [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 110" title="Song Z, Chen Q, Huang Z, Hua Y, Yan S. Contextualizing object detection and classification. IEEE Trans Pattern Anal Mach Intell. 2011;37(1):13–27." href="/article/10.1186/s40537-016-0043-6#ref-CR110" id="ref-link-section-d43522e4362">110</a>] approaches are not transfer learning approaches and are trained on the limited labeled target data. The first experiment is performed using the Pascal VOC 2007 data set as the target and ImageNet 2012 as the source. The Oquab [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 81" title="Oquab M, Bottou L, Laptev I, Sivic J. Learning and transferring mid-level image representations using convolutional neural networks. In: Proceedings of the 2014 IEEE conference on computer vision and pattern recognition. 2013. p. 1717–24." href="/article/10.1186/s40537-016-0043-6#ref-CR81" id="ref-link-section-d43522e4365">81</a>] method outperformed both Song [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 110" title="Song Z, Chen Q, Huang Z, Hua Y, Yan S. Contextualizing object detection and classification. IEEE Trans Pattern Anal Mach Intell. 2011;37(1):13–27." href="/article/10.1186/s40537-016-0043-6#ref-CR110" id="ref-link-section-d43522e4368">110</a>] and Marszalek [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 73" title="Marszalek M, Schmid C, Harzallah H, Van de Weijer J. Learning object representations for visual object class recognition. In: Visual recognition challenge workshop ICCV. 2007. p. 1–10." href="/article/10.1186/s40537-016-0043-6#ref-CR73" id="ref-link-section-d43522e4371">73</a>] approaches for this test. The second experiment is performed using the Pascal VOC 2012 data set as the target and ImageNet 2012 as the source. In the second test, the Oquab [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 81" title="Oquab M, Bottou L, Laptev I, Sivic J. Learning and transferring mid-level image representations using convolutional neural networks. In: Proceedings of the 2014 IEEE conference on computer vision and pattern recognition. 2013. p. 1717–24." href="/article/10.1186/s40537-016-0043-6#ref-CR81" id="ref-link-section-d43522e4374">81</a>] method marginally outperformed the Song [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 110" title="Song Z, Chen Q, Huang Z, Hua Y, Yan S. Contextualizing object detection and classification. IEEE Trans Pattern Anal Mach Intell. 2011;37(1):13–27." href="/article/10.1186/s40537-016-0043-6#ref-CR110" id="ref-link-section-d43522e4378">110</a>] method (the Marszalek [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 73" title="Marszalek M, Schmid C, Harzallah H, Van de Weijer J. Learning object representations for visual object class recognition. In: Visual recognition challenge workshop ICCV. 2007. p. 1–10." href="/article/10.1186/s40537-016-0043-6#ref-CR73" id="ref-link-section-d43522e4381">73</a>] method was not tested for the second test). The tests successfully demonstrated the ability to transfer information from one CNN learner to another.</p><h3 class="c-article__sub-heading" id="Sec7">Parameter-based transfer learning</h3><p>The paper by Tommasi [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 114" title="Tommasi T, Orabona F, Caputo B. Safety in numbers: learning categories from few examples with multi model knowledge transfer. IEEE Conf Comput Vision Pattern Recog. 2010;2010:3081–8." href="/article/10.1186/s40537-016-0043-6#ref-CR114" id="ref-link-section-d43522e4392">114</a>] addresses the transfer learning environment characterized by limited labeled target data and multiple labeled source domains where each source corresponds to a particular class. In this case, each source is able to build a binary learner to predict that class. The objective is to build a target binary learner for a new class using minimal labeled target data and knowledge transferred from the multiple source learners. An algorithm is proposed to transfer the SVM hyperplane information of each of the source learners to the new target learner. To minimize the effects of negative transfer, the information transferred from each source to the target will be weighted such that the most related source domains receive the highest weighting. The weights are determined through a leave out one process as defined by Cawley [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 13" title="Cawley G. Leave-one-out cross-validation based model selection criteria for weighted LS-SVMs. In: IEEE 2006 international joint conference on neural network proceedings 2006. p. 1661–68." href="/article/10.1186/s40537-016-0043-6#ref-CR13" id="ref-link-section-d43522e4395">13</a>]. The Tommasi [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 114" title="Tommasi T, Orabona F, Caputo B. Safety in numbers: learning categories from few examples with multi model knowledge transfer. IEEE Conf Comput Vision Pattern Recog. 2010;2010:3081–8." href="/article/10.1186/s40537-016-0043-6#ref-CR114" id="ref-link-section-d43522e4398">114</a>] approach, called the multi-model knowledge transfer (MMKT) method, extends the method proposed by Tommasi [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 113" title="Tommasi T, Caputo B. The more you know, the less you learn: from knowledge transfer to one-shot learning of object categories. BMVC. 2009;1–11." href="/article/10.1186/s40537-016-0043-6#ref-CR113" id="ref-link-section-d43522e4401">113</a>] that only transfers a single source domain. Experiments are performed on the application of image recognition where classification accuracy is measured as the performance metric. Transfer learning methods tested include an average weight approach (same as Tommasi [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 114" title="Tommasi T, Orabona F, Caputo B. Safety in numbers: learning categories from few examples with multi model knowledge transfer. IEEE Conf Comput Vision Pattern Recog. 2010;2010:3081–8." href="/article/10.1186/s40537-016-0043-6#ref-CR114" id="ref-link-section-d43522e4404">114</a>] but all source weights are equal), and the Tommasi [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 113" title="Tommasi T, Caputo B. The more you know, the less you learn: from knowledge transfer to one-shot learning of object categories. BMVC. 2009;1–11." href="/article/10.1186/s40537-016-0043-6#ref-CR113" id="ref-link-section-d43522e4408">113</a>] approach. A baseline approach is tested, which is trained on the limited labeled target data. The best performing method is Tommasi [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 114" title="Tommasi T, Orabona F, Caputo B. Safety in numbers: learning categories from few examples with multi model knowledge transfer. IEEE Conf Comput Vision Pattern Recog. 2010;2010:3081–8." href="/article/10.1186/s40537-016-0043-6#ref-CR114" id="ref-link-section-d43522e4411">114</a>], followed by the average weight, Tommasi [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 113" title="Tommasi T, Caputo B. The more you know, the less you learn: from knowledge transfer to one-shot learning of object categories. BMVC. 2009;1–11." href="/article/10.1186/s40537-016-0043-6#ref-CR113" id="ref-link-section-d43522e4414">113</a>], and the baseline approach. As the number of labeled target instances goes up, the Tommasi [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 114" title="Tommasi T, Orabona F, Caputo B. Safety in numbers: learning categories from few examples with multi model knowledge transfer. IEEE Conf Comput Vision Pattern Recog. 2010;2010:3081–8." href="/article/10.1186/s40537-016-0043-6#ref-CR114" id="ref-link-section-d43522e4417">114</a>] and average weight methods converge to the same performance. This is because the adverse effects of negative transfer are lessened as the labeled target data increases. This result demonstrates the Tommasi [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 114" title="Tommasi T, Orabona F, Caputo B. Safety in numbers: learning categories from few examples with multi model knowledge transfer. IEEE Conf Comput Vision Pattern Recog. 2010;2010:3081–8." href="/article/10.1186/s40537-016-0043-6#ref-CR114" id="ref-link-section-d43522e4420">114</a>] approach is able to lessen the effects of negative transfer from unrelated sources.</p><p>The transfer learning approach presented in the paper by Duan [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 28" title="Duan L, Xu D, Chang SF. Exploiting web images for event recognition in consumer videos: a multiple source domain adaptation approach. In: IEEE 2012 conference on computer vision and pattern recognition. 2012. p. 1338–45." href="/article/10.1186/s40537-016-0043-6#ref-CR28" id="ref-link-section-d43522e4426">28</a>], referred to as the Domain Selection Machine (DSM), is tightly coupled to the application of event recognition in consumer videos. Event recognition in videos is the process of predicting the occurrence of a particular event or topic (e.g. “show” or “performance”) in a given video. In this scenario, the target domain is unlabeled and the source information is obtained from annotated images found via web searches. For example, a text query of the event “show” for images on Photosig.com represents one source and the same query on Flickr.com represents another separate source. The domain selection machine proposed in this paper is realized as follows. For each individual source, an SVM classifier is created using SIFT (Lowe [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 70" title="Lowe DG. Distinctive image features from scale-invariant keypoints. Int Comput Vis. 2004;60(2):91–110." href="/article/10.1186/s40537-016-0043-6#ref-CR70" id="ref-link-section-d43522e4429">70</a>]) image features. The final target classifier is made up of two parts. The first part is a weighted sum of the source classifier outputs whose input is the SIFT features from key frames of the input video. The second part is a learning function whose inputs are space–time features [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 123" title="Wang H, Klaser A, Schmid C, Liu CL (2011) Action recognition by dense trajectories. In: IEEE 2011 conference on computer vision and pattern recognition. 2011. p. 3169–76." href="/article/10.1186/s40537-016-0043-6#ref-CR123" id="ref-link-section-d43522e4432">123</a>] from the input video and is trained from target data where the target labels are estimated (pseudo labels) from the weighted sum of the source classifiers. To combat the effects of negative transfer from unrelated sources, the most relevant source domains are selected by using an alternating optimization algorithm that iteratively solves the target decision function and the domain selection vector. Experiments are performed in the application of event recognition in videos as described above where the mean average precision is measured as the performance metric. A baseline method is created by training a separate SVM classifier on each source domain and then equally combining the classifiers. The other transfer learning approaches tested include the approach by Bruzzone [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 11" title="Bruzzone L, Marconcini M. Domain adaptation problems: a DASVM classification technique and a circular validation strategy. IEEE Trans Pattern Anal Mach Intell. 2010;32(5):770–87." href="/article/10.1186/s40537-016-0043-6#ref-CR11" id="ref-link-section-d43522e4435">11</a>], Schweikert [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 101" title="Schweikert G, Widmer C, Schölkopf B, Rätsch G. An empirical analysis of domain adaptation algorithms for genomic sequence analysis. Adv Neural Inf Process Syst. 2009;21:1433–40." href="/article/10.1186/s40537-016-0043-6#ref-CR101" id="ref-link-section-d43522e4438">101</a>], Duan [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 29" title="Duan L, Xu D, Tsang IW. Domain adaptation from multiple sources: a domain-dependent regularization approach. IEEE Trans Neural Netw Learn Syst. 2012;23(3):504–18." href="/article/10.1186/s40537-016-0043-6#ref-CR29" id="ref-link-section-d43522e4442">29</a>], and Chattopadhyay [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="Chattopadhyay R, Ye J, Panchanathan S, Fan W, Davidson I. Multi-source domain adaptation and its application to early detection of fatigue. ACM Trans Knowl Dis Data (Best of SIGKDD 2011 TKDD Homepage archive) 2011; 6(4) (Article 18)." href="/article/10.1186/s40537-016-0043-6#ref-CR14" id="ref-link-section-d43522e4445">14</a>]. The Duan [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 29" title="Duan L, Xu D, Tsang IW. Domain adaptation from multiple sources: a domain-dependent regularization approach. IEEE Trans Neural Netw Learn Syst. 2012;23(3):504–18." href="/article/10.1186/s40537-016-0043-6#ref-CR29" id="ref-link-section-d43522e4448">29</a>] approach outperforms all the other approaches tested. The other approaches all have similar results, meaning the transfer learning methods did not outperform the baseline approach. The possible reason for this result is the existence of unrelated sources in the experiment. The other transfer learning approaches tested had no mechanism to guard against negative transfer from unrelated sources.</p><p>The paper by Yao [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 138" title="Yao Y, Doretto G. Boosting for transfer learning with multiple sources. In: Proceedings of the IEEE computer society conference on computer vision and pattern recognition. 2010. p. 1855–62." href="/article/10.1186/s40537-016-0043-6#ref-CR138" id="ref-link-section-d43522e4454">138</a>] first presents an instance-based transfer learning approach followed by a separate parameter-based transfer learning approach. In the transfer learning process, if the source and target domains are not related enough, negative transfer can occur. Since it is difficult to measure the relatedness between any particular source and target domain, Yao [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 138" title="Yao Y, Doretto G. Boosting for transfer learning with multiple sources. In: Proceedings of the IEEE computer society conference on computer vision and pattern recognition. 2010. p. 1855–62." href="/article/10.1186/s40537-016-0043-6#ref-CR138" id="ref-link-section-d43522e4457">138</a>] proposes to transfer knowledge from multiple source domains using a boosting method in an attempt to minimize the effects of negative transfer from a single unrelated source domain. The boosting process requires some amount of labeled target data. Yao [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 138" title="Yao Y, Doretto G. Boosting for transfer learning with multiple sources. In: Proceedings of the IEEE computer society conference on computer vision and pattern recognition. 2010. p. 1855–62." href="/article/10.1186/s40537-016-0043-6#ref-CR138" id="ref-link-section-d43522e4460">138</a>] effectively extends the work of Dai [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 21" title="Dai W, Yang Q, Xue GR, Yu Y (2007) Boosting for transfer learning. In: Proceedings of the 24th international conference on machine learning. p. 193–200." href="/article/10.1186/s40537-016-0043-6#ref-CR21" id="ref-link-section-d43522e4463">21</a>] (TrAdaBoost) by expanding the transfer boosting algorithm to multiple source domains. In the TrAdaBoost algorithm, during every boosting iteration, a so-called weak classifier is built using weighted instance data from the previous iteration. Then, the misclassified source instances are lowered in importance and the misclassified target instances are raised in importance. In the multi-source TrAdaBoost algorithm (called MsTrAdaBoost), each iteration step first finds a weak classifier for each source and target combination, and then the final weak classifier is selected for that iteration by finding the one that minimizes the target classification error. The instance reweighting step remains the same as in the TrAdaBoost. An alternative multi-source boosting method (TaskTrAdaBoost) is proposed that transfers internal learner parameter information from the source to the target. The TaskTrAdaBoost algorithm first finds candidate weak classifiers from each individual source by performing an AdaBoost process on each source domain. Then an AdaBoost process is performed on the labeled target data, and at every boosting iteration, the weak classifier used is selected from the candidate weak source classifiers (found in the previous step) that has the lowest classification error using the labeled target data. Experiments are performed for the application of object category recognition where the area under the curve (AUC) is measured as the performance metric. An AdaBoost baseline approach using only the limited labeled target data is measured along with a TrAdaBoost approach using a single source (the multiple sources are combined to one) and the limited labeled target data. Linear SVM learners are used as the base classifiers in all approaches. Both the MsTrAdaBoost and TaskTrAdaBoost approaches outperform the baseline approach and TrAdaBoost approach. The MsTrAdaBoost and TaskTrAdaBoost demonstrated similar performance.</p><h3 class="c-article__sub-heading" id="Sec8">Relational-based transfer learning</h3><p>The specific application addressed in the paper by Li [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 62" title="Li F, Pan SJ, Jin O, Yang Q, Zhu X. Cross-domain co-extraction of sentiment and topic lexicons. In: Proceedings of the 50th annual meeting of the association for computational linguistics long papers, vol. 1. 2012. p. 410–19." href="/article/10.1186/s40537-016-0043-6#ref-CR62" id="ref-link-section-d43522e4474">62</a>] is to classify words from a text document into one of three classes (e.g. sentiments, topics, or neither). In this scenario, there exists a labeled text source domain on one particular subject matter and an unlabeled text target domain on a different subject matter. The main idea is that sentiment words remain constant between the source and target domains. By learning the grammatical and sentence structure patterns of the source, a relational pattern is found between the source and target domains, which is used to predict the topic words in the target. The sentiment words act as a common linkage or bridge between the source and target domains. A bipartite word graph is used to represent and score the sentence structure patterns. A bootstrapping algorithm is used to iteratively build a target classifier from the two domains. The bootstrapping process starts with defining seeds which are instances from the source that match frequent patterns in the target. A cross domain classifier is then trained with the seed information and extracted target information (there is no target information in the first iteration). The classifier is used to predict the target labels and the top confidence rated target instances are selected to reconstruct the bipartite word graph. The bipartite word graph is now used to select new target instances that are added to the seed list. This bootstrapping process continues over a selected number of iterations, and the cross domain classifier learned in the bootstrapping process is now available to predict target samples. This method is referred to as the Relational Adaptive bootstraPping (RAP) approach. The experiments tested the Li [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 62" title="Li F, Pan SJ, Jin O, Yang Q, Zhu X. Cross-domain co-extraction of sentiment and topic lexicons. In: Proceedings of the 50th annual meeting of the association for computational linguistics long papers, vol. 1. 2012. p. 410–19." href="/article/10.1186/s40537-016-0043-6#ref-CR62" id="ref-link-section-d43522e4477">62</a>] approach against an upper bound method where a standard classifier is trained with a large amount of target data. Other transfer learning methods tested include an approach by Hu [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 50" title="Hu M, Liu B. Mining and summarizing customer reviews. In: Proceedings of the 10th ACM SIGKDD international conference on Knowledge discovery and data mining. 2004. p. 168–77." href="/article/10.1186/s40537-016-0043-6#ref-CR50" id="ref-link-section-d43522e4480">50</a>], Qiu [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 93" title="Qiu G, Liu B, Bu J, Chen C. Expanding domain sentiment lexicon through double propagation. In: Proceedings of the 21st international joint conference on artificial intelligence. p. 1199–204." href="/article/10.1186/s40537-016-0043-6#ref-CR93" id="ref-link-section-d43522e4483">93</a>], Jakob [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 52" title="Jakob N, Gurevych I. Extracting opinion targets in a single and cross-domain setting with conditional random fields. In: Proceedings of the 2010 conference on empirical methods in NLP. 2010. p. 1035–45." href="/article/10.1186/s40537-016-0043-6#ref-CR52" id="ref-link-section-d43522e4486">52</a>], and Dai [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 21" title="Dai W, Yang Q, Xue GR, Yu Y (2007) Boosting for transfer learning. In: Proceedings of the 24th international conference on machine learning. p. 193–200." href="/article/10.1186/s40537-016-0043-6#ref-CR21" id="ref-link-section-d43522e4490">21</a>]. The application tested is word classification as described above where the F1 score is measured as the performance metric. The two domains tested are related to movie reviews and product reviews. The Li [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 62" title="Li F, Pan SJ, Jin O, Yang Q, Zhu X. Cross-domain co-extraction of sentiment and topic lexicons. In: Proceedings of the 50th annual meeting of the association for computational linguistics long papers, vol. 1. 2012. p. 410–19." href="/article/10.1186/s40537-016-0043-6#ref-CR62" id="ref-link-section-d43522e4493">62</a>] method performed better than the other transfer learning methods, but fell short of the upper bound method as expected. In its current form, this algorithm is tightly coupled with its underlying text application, which makes it difficult to use for other non-text applications.</p><h3 class="c-article__sub-heading" id="Sec9">Hybrid-based (instance and parameter) transfer learning</h3><p>The paper by Xia [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 132" title="Xia R, Zong C, Hu X, Cambria E. Feature ensemble plus sample selection: domain adaptation for sentiment classification. IEEE Intell Syst. 2013;28(3):10–8." href="/article/10.1186/s40537-016-0043-6#ref-CR132" id="ref-link-section-d43522e4504">132</a>] proposes a two step approach to address marginal distribution differences and conditional distribution differences between the source and target domains called the sample selection and feature ensemble (SSFE) method. A sample selection process, using a modified version of principal component analysis, is employed to select labeled source domain samples such that the source and target marginal distributions are equalized. Next, a feature ensemble step attempts to resolve the conditional distribution differences between the source and target domains. Four individual classifiers are defined corresponding to parts of speech of noun, verb, adverb/adjective, and other. The four classifiers are trained using only the features that correspond to that part of speech. The training data is the limited labeled target and the labeled source selected in the previous sample selection step. The four classifiers are weighted as a function of minimizing the classification error using the limited labeled target data. The weighted output of the four classifiers is used as the final target classifier. This work by Xia [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 132" title="Xia R, Zong C, Hu X, Cambria E. Feature ensemble plus sample selection: domain adaptation for sentiment classification. IEEE Intell Syst. 2013;28(3):10–8." href="/article/10.1186/s40537-016-0043-6#ref-CR132" id="ref-link-section-d43522e4507">132</a>] extends the earlier work of Xia [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 131" title="Xia R, Zong C. A POS-based ensemble model for cross-domain sentiment classification. In: Proceedings of the 5th international joint conference on natural language processing. 2011. p. 614–22." href="/article/10.1186/s40537-016-0043-6#ref-CR131" id="ref-link-section-d43522e4510">131</a>]. The experiments are performed for the application of review sentiment classification using four different review categories, where each category is combined to create 12 different source and target pairs. Classification accuracy is measured as the performance metric. A baseline approach using all the training data from the source is constructed, along with a sample selection approach (only using the first step defined above), a feature ensemble approach (only using the second step defined above) and the complete approach outlined above. The complete approach is the best performing, followed by sample selection and feature ensemble approaches, and the baseline approach. The sample selection and feature ensemble approaches perform equally as well in head-to-head tests. The weighting of the four classifiers (defined by the corresponding parts of speech) in the procedure above gives limited resolution in attempting to adjust for context feature bias issues. A method of having more classifiers in the ensemble step could yield better performance at the expense of higher complexity.</p><h3 class="c-article__sub-heading" id="Sec10">Discussion of homogeneous transfer learning</h3><p>The previous surveyed homogeneous transfer learning works (summarized in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1186/s40537-016-0043-6#Tab2">2</a>) demonstrate many different characteristics and attributes. Which homogeneous transfer learning solution is best for a particular application? An important characteristic to evaluate in the selection process is what type of differences exist between a given source and target domain. The previous solutions surveyed address domain adaptation by correcting for marginal distribution differences, correcting for conditional distribution differences, or correcting for both marginal and conditional distribution differences. The surveyed works of Duan [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 27" title="Duan L, Tsang IW, Xu D. Domain transfer multiple kernel learning. IEEE Trans Pattern Anal Mach Intell. 2012;34(3):465–79." href="/article/10.1186/s40537-016-0043-6#ref-CR27" id="ref-link-section-d43522e4524">27</a>], Gong [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 42" title="Gong B, Shi Y, Sha F, Grauman K. Geodesic flow kernel for unsupervised domain adaptation. In: Proceedings of the 2012 IEEE conference on computer vision and pattern recognition. 2012. p. 2066–73." href="/article/10.1186/s40537-016-0043-6#ref-CR42" id="ref-link-section-d43522e4527">42</a>], Pan [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 87" title="Pan SJ, Tsang IW, Kwok JT, Yang Q. Domain adaptation via transfer component analysis. IEEE Trans Neural Netw. 2009;22(2):199–210." href="/article/10.1186/s40537-016-0043-6#ref-CR87" id="ref-link-section-d43522e4530">87</a>], Li [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 62" title="Li F, Pan SJ, Jin O, Yang Q, Zhu X. Cross-domain co-extraction of sentiment and topic lexicons. In: Proceedings of the 50th annual meeting of the association for computational linguistics long papers, vol. 1. 2012. p. 410–19." href="/article/10.1186/s40537-016-0043-6#ref-CR62" id="ref-link-section-d43522e4533">62</a>], Shi [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 106" title="Shi Y, Sha F. Information-theoretical learning of discriminative clusters for unsupervised domain adaptation. In: Proceedings of the 29th international conference on machine learning. 2012. p. 1–8." href="/article/10.1186/s40537-016-0043-6#ref-CR106" id="ref-link-section-d43522e4537">106</a>], Oquab [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 81" title="Oquab M, Bottou L, Laptev I, Sivic J. Learning and transferring mid-level image representations using convolutional neural networks. In: Proceedings of the 2014 IEEE conference on computer vision and pattern recognition. 2013. p. 1717–24." href="/article/10.1186/s40537-016-0043-6#ref-CR81" id="ref-link-section-d43522e4540">81</a>], Glorot [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 41" title="Glorot X, Bordes A, Bengio Y. Domain adaptation for large-scale sentiment classification: A deep learning approach. In: Proceedings of the twenty-eight international conference on machine learning, vol. 27. 2011. p. 97–110." href="/article/10.1186/s40537-016-0043-6#ref-CR41" id="ref-link-section-d43522e4543">41</a>], and Pan [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 83" title="Pan SJ, Ni X, Sun JT, Yang Q, Chen Z. Cross-domain sentiment classification via spectral feature alignment. In: Proceedings of the 19th international conference on world wide web. 2010. p. 751–60." href="/article/10.1186/s40537-016-0043-6#ref-CR83" id="ref-link-section-d43522e4546">83</a>] are focused on solving the differences in marginal distribution between the source and target domains. The surveyed works of Daumé [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 22" title="Daumé H III. Frustratingly easy domain adaptation. In: Proceedings of ACL. 2007. p. 256–63." href="/article/10.1186/s40537-016-0043-6#ref-CR22" id="ref-link-section-d43522e4549">22</a>], Yao [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 138" title="Yao Y, Doretto G. Boosting for transfer learning with multiple sources. In: Proceedings of the IEEE computer society conference on computer vision and pattern recognition. 2010. p. 1855–62." href="/article/10.1186/s40537-016-0043-6#ref-CR138" id="ref-link-section-d43522e4552">138</a>], Tommasi [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 114" title="Tommasi T, Orabona F, Caputo B. Safety in numbers: learning categories from few examples with multi model knowledge transfer. IEEE Conf Comput Vision Pattern Recog. 2010;2010:3081–8." href="/article/10.1186/s40537-016-0043-6#ref-CR114" id="ref-link-section-d43522e4556">114</a>] are focused on solving the differences in conditional distribution between the source and target domains. Lastly, the surveyed works of Long [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 68" title="Long M, Wang J, Ding G, Pan SJ, Yu PS. Adaptation regularization: a general framework for transfer learning. IEEE Trans Knowl Data Eng. 2014;26(5):1076–89." href="/article/10.1186/s40537-016-0043-6#ref-CR68" id="ref-link-section-d43522e4559">68</a>], Xia [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 132" title="Xia R, Zong C, Hu X, Cambria E. Feature ensemble plus sample selection: domain adaptation for sentiment classification. IEEE Intell Syst. 2013;28(3):10–8." href="/article/10.1186/s40537-016-0043-6#ref-CR132" id="ref-link-section-d43522e4562">132</a>], Chattopadhyay [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="Chattopadhyay R, Ye J, Panchanathan S, Fan W, Davidson I. Multi-source domain adaptation and its application to early detection of fatigue. ACM Trans Knowl Dis Data (Best of SIGKDD 2011 TKDD Homepage archive) 2011; 6(4) (Article 18)." href="/article/10.1186/s40537-016-0043-6#ref-CR14" id="ref-link-section-d43522e4565">14</a>], Duan [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 28" title="Duan L, Xu D, Chang SF. Exploiting web images for event recognition in consumer videos: a multiple source domain adaptation approach. In: IEEE 2012 conference on computer vision and pattern recognition. 2012. p. 1338–45." href="/article/10.1186/s40537-016-0043-6#ref-CR28" id="ref-link-section-d43522e4568">28</a>], and Long [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 69" title="Long M, Wang J, Ding G, Sun J, Yu PS. Transfer feature learning with joint distribution adaptation. In: Proceedings of the 2013 IEEE international conference on computer vision. 2013. p. 2200–07." href="/article/10.1186/s40537-016-0043-6#ref-CR69" id="ref-link-section-d43522e4571">69</a>] correct the differences in both the marginal and conditional distributions. Correcting for the conditional distribution differences between the source and target domain can be problematic as the nature of a transfer learning environment is to have minimal labeled target data. To compensate for the limited labeled target data, many of the recent transfer learning solutions create pseudo labels for the unlabeled target data to facilitate the conditional distribution correction process between the source and target domains. To further help determine which solution is best for a given transfer learning application, the information in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1186/s40537-016-0043-6#Tab2">2</a> should be used to match the characteristics of the solution to that of the desired application environment. If the application domain contains multiple sources where the sources are not mutually uniformly distributed, a solution that guards against negative transfer may be of greater benefit. A recent trend in the development of transfer learning solutions is for solutions to address both marginal and conditional distribution differences between the source and target domains. Another emerging solution trend is the implementation of a one-stage process as compared to a two-stage process. In the recent works of Long [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 68" title="Long M, Wang J, Ding G, Pan SJ, Yu PS. Adaptation regularization: a general framework for transfer learning. IEEE Trans Knowl Data Eng. 2014;26(5):1076–89." href="/article/10.1186/s40537-016-0043-6#ref-CR68" id="ref-link-section-d43522e4578">68</a>], Duan [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 27" title="Duan L, Tsang IW, Xu D. Domain transfer multiple kernel learning. IEEE Trans Pattern Anal Mach Intell. 2012;34(3):465–79." href="/article/10.1186/s40537-016-0043-6#ref-CR27" id="ref-link-section-d43522e4581">27</a>], Shi [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 106" title="Shi Y, Sha F. Information-theoretical learning of discriminative clusters for unsupervised domain adaptation. In: Proceedings of the 29th international conference on machine learning. 2012. p. 1–8." href="/article/10.1186/s40537-016-0043-6#ref-CR106" id="ref-link-section-d43522e4584">106</a>], and Xia [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 132" title="Xia R, Zong C, Hu X, Cambria E. Feature ensemble plus sample selection: domain adaptation for sentiment classification. IEEE Intell Syst. 2013;28(3):10–8." href="/article/10.1186/s40537-016-0043-6#ref-CR132" id="ref-link-section-d43522e4587">132</a>], a one-stage process is employed that simultaneously performs the domain adaptation process while learning the final classifier. A two-stage solution first performs the domain adaptation process and then independently learns the final classifier. The claim by Long [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 68" title="Long M, Wang J, Ding G, Pan SJ, Yu PS. Adaptation regularization: a general framework for transfer learning. IEEE Trans Knowl Data Eng. 2014;26(5):1076–89." href="/article/10.1186/s40537-016-0043-6#ref-CR68" id="ref-link-section-d43522e4590">68</a>] is a one-stage solution achieves enhanced performance because the simultaneous solving of domain adaptation and the classifier establishes mutual reinforcement. The surveyed homogeneous transfer learning works are not specifically applied to big data solutions; however, there is nothing to preclude their use in a big data environment.</p></div></div></section><section aria-labelledby="Sec11" data-title="Heterogeneous transfer learning"><div class="c-article-section" id="Sec11-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec11">Heterogeneous transfer learning</h2><div class="c-article-section__content" id="Sec11-content"><p>Heterogeneous transfer learning is the scenario where the source and target domains are represented in different feature spaces. There are many applications where heterogeneous transfer learning is beneficial. Heterogeneous transfer learning applications that are covered in this section include image recognition [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 30" title="Duan L, Xu D, Tsang IW. Learning with augmented features for heterogeneous domain adaptation. IEEE Trans Pattern Anal Mach Intell. 2012;36(6):1134–48." href="/article/10.1186/s40537-016-0043-6#ref-CR30" id="ref-link-section-d43522e4602">30</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 58" title="Kulis B, Saenko K, Darrell T. What you saw is not what you get: domain adaptation using asymmetric kernel transforms. In: IEEE 2011 conference on computer vision and pattern recognition. 2011. p. 1785–92." href="/article/10.1186/s40537-016-0043-6#ref-CR58" id="ref-link-section-d43522e4605">58</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 146" title="Zhu Y, Chen Y, Lu Z, Pan S, Xue G, Yu Y, Yang Q. Heterogeneous transfer learning for image classification. In: Proceedings of the national conference on artificial intelligence, vol. 2. 2011. p. 1304–9." href="/article/10.1186/s40537-016-0043-6#ref-CR146" id="ref-link-section-d43522e4608">146</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 105" title="Shi X, Liu Q, Fan W, Yu PS, Zhu R. Transfer learning on heterogeneous feature spaces via spectral transformation. In: 2010 IEEE international conference on data mining. 2010. p. 1049–1054." href="/article/10.1186/s40537-016-0043-6#ref-CR105" id="ref-link-section-d43522e4611">105</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 92" title="Qi GJ, Aggarwal C, Huang T. Towards semantic knowledge propagation from text corpus to Web images. In: Proceedings of the 20th international conference on world wide web. p. 297–306." href="/article/10.1186/s40537-016-0043-6#ref-CR92" id="ref-link-section-d43522e4614">92</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 64" title="Li W, Duan L, Xu D, Tsang IW. Learning with augmented features for supervised and semi-supervised heterogeneous domain adaptation. IEEE Trans Pattern Anal Mach Intell. 2014;36(6):1134–48." href="/article/10.1186/s40537-016-0043-6#ref-CR64" id="ref-link-section-d43522e4618">64</a>], multi-language text classification [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 145" title="Zhou JT, Tsang IW, Pan SJ Tan M. Heterogeneous domain adaptation for multiple classes. In: International conference on artificial intelligence and statistics. 2014. p. 1095–103." href="/article/10.1186/s40537-016-0043-6#ref-CR145" id="ref-link-section-d43522e4621">145</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 30" title="Duan L, Xu D, Tsang IW. Learning with augmented features for heterogeneous domain adaptation. IEEE Trans Pattern Anal Mach Intell. 2012;36(6):1134–48." href="/article/10.1186/s40537-016-0043-6#ref-CR30" id="ref-link-section-d43522e4624">30</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 91" title="Prettenhofer P, Stein B. (2010) Cross-language text classification using structural correspondence learning. In: Proceedings of the 48th annual meeting of the association for computational linguistics. 2010. p. 1118–27." href="/article/10.1186/s40537-016-0043-6#ref-CR91" id="ref-link-section-d43522e4627">91</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 144" title="Zhou JT, Pan S, Tsang IW, Yan Y. Hybrid heterogeneous transfer learning through deep learning. In: Proceedings of the national conference on artificial intelligence, vol. 3. 2014. p. 2213–20." href="/article/10.1186/s40537-016-0043-6#ref-CR144" id="ref-link-section-d43522e4630">144</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 64" title="Li W, Duan L, Xu D, Tsang IW. Learning with augmented features for supervised and semi-supervised heterogeneous domain adaptation. IEEE Trans Pattern Anal Mach Intell. 2014;36(6):1134–48." href="/article/10.1186/s40537-016-0043-6#ref-CR64" id="ref-link-section-d43522e4633">64</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 124" title="Wei B, Pal C (2010) Cross-lingual adaptation: an experiment on sentiment classifications. In: Proceedings of the ACL 2010 conference short papers. 2010. p. 258–62." href="/article/10.1186/s40537-016-0043-6#ref-CR124" id="ref-link-section-d43522e4637">124</a>], single language text classification [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 121" title="Wang C, Mahadevan S. Heterogeneous domain adaptation using manifold alignment. In: Proceedings of the 22nd international joint conference on artificial intelligence, vol. 2. 2011. p. 541–46." href="/article/10.1186/s40537-016-0043-6#ref-CR121" id="ref-link-section-d43522e4640">121</a>], drug efficacy classification [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 105" title="Shi X, Liu Q, Fan W, Yu PS, Zhu R. Transfer learning on heterogeneous feature spaces via spectral transformation. In: 2010 IEEE international conference on data mining. 2010. p. 1049–1054." href="/article/10.1186/s40537-016-0043-6#ref-CR105" id="ref-link-section-d43522e4643">105</a>], human activity classification [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 46" title="Harel M, Mannor S. Learning from multiple outlooks. In: Proceedings of the 28th international conference on machine learning. 2011. p. 401–8." href="/article/10.1186/s40537-016-0043-6#ref-CR46" id="ref-link-section-d43522e4646">46</a>], and software defect classification [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 77" title="Nam J, Kim S (2015) Heterogeneous defect prediction. In: Proceedings of the 2015 10th joint meeting on foundations of software engineering. 2015. p. 508–19." href="/article/10.1186/s40537-016-0043-6#ref-CR77" id="ref-link-section-d43522e4649">77</a>]. Heterogeneous transfer learning is also directly applicable to a big data environment. As repositories of big data become more available, there is a desire to use this abundant resource for machine learning tasks, avoiding the timely and potentially costly collection of new data. If there is an available dataset drawn from a target domain of interest that has a different feature space from another target dataset (also drawn from the same target domain), then heterogeneous transfer learning can be used to bridge the difference in the feature spaces and build a predictive model for that target domain. Heterogeneous transfer learning is still a relatively new area of study as the majority of the works covering this topic have been published in the last 5 years. From a high-level view, there are two main approaches to solving the heterogeneous feature space difference. The first approach, referred to as symmetric transformation shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1186/s40537-016-0043-6#Fig1">1</a>a, separately transforms the source and target domains into a common latent feature space in an attempt to unify the input spaces of the domains. The second approach, referred to as asymmetric transformation as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1186/s40537-016-0043-6#Fig1">1</a>b, transforms the source feature space to the target feature space to align the input feature spaces. The asymmetrical transformation approach is best used when the same class instances in the source and target can be transformed without context feature bias. Many of the heterogeneous transfer learning solutions surveyed make the implicit or explicit assumption that the source and the target domain instances are drawn from the same domain space. With this assumption there should be no significant distribution differences between the domains. Therefore, once the differences in input feature spaces are resolved, no further domain adaptation needs to be performed.</p><p>As is the case with homogeneous transfer learning solutions, whether the source and target domains contain labeled data drives the solution formulation for heterogeneous approaches. Data label availability is a function of the underlying application. The solutions surveyed in this paper have different labeled data requirements. For transfer learning to be feasible, the source and the target domains must be related in some way. Some heterogeneous solutions require an explicit mapping of the relationship or correspondence between the source and target domains. For example, the solutions defined for Prettenhofer [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 91" title="Prettenhofer P, Stein B. (2010) Cross-language text classification using structural correspondence learning. In: Proceedings of the 48th annual meeting of the association for computational linguistics. 2010. p. 1118–27." href="/article/10.1186/s40537-016-0043-6#ref-CR91" id="ref-link-section-d43522e4662">91</a>] and Wei [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 125" title="Wei B, Pal C (2011) Heterogeneous transfer learning with RBMs. In: Proceedings of the twenty-fifth AAAI conference on artificial intelligence. 2011. p. 531–36." href="/article/10.1186/s40537-016-0043-6#ref-CR125" id="ref-link-section-d43522e4665">125</a>] require manual definitions of source and target correspondence.</p><h3 class="c-article__sub-heading" id="Sec12">Symmetric feature-based transfer learning</h3><p>The transfer learning approach proposed by Prettenhofer [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 91" title="Prettenhofer P, Stein B. (2010) Cross-language text classification using structural correspondence learning. In: Proceedings of the 48th annual meeting of the association for computational linguistics. 2010. p. 1118–27." href="/article/10.1186/s40537-016-0043-6#ref-CR91" id="ref-link-section-d43522e4675">91</a>] addresses the heterogeneous scenario of a source domain containing labeled and unlabeled data, and a target domain containing unlabeled data. The structural correspondence learning technique from Blitzer [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 5" title="Blitzer, J, McDonald R, Pereira F. Domain adaptation with structural correspondence learning. In: Proceedings of the 2006 conference on empirical methods in natural language processing. 2006;120–8." href="/article/10.1186/s40537-016-0043-6#ref-CR5" id="ref-link-section-d43522e4678">5</a>] is applied to this problem. Structural correspondence learning depends on the manual definition of pivot functions that capture correspondence between the source and target domains. Effective pivot functions should use features that occur frequently in both domains and have good predictive qualities. Each pivot function is turned into a linear classifier using data from the source and target domains. From these pivot classifiers, correspondences between features are discovered and a latent feature space is learned. The latent feature space is used to train the final target classifier. The paper by Prettenhofer [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 91" title="Prettenhofer P, Stein B. (2010) Cross-language text classification using structural correspondence learning. In: Proceedings of the 48th annual meeting of the association for computational linguistics. 2010. p. 1118–27." href="/article/10.1186/s40537-016-0043-6#ref-CR91" id="ref-link-section-d43522e4681">91</a>] uses this solution to solve the problem of text classification where the source is written in one language and the target is written in a different language. In this specific implementation referred to as cross-language structural correspondence learning (CLSCL), the pivot functions are defined by pairs of words, one from the target and one from the source, that represent direct word translations from one language to the other. The experiments are performed on the applications of document sentiment classification and document topic classification. English documents are used in the source and other language documents are used in the target. The baseline method used in this test trains a learner on the labeled source documents, then translates the target documents to the source language and tests the translated version. An upper bound method is established by training a learner with the labeled target documents and testing with the target documents. Average classification accuracy is measured as the performance metric. The average results show the upper bound method performing the best and the Prettenhofer [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 91" title="Prettenhofer P, Stein B. (2010) Cross-language text classification using structural correspondence learning. In: Proceedings of the 48th annual meeting of the association for computational linguistics. 2010. p. 1118–27." href="/article/10.1186/s40537-016-0043-6#ref-CR91" id="ref-link-section-d43522e4684">91</a>] method performing better than the baseline method. An issue with using structural correspondence learning is the difficulty in generalizing the pivot functions. For this solution, the pivot functions need to be manually and uniquely defined for a specific application, which makes it very difficult to port to other applications.</p><p>The paper by Shi [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 105" title="Shi X, Liu Q, Fan W, Yu PS, Zhu R. Transfer learning on heterogeneous feature spaces via spectral transformation. In: 2010 IEEE international conference on data mining. 2010. p. 1049–1054." href="/article/10.1186/s40537-016-0043-6#ref-CR105" id="ref-link-section-d43522e4690">105</a>], referred to as Heterogeneous Spectral Mapping (HeMap), addresses the specific transfer learning scenario where the input feature space is different between the source and target <span class="mathjax-tex">\({\mathcal{X}}_{{\mathcal{S}}} \, \ne \,{\mathcal{X}}_{{\mathcal{T}}}\)</span>, the marginal distribution is different between the source and the target (<span class="mathjax-tex">\({\text{P}}({\text{X}}_{\text{S}} )\, \ne \,{\text{P}}\left( {{\text{X}}_{\text{T}} } \right)\text{ }\)</span>), and the output space is different between the source and the target (<span class="mathjax-tex">\({\mathcal{Y}}_{{\mathcal{S}}} \, \ne {\mathcal{Y}}_{{\mathcal{T}}}\)</span>). This solution uses labeled source data that is related to the target domain and limited labeled target data. The first step is to find a common latent input space between the source and target domains using a spectral mapping technique. The spectral mapping technique is modeled as an optimization objective that maintains the original structure of the data while minimizing the difference between the two domains. The next step is to apply a clustering based sample selection method to select related instances as new training data, which resolves the marginal distribution differences in the latent input space. Finally, a Bayesian based method is used to find the relationship and resolve the differences in the output space. Experiments are performed for the applications of image classification and drug efficacy prediction. Classification error rate is measured as the performance metric. This solution demonstrated better performance as compared to a baseline approach; however, details on the baseline approach are not documented in the paper and no other transfer learning solutions are tested.</p><p>The algorithm by Wang [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 121" title="Wang C, Mahadevan S. Heterogeneous domain adaptation using manifold alignment. In: Proceedings of the 22nd international joint conference on artificial intelligence, vol. 2. 2011. p. 541–46." href="/article/10.1186/s40537-016-0043-6#ref-CR121" id="ref-link-section-d43522e4841">121</a>], referred to as the domain adaptation manifold alignment (DAMA) algorithm, proposes using a manifold alignment [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 45" title="Ham JH, Lee DD, Saul LK. Learning high dimensional correspondences from low dimensional manifolds. In: Proceedings of the twentieth international conference on machine learning. 2003. p. 1–8." href="/article/10.1186/s40537-016-0043-6#ref-CR45" id="ref-link-section-d43522e4844">45</a>] process to perform a symmetric transformation of the domain input spaces. In this solution, there are multiple labeled source domains and a limited labeled target domain for a total of K domains where all K domains share the same output label space. The approach is to create a separate mapping function for each domain to transform the heterogeneous input space to a common latent input space while preserving the underlying structure of each domain. Each domain is modeled as a manifold. To create the latent input space, a larger matrix model is created that represents and captures the joint manifold union of all input domains. In this manifold model, each domain is represented by a Laplacian matrix that captures the closeness to other instances sharing the same label. The instances with the same labels are forced to be neighbors while separating the instances with different labels. A dimensionality reduction step is performed through a generalized eigenvalue decomposition process to eliminate feature redundancy. The final learner is built in two stages. The first stage is a linear regression model trained on the source data using the latent feature space. The second stage is also a linear regression model that is summed with the first stage. The second stage uses a manifold regularization [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 4" title="Belkin M, Niyogi P, Sindhwani V. Manifold regularization: a geometric framework for learning from examples. J Mach Learn Res Arch. 2006;7:2399–434." href="/article/10.1186/s40537-016-0043-6#ref-CR4" id="ref-link-section-d43522e4847">4</a>] process to ensure the prediction error is minimized when using the labeled target data. The first stage is trained only using the source data and the second stage compensates for the domain differences caused by the first stage to achieve enhanced target predictions. The experiments are focused on the application of document text classification where classification accuracy is measured as the performance metric. The methods tested against include a canonical correlation analysis approach and a manifold regularization approach, which is considered the baseline method. The baseline method uses the limited labeled target domain data and does not use source domain information. The approach presented in this paper substantially outperforms the canonical correlation analysis and baseline approach; however, these approaches are not directly referenced so it is difficult to understand the significance of the test results. A unique aspect of this paper is the modeling of multiple source domains in a heterogeneous solution.</p><p>There are scenarios where a large amount of unlabeled heterogeneous source data is readily available that could be used to improve the predictive performance of a particular target learner. The paper by Zhu [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 146" title="Zhu Y, Chen Y, Lu Z, Pan S, Xue G, Yu Y, Yang Q. Heterogeneous transfer learning for image classification. In: Proceedings of the national conference on artificial intelligence, vol. 2. 2011. p. 1304–9." href="/article/10.1186/s40537-016-0043-6#ref-CR146" id="ref-link-section-d43522e4853">146</a>], which presents the method called the Heterogeneous transfer learning image classification (HTLIC), addresses this scenario with the assumption of having access to a sufficiently large amount of labeled target data. The objective is to use the large supply of available unlabeled source data to create a common latent feature input space that will improve prediction performance in the target classifier. The solution proposed by Zhu [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 146" title="Zhu Y, Chen Y, Lu Z, Pan S, Xue G, Yu Y, Yang Q. Heterogeneous transfer learning for image classification. In: Proceedings of the national conference on artificial intelligence, vol. 2. 2011. p. 1304–9." href="/article/10.1186/s40537-016-0043-6#ref-CR146" id="ref-link-section-d43522e4856">146</a>] is tightly coupled to the application of image classification and is described as follows. Images with labeled categories (e.g. dog, cake, starfish, etc.) are available in the target domain. To obtain the source data, a web search is performed from Flickr for images that “relate” to the labeled categories. For example, for the category of dog, the words dog, doggy, and greyhound may be used in the Flickr search. As a reference point, the idea of using annotated images from Flickr as unlabeled source data was first proposed by Yang [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 137" title="Yang Q, Chen Y, Xue GR, Dai W, Yu Y. Heterogeneous transfer learning for image clustering via the social web. In: Proceedings of the joint conference of the 47th annual meeting of the ACL, vol. 1. 2009. p. 1–9." href="/article/10.1186/s40537-016-0043-6#ref-CR137" id="ref-link-section-d43522e4859">137</a>]. The retrieved images from Flickr have one or more word tags associated with each image. These tagged image words are then used to search for text documents using Google search. Next, a two-layer bipartite graph is constructed where the first layer represents linkages between the source images and the image tags. The second layer represents linkages between the image tags and the text documents. If an image tag appears in a text document, then a link is created, otherwise there is no link. Images in both the source and the target are initially represented by an input feature set that is derived from the pixel information using SIFT descriptors [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 70" title="Lowe DG. Distinctive image features from scale-invariant keypoints. Int Comput Vis. 2004;60(2):91–110." href="/article/10.1186/s40537-016-0043-6#ref-CR70" id="ref-link-section-d43522e4862">70</a>]. Using the initial source image features and the bipartite graph representation derived only from the source image tags and text data, a common latent semantic feature set is learned by employing Latent Semantic Analysis [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 24" title="Deerwester S, Dumais ST, Furnas GW, Landauer TK, Harshman R. Indexing by latent semantic analysis. J Am Soc Inf Sci. 1990;41:391–407." href="/article/10.1186/s40537-016-0043-6#ref-CR24" id="ref-link-section-d43522e4865">24</a>]. A learner is now trained with the transformed labeled target instances. Experiments are performed on the proposed approach where 19 different image categories are selected. Binary classification is performed testing different image category pairs. A baseline method is tested using an SVM classifier trained only with the labeled target data. Methods by Raina [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 95" title="Raina R, Battle A, Lee H, Packer B, Ng AY. Self-taught learning: transfer learning from unlabeled data. In: Proceedings of the 24th international conference on machine learning. 2007. p. 759–66." href="/article/10.1186/s40537-016-0043-6#ref-CR95" id="ref-link-section-d43522e4869">95</a>] and by Wang [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 122" title="Wang G, Hoiem D, Forsyth DA. Building text Features for object image classification. In: 2009 IEEE conference on computer vision and pattern recognition. 2009. p. 1367–74." href="/article/10.1186/s40537-016-0043-6#ref-CR122" id="ref-link-section-d43522e4872">122</a>] are also tested. The approach proposed by Zhu [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 146" title="Zhu Y, Chen Y, Lu Z, Pan S, Xue G, Yu Y, Yang Q. Heterogeneous transfer learning for image classification. In: Proceedings of the national conference on artificial intelligence, vol. 2. 2011. p. 1304–9." href="/article/10.1186/s40537-016-0043-6#ref-CR146" id="ref-link-section-d43522e4875">146</a>] performed the best overall followed by Raina [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 95" title="Raina R, Battle A, Lee H, Packer B, Ng AY. Self-taught learning: transfer learning from unlabeled data. In: Proceedings of the 24th international conference on machine learning. 2007. p. 759–66." href="/article/10.1186/s40537-016-0043-6#ref-CR95" id="ref-link-section-d43522e4878">95</a>], Wang [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 122" title="Wang G, Hoiem D, Forsyth DA. Building text Features for object image classification. In: 2009 IEEE conference on computer vision and pattern recognition. 2009. p. 1367–74." href="/article/10.1186/s40537-016-0043-6#ref-CR122" id="ref-link-section-d43522e4881">122</a>], and baseline approach. The idea of using an abundant source of unlabeled data available through an internet search to improve prediction performance is a very alluring premise. However, this method is very specific to image classification and is enabled by having a web site like Flickr, which essentially provides unlimited labeled image data. This method is difficult to port to other applications.</p><p>The transfer learning solution proposed by Qi [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 92" title="Qi GJ, Aggarwal C, Huang T. Towards semantic knowledge propagation from text corpus to Web images. In: Proceedings of the 20th international conference on world wide web. p. 297–306." href="/article/10.1186/s40537-016-0043-6#ref-CR92" id="ref-link-section-d43522e4888">92</a>] is another example of an approach that specifically addresses the application of image classification. In the paper by Qi [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 92" title="Qi GJ, Aggarwal C, Huang T. Towards semantic knowledge propagation from text corpus to Web images. In: Proceedings of the 20th international conference on world wide web. p. 297–306." href="/article/10.1186/s40537-016-0043-6#ref-CR92" id="ref-link-section-d43522e4891">92</a>], the author claims the application of image classification is inherently more difficult than text classification because image features are not directly related to semantic concepts inherent in class labels. Image features are derived from pixel information, which is not semantically related to class labels, as opposed to word features that have semantic interpretability to class labels. Further, labeled image data is more scarce as compared to labeled text data. Therefore, a transfer learning environment for image classification is desired where an abundance of labeled text data (source) is used to enhance a learner trained on limited labeled image data (target). In this solution, text documents are identified by performing a web search (from Wikipedia for example) on class labels. In order to perform the knowledge transfer from the text documents (source) to the image (target) domain, a bridge in the form of a co-occurrence matrix is used that relates the text and image information. The co-occurrence matrix contains text instances with the corresponding image instances that are found in that particular text document. The co-occurrence matrix can be programmatically built by crawling web pages and extracting the relevant text and image feature information. Using the co-occurrence matrix, a common latent feature space is found between the text and image feature, which is used to learn the final target classifier. This approach, called the Text to Image (TTI) method, is similar to Zhu [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 146" title="Zhu Y, Chen Y, Lu Z, Pan S, Xue G, Yu Y, Yang Q. Heterogeneous transfer learning for image classification. In: Proceedings of the national conference on artificial intelligence, vol. 2. 2011. p. 1304–9." href="/article/10.1186/s40537-016-0043-6#ref-CR146" id="ref-link-section-d43522e4894">146</a>]. However, Zhu [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 146" title="Zhu Y, Chen Y, Lu Z, Pan S, Xue G, Yu Y, Yang Q. Heterogeneous transfer learning for image classification. In: Proceedings of the national conference on artificial intelligence, vol. 2. 2011. p. 1304–9." href="/article/10.1186/s40537-016-0043-6#ref-CR146" id="ref-link-section-d43522e4897">146</a>] does not use labeled source data to enhance the knowledge transfer, which will result in degraded performance when there is limited labeled target data. Experiments are performed with the methods proposed by Qi [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 92" title="Qi GJ, Aggarwal C, Huang T. Towards semantic knowledge propagation from text corpus to Web images. In: Proceedings of the 20th international conference on world wide web. p. 297–306." href="/article/10.1186/s40537-016-0043-6#ref-CR92" id="ref-link-section-d43522e4900">92</a>], Dai [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 20" title="Dai W, Chen Y, Xue GR, Yang Q, Yu Y. Translated learning: transfer learning across different feature spaces. Adv Neural Inform Process Syst. 2008;21:353–60." href="/article/10.1186/s40537-016-0043-6#ref-CR20" id="ref-link-section-d43522e4904">20</a>], Zhu [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 146" title="Zhu Y, Chen Y, Lu Z, Pan S, Xue G, Yu Y, Yang Q. Heterogeneous transfer learning for image classification. In: Proceedings of the national conference on artificial intelligence, vol. 2. 2011. p. 1304–9." href="/article/10.1186/s40537-016-0043-6#ref-CR146" id="ref-link-section-d43522e4907">146</a>], and a baseline approach using a standard SVM classifier trained on the limited labeled target data. The text documents are collected from wikipedia, and classification error rate is measured as the performance metric. The results show the Zhu [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 146" title="Zhu Y, Chen Y, Lu Z, Pan S, Xue G, Yu Y, Yang Q. Heterogeneous transfer learning for image classification. In: Proceedings of the national conference on artificial intelligence, vol. 2. 2011. p. 1304–9." href="/article/10.1186/s40537-016-0043-6#ref-CR146" id="ref-link-section-d43522e4910">146</a>] method performing the best in 15 % of the trials, the Dai [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 20" title="Dai W, Chen Y, Xue GR, Yang Q, Yu Y. Translated learning: transfer learning across different feature spaces. Adv Neural Inform Process Syst. 2008;21:353–60." href="/article/10.1186/s40537-016-0043-6#ref-CR20" id="ref-link-section-d43522e4913">20</a>] method being the best in 10 % of the trials, and the Qi [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 92" title="Qi GJ, Aggarwal C, Huang T. Towards semantic knowledge propagation from text corpus to Web images. In: Proceedings of the 20th international conference on world wide web. p. 297–306." href="/article/10.1186/s40537-016-0043-6#ref-CR92" id="ref-link-section-d43522e4916">92</a>] method leading in 75 % of the trials. As with the case of Zhu [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 146" title="Zhu Y, Chen Y, Lu Z, Pan S, Xue G, Yu Y, Yang Q. Heterogeneous transfer learning for image classification. In: Proceedings of the national conference on artificial intelligence, vol. 2. 2011. p. 1304–9." href="/article/10.1186/s40537-016-0043-6#ref-CR146" id="ref-link-section-d43522e4919">146</a>], this method is very specific to the application of image classification and is difficult to port to other applications.</p><p>The scenario addressed in the paper by Duan [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 30" title="Duan L, Xu D, Tsang IW. Learning with augmented features for heterogeneous domain adaptation. IEEE Trans Pattern Anal Mach Intell. 2012;36(6):1134–48." href="/article/10.1186/s40537-016-0043-6#ref-CR30" id="ref-link-section-d43522e4925">30</a>] is focused on heterogeneous domain adaptation with a single labeled source domain and a target domain with limited labeled samples. The solution proposed is called heterogeneous feature augmentation (HFA). A transformation matrix P is defined for the source and a transformation matrix Q is defined for the target to project the feature spaces to a common latent space. The latent feature space is augmented with the original source and target feature set and zeros where appropriate. This means the source input data projection has the common latent features, the original source features, and zeros for the original target features. The target input data projection has the common latent features, zeros for the original source features, and the original target features. This feature augmentation method was first introduced by Daumé [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 22" title="Daumé H III. Frustratingly easy domain adaptation. In: Proceedings of ACL. 2007. p. 256–63." href="/article/10.1186/s40537-016-0043-6#ref-CR22" id="ref-link-section-d43522e4928">22</a>] and is used to correct for conditional distribution differences between the domains. For computational simplification, the P and Q matrices are not directly found but combined and represented by an H matrix. An optimization problem is defined by minimizing the structural risk functional [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 117" title="Vapnik V. Principles of risk minimization for learning theory. Adv Neural Inf Process Syst. 1992;4:831–8." href="/article/10.1186/s40537-016-0043-6#ref-CR117" id="ref-link-section-d43522e4931">117</a>] of SVM as a function of the H matrix. The final target prediction function is found using an alternating optimization algorithm to simultaneously solve the dual problem of SVM and the optimal transformation H matrix. The experiments are performed for the applications of image classification and text classification. The source contains labeled image data and the target contains limited labeled image data. For the image features, SURF [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 3" title="Bay H, Tuytelaars T, Gool LV. Surf: speeded up robust features. Comput Vis Image Underst. 2006;110(3):346–59." href="/article/10.1186/s40537-016-0043-6#ref-CR3" id="ref-link-section-d43522e4934">3</a>] features are extracted from the pixel information and then clustered into different dimension feature spaces creating the heterogeneous source and target environment. For the text classification experiments, the target contains Spanish language documents and the source contains documents in four different languages. The experiments test against a baseline method, which is constructed by training an SVM learner on the limited labeled target data. Other heterogeneous adaptation methods that are tested include the method by Wang [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 121" title="Wang C, Mahadevan S. Heterogeneous domain adaptation using manifold alignment. In: Proceedings of the 22nd international joint conference on artificial intelligence, vol. 2. 2011. p. 541–46." href="/article/10.1186/s40537-016-0043-6#ref-CR121" id="ref-link-section-d43522e4937">121</a>], Shi [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 105" title="Shi X, Liu Q, Fan W, Yu PS, Zhu R. Transfer learning on heterogeneous feature spaces via spectral transformation. In: 2010 IEEE international conference on data mining. 2010. p. 1049–1054." href="/article/10.1186/s40537-016-0043-6#ref-CR105" id="ref-link-section-d43522e4941">105</a>], and Kulis [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 58" title="Kulis B, Saenko K, Darrell T. What you saw is not what you get: domain adaptation using asymmetric kernel transforms. In: IEEE 2011 conference on computer vision and pattern recognition. 2011. p. 1785–92." href="/article/10.1186/s40537-016-0043-6#ref-CR58" id="ref-link-section-d43522e4944">58</a>]. For the image classification test, the HFA method outperforms all the methods tested by an average of one standard deviation with respect to classification accuracy. The Kulis [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 58" title="Kulis B, Saenko K, Darrell T. What you saw is not what you get: domain adaptation using asymmetric kernel transforms. In: IEEE 2011 conference on computer vision and pattern recognition. 2011. p. 1785–92." href="/article/10.1186/s40537-016-0043-6#ref-CR58" id="ref-link-section-d43522e4947">58</a>] method has comparable results to the baseline method (possibly due to some uniqueness in the data set) and the Wang [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 121" title="Wang C, Mahadevan S. Heterogeneous domain adaptation using manifold alignment. In: Proceedings of the 22nd international joint conference on artificial intelligence, vol. 2. 2011. p. 541–46." href="/article/10.1186/s40537-016-0043-6#ref-CR121" id="ref-link-section-d43522e4950">121</a>] method slightly outperforms the baseline method (possibly due to a weak manifold structure in the data set). For the text classification test, the HFA method outperforms all methods tested by an average of 1.5 standard deviation. For this test, the Kulis [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 58" title="Kulis B, Saenko K, Darrell T. What you saw is not what you get: domain adaptation using asymmetric kernel transforms. In: IEEE 2011 conference on computer vision and pattern recognition. 2011. p. 1785–92." href="/article/10.1186/s40537-016-0043-6#ref-CR58" id="ref-link-section-d43522e4953">58</a>] method is second in performance, followed by Wang [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 121" title="Wang C, Mahadevan S. Heterogeneous domain adaptation using manifold alignment. In: Proceedings of the 22nd international joint conference on artificial intelligence, vol. 2. 2011. p. 541–46." href="/article/10.1186/s40537-016-0043-6#ref-CR121" id="ref-link-section-d43522e4956">121</a>], and then the baseline method. The Shi [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 105" title="Shi X, Liu Q, Fan W, Yu PS, Zhu R. Transfer learning on heterogeneous feature spaces via spectral transformation. In: 2010 IEEE international conference on data mining. 2010. p. 1049–1054." href="/article/10.1186/s40537-016-0043-6#ref-CR105" id="ref-link-section-d43522e4960">105</a>] method performed worse than the baseline method in both tests. A possible reason for this result is the Shi [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 105" title="Shi X, Liu Q, Fan W, Yu PS, Zhu R. Transfer learning on heterogeneous feature spaces via spectral transformation. In: 2010 IEEE international conference on data mining. 2010. p. 1049–1054." href="/article/10.1186/s40537-016-0043-6#ref-CR105" id="ref-link-section-d43522e4963">105</a>] method does not specifically use the labeled information from the target when performing the symmetric transformation, which will result in degraded classification performance [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 64" title="Li W, Duan L, Xu D, Tsang IW. Learning with augmented features for supervised and semi-supervised heterogeneous domain adaptation. IEEE Trans Pattern Anal Mach Intell. 2014;36(6):1134–48." href="/article/10.1186/s40537-016-0043-6#ref-CR64" id="ref-link-section-d43522e4966">64</a>].</p><p>The work of Li [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 64" title="Li W, Duan L, Xu D, Tsang IW. Learning with augmented features for supervised and semi-supervised heterogeneous domain adaptation. IEEE Trans Pattern Anal Mach Intell. 2014;36(6):1134–48." href="/article/10.1186/s40537-016-0043-6#ref-CR64" id="ref-link-section-d43522e4972">64</a>], called the Semi-supervised heterogeneous feature augmentation (SHFA) approach, addresses the heterogeneous scenario of an abundance of labeled source data and limited target data, and directly extends the work of Duan [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 30" title="Duan L, Xu D, Tsang IW. Learning with augmented features for heterogeneous domain adaptation. IEEE Trans Pattern Anal Mach Intell. 2012;36(6):1134–48." href="/article/10.1186/s40537-016-0043-6#ref-CR30" id="ref-link-section-d43522e4975">30</a>]. In this work, the H transformation matrix, which is described above by Duan [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 30" title="Duan L, Xu D, Tsang IW. Learning with augmented features for heterogeneous domain adaptation. IEEE Trans Pattern Anal Mach Intell. 2012;36(6):1134–48." href="/article/10.1186/s40537-016-0043-6#ref-CR30" id="ref-link-section-d43522e4978">30</a>], is decomposed into a linear combination of a set of rank-one positive semi-definite matrices that allow for Multiple Kernel Learning solvers (defined by Kloft [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 57" title="Kloft M, Brefeld U, Sonnenburg S, Zien A. Lp-norm multiple kernel learning. J Mach Learn Res. 2011;12:953–97." href="/article/10.1186/s40537-016-0043-6#ref-CR57" id="ref-link-section-d43522e4981">57</a>]) to be used to find a solution. In the process of learning the H transformation matrix, the labels for the unlabeled target data are estimated (pseudo labels created) and used while learning the final target classifier. The pseudo labels for the unlabeled target data are found from an SVM classifier trained on the limited labeled target data. The high-level domain adaptation is shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1186/s40537-016-0043-6#Fig3">3</a>. Experiments are performed for three applications which include image classification (where 31 unique classes are defined), multi-language text document classification (where six unique classes are defined), and multi-language text sentiment classification. Classification accuracy is measured as the performance metric. The method by Li [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 64" title="Li W, Duan L, Xu D, Tsang IW. Learning with augmented features for supervised and semi-supervised heterogeneous domain adaptation. IEEE Trans Pattern Anal Mach Intell. 2014;36(6):1134–48." href="/article/10.1186/s40537-016-0043-6#ref-CR64" id="ref-link-section-d43522e4988">64</a>] is tested against a baseline method using an SVM learner and trained on the limited labeled target data. Further, other heterogeneous methods tested include Duan [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 30" title="Duan L, Xu D, Tsang IW. Learning with augmented features for heterogeneous domain adaptation. IEEE Trans Pattern Anal Mach Intell. 2012;36(6):1134–48." href="/article/10.1186/s40537-016-0043-6#ref-CR30" id="ref-link-section-d43522e4991">30</a>], Shi [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 105" title="Shi X, Liu Q, Fan W, Yu PS, Zhu R. Transfer learning on heterogeneous feature spaces via spectral transformation. In: 2010 IEEE international conference on data mining. 2010. p. 1049–1054." href="/article/10.1186/s40537-016-0043-6#ref-CR105" id="ref-link-section-d43522e4994">105</a>], Wang [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 121" title="Wang C, Mahadevan S. Heterogeneous domain adaptation using manifold alignment. In: Proceedings of the 22nd international joint conference on artificial intelligence, vol. 2. 2011. p. 541–46." href="/article/10.1186/s40537-016-0043-6#ref-CR121" id="ref-link-section-d43522e4997">121</a>], and Kulis [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 58" title="Kulis B, Saenko K, Darrell T. What you saw is not what you get: domain adaptation using asymmetric kernel transforms. In: IEEE 2011 conference on computer vision and pattern recognition. 2011. p. 1785–92." href="/article/10.1186/s40537-016-0043-6#ref-CR58" id="ref-link-section-d43522e5000">58</a>]. By averaging the three different application test results, the order of performance from best to worst is Li [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 64" title="Li W, Duan L, Xu D, Tsang IW. Learning with augmented features for supervised and semi-supervised heterogeneous domain adaptation. IEEE Trans Pattern Anal Mach Intell. 2014;36(6):1134–48." href="/article/10.1186/s40537-016-0043-6#ref-CR64" id="ref-link-section-d43522e5003">64</a>], Duan [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 30" title="Duan L, Xu D, Tsang IW. Learning with augmented features for heterogeneous domain adaptation. IEEE Trans Pattern Anal Mach Intell. 2012;36(6):1134–48." href="/article/10.1186/s40537-016-0043-6#ref-CR30" id="ref-link-section-d43522e5007">30</a>], Wang [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 121" title="Wang C, Mahadevan S. Heterogeneous domain adaptation using manifold alignment. In: Proceedings of the 22nd international joint conference on artificial intelligence, vol. 2. 2011. p. 541–46." href="/article/10.1186/s40537-016-0043-6#ref-CR121" id="ref-link-section-d43522e5010">121</a>], baseline and Kulis [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 58" title="Kulis B, Saenko K, Darrell T. What you saw is not what you get: domain adaptation using asymmetric kernel transforms. In: IEEE 2011 conference on computer vision and pattern recognition. 2011. p. 1785–92." href="/article/10.1186/s40537-016-0043-6#ref-CR58" id="ref-link-section-d43522e5013">58</a>] (tie), and Shi [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 105" title="Shi X, Liu Q, Fan W, Yu PS, Zhu R. Transfer learning on heterogeneous feature spaces via spectral transformation. In: 2010 IEEE international conference on data mining. 2010. p. 1049–1054." href="/article/10.1186/s40537-016-0043-6#ref-CR105" id="ref-link-section-d43522e5016">105</a>].</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3" data-title="Fig. 3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1186/s40537-016-0043-6/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1186%2Fs40537-016-0043-6/MediaObjects/40537_2016_43_Fig3_HTML.gif?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1186%2Fs40537-016-0043-6/MediaObjects/40537_2016_43_Fig3_HTML.gif" alt="figure3" loading="lazy" /></picture></a><p class="c-article-section__figure-credit text-right c-article-section__figure-credit-right" data-test="figure-credit">Diagram adapted from Li [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 64" title="Li W, Duan L, Xu D, Tsang IW. Learning with augmented features for supervised and semi-supervised heterogeneous domain adaptation. IEEE Trans Pattern Anal Mach Intell. 2014;36(6):1134–48." href="/article/10.1186/s40537-016-0043-6#ref-CR64" id="ref-link-section-d43522e5044">64</a>]</p></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>Depicts algorithm approach by Li [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 64" title="Li W, Duan L, Xu D, Tsang IW. Learning with augmented features for supervised and semi-supervised heterogeneous domain adaptation. IEEE Trans Pattern Anal Mach Intell. 2014;36(6):1134–48." href="/article/10.1186/s40537-016-0043-6#ref-CR64" id="ref-link-section-d43522e5029">64</a>] where the heterogeneous source and target features are transformed to an augmented latent feature space. T<sub>S</sub> and T<sub>T</sub> are transformation functions. P and Q are projection matrices as described in Duan [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 30" title="Duan L, Xu D, Tsang IW. Learning with augmented features for heterogeneous domain adaptation. IEEE Trans Pattern Anal Mach Intell. 2012;36(6):1134–48." href="/article/10.1186/s40537-016-0043-6#ref-CR30" id="ref-link-section-d43522e5036">30</a>]</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1186/s40537-016-0043-6/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <h3 class="c-article__sub-heading" id="Sec13">Asymmetric feature-based transfer learning</h3><p>The work of Kulis [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 58" title="Kulis B, Saenko K, Darrell T. What you saw is not what you get: domain adaptation using asymmetric kernel transforms. In: IEEE 2011 conference on computer vision and pattern recognition. 2011. p. 1785–92." href="/article/10.1186/s40537-016-0043-6#ref-CR58" id="ref-link-section-d43522e5065">58</a>], referred to as the asymmetric regularized cross-domain Transformation (ARC-t), proposes an asymmetric transformation algorithm to resolve the heterogeneous feature space between domains. For this scenario, there is an abundance of labeled source data and limited labeled target data. An objective function is first defined for learning the transformation matrix. The objective function contains a regularizer term and a cost function term that is applied to each pair of cross-domain instances and the learned transformation matrix. The construction of the objective function is responsible for the domain invariant transformation process. The optimization of the objective function aims to minimize the regularizer and the cost function terms. The transformation matrix is learned in a non-linear Gaussian RBF kernel space. The method presented is referred to as the asymmetric regularized cross-domain transformation. Two experiments using this approach are performed for image classification where classification accuracy is measured as the performance metric. There are 31 image classes defined for these experiments. The first experiment (test 1) is where instances of all 31 image classes are included in the source and target training data. In the second experiment (test 2), only 16 image classes are represented in the target training data (all 31 are represented in the source). To test against other baseline approaches, a method is needed to bring the source and target input domains together. A preprocessing step called Kernel Canonical Correlation Analysis (proposed by Shawe-Taylor [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 104" title="Shawe-Taylor J, Cristianini N. Kernel methods for pattern analysis. Cambridge: Cambridge University Press; 2004." href="/article/10.1186/s40537-016-0043-6#ref-CR104" id="ref-link-section-d43522e5068">104</a>]) is used to project the source and target domains into a common domain space using symmetric transformation. Baseline approaches tested include k-nearest neighbors, SVM, metric learning proposed by Davis [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 23" title="Davis J, Kulis B, Jain P, Sra S, Dhillon I. Information theoretic metric learning. In: Proceedings of the 24th international conference on machine learning. 2007. p. 209–16." href="/article/10.1186/s40537-016-0043-6#ref-CR23" id="ref-link-section-d43522e5071">23</a>], feature augmentation proposed by Daumé [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 22" title="Daumé H III. Frustratingly easy domain adaptation. In: Proceedings of ACL. 2007. p. 256–63." href="/article/10.1186/s40537-016-0043-6#ref-CR22" id="ref-link-section-d43522e5074">22</a>], and a cross domain metric learning method proposed by Saenko [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 100" title="Saenko K, Kulis B, Fritz M, Darrell T. Adapting visual category models to new domains. Comput Vision ECCV. 2010;6314:213–26." href="/article/10.1186/s40537-016-0043-6#ref-CR100" id="ref-link-section-d43522e5077">100</a>]. For test 1, the Kulis [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 58" title="Kulis B, Saenko K, Darrell T. What you saw is not what you get: domain adaptation using asymmetric kernel transforms. In: IEEE 2011 conference on computer vision and pattern recognition. 2011. p. 1785–92." href="/article/10.1186/s40537-016-0043-6#ref-CR58" id="ref-link-section-d43522e5081">58</a>] approach performs marginally better than the other methods tested. For test 2, the Kulis [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 58" title="Kulis B, Saenko K, Darrell T. What you saw is not what you get: domain adaptation using asymmetric kernel transforms. In: IEEE 2011 conference on computer vision and pattern recognition. 2011. p. 1785–92." href="/article/10.1186/s40537-016-0043-6#ref-CR58" id="ref-link-section-d43522e5084">58</a>] approach performs significantly better compared to the k-nearest neighbors approach (note the other methods cannot be tested against as they require all 31 classes to be represented in the target training data). The Kulis [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 58" title="Kulis B, Saenko K, Darrell T. What you saw is not what you get: domain adaptation using asymmetric kernel transforms. In: IEEE 2011 conference on computer vision and pattern recognition. 2011. p. 1785–92." href="/article/10.1186/s40537-016-0043-6#ref-CR58" id="ref-link-section-d43522e5087">58</a>] approach is best suited for scenarios where all of the classes are not represented in the target training data as demonstrated in test 2.</p><p>The problem domain defined by Harel [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 46" title="Harel M, Mannor S. Learning from multiple outlooks. In: Proceedings of the 28th international conference on machine learning. 2011. p. 401–8." href="/article/10.1186/s40537-016-0043-6#ref-CR46" id="ref-link-section-d43522e5093">46</a>] is of limited labeled target data and multiple labeled data sources where an asymmetric transformation is desired for each source to resolve the mismatch in feature space. The first step in the process is to normalize the features in the source and target domains, then group the instances by class in the source and target domains. For each class grouping, the features are mean adjusted to zero. Next, each individual source class group is paired with the corresponding target class group, and a singular value decomposition process is performed to find the specific transformation matrix for that class grouping. Once the transformation is performed, the features are mean shifted back reversing the previous step, and the final target classifier is trained using the transformed data. Finding the transformation matrix using the singular value decomposition process allows for the marginal distributions within the class groupings to be aligned while maintaining the structure of the data. This approach is referred to as the Multiple Outlook MAPping algorithm (MOMAP). The experiments use data taken from wearable sensors for the application of activity classification. There are five different activities defined for the experiment which include walking, running, going upstairs, going downstairs, and lingering. The source domain contains similar (but different) sensor readings as compared to the target. The method proposed by Harel [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 46" title="Harel M, Mannor S. Learning from multiple outlooks. In: Proceedings of the 28th international conference on machine learning. 2011. p. 401–8." href="/article/10.1186/s40537-016-0043-6#ref-CR46" id="ref-link-section-d43522e5096">46</a>] is compared against a baseline method that trains a classifier with the limited labeled target data and an upper bound method that uses a significantly larger set of labeled target data to train a classifier. An SVM learner is used as the base classifier and a balanced error rate (due to an imbalance in the test data) is measured as the performance metric. The Harel [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 46" title="Harel M, Mannor S. Learning from multiple outlooks. In: Proceedings of the 28th international conference on machine learning. 2011. p. 401–8." href="/article/10.1186/s40537-016-0043-6#ref-CR46" id="ref-link-section-d43522e5099">46</a>] approach outperforms the baseline method in every test and falls short of the upper bound method in every test with respect to the balanced error rate.</p><p>The heterogeneous transfer learning scenario addressed by Zhou [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 145" title="Zhou JT, Tsang IW, Pan SJ Tan M. Heterogeneous domain adaptation for multiple classes. In: International conference on artificial intelligence and statistics. 2014. p. 1095–103." href="/article/10.1186/s40537-016-0043-6#ref-CR145" id="ref-link-section-d43522e5105">145</a>] requires an abundance of labeled source data and limited labeled target data. An asymmetric transformation function is proposed to map the source features to the target features. To learn the transformation matrix, a multi-task learning method based on Ando [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2" title="Ando RK, Zhang T. A framework for learning predictive structures from multiple tasks and unlabeled data. J Mach Learn Res. 2005;6:1817–53." href="/article/10.1186/s40537-016-0043-6#ref-CR2" id="ref-link-section-d43522e5108">2</a>] is adopted. The solution, referred to as the sparse heterogeneous feature representation (SHFR), is implemented by creating a binary classifier for each class in the source and the target domains separately. Each binary classifier is assigned a weight term where the weight terms are learned by combining the weighted classifier outputs, while minimizing the classification error of each domain. The weight terms are now used to find the transformation matrix by minimizing the difference between the target weights and the transformed source weights. The final target classifier is trained using the transformed source data and original target data. Experiments are performed for text document classification where the target domain contains documents written in one language and the source domain contains documents written in different languages. A baseline method using a linear SVM classifier trained on the labeled target is established along with testing against the methods proposed by Wang [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 121" title="Wang C, Mahadevan S. Heterogeneous domain adaptation using manifold alignment. In: Proceedings of the 22nd international joint conference on artificial intelligence, vol. 2. 2011. p. 541–46." href="/article/10.1186/s40537-016-0043-6#ref-CR121" id="ref-link-section-d43522e5111">121</a>], Kulis [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 58" title="Kulis B, Saenko K, Darrell T. What you saw is not what you get: domain adaptation using asymmetric kernel transforms. In: IEEE 2011 conference on computer vision and pattern recognition. 2011. p. 1785–92." href="/article/10.1186/s40537-016-0043-6#ref-CR58" id="ref-link-section-d43522e5114">58</a>], and Duan [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 30" title="Duan L, Xu D, Tsang IW. Learning with augmented features for heterogeneous domain adaptation. IEEE Trans Pattern Anal Mach Intell. 2012;36(6):1134–48." href="/article/10.1186/s40537-016-0043-6#ref-CR30" id="ref-link-section-d43522e5117">30</a>]. The method proposed by Zhou [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 145" title="Zhou JT, Tsang IW, Pan SJ Tan M. Heterogeneous domain adaptation for multiple classes. In: International conference on artificial intelligence and statistics. 2014. p. 1095–103." href="/article/10.1186/s40537-016-0043-6#ref-CR145" id="ref-link-section-d43522e5121">145</a>] performed the best for all tests with respect to classification accuracy. The results of the other approaches are mixed as a function of the data sets used where the Duan [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 30" title="Duan L, Xu D, Tsang IW. Learning with augmented features for heterogeneous domain adaptation. IEEE Trans Pattern Anal Mach Intell. 2012;36(6):1134–48." href="/article/10.1186/s40537-016-0043-6#ref-CR30" id="ref-link-section-d43522e5124">30</a>] method performed either second or third best.</p><p>The application of software module defect prediction is usually addressed by training a classifier with labeled data taken from the software project of interest. The environment described in Nam [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 77" title="Nam J, Kim S (2015) Heterogeneous defect prediction. In: Proceedings of the 2015 10th joint meeting on foundations of software engineering. 2015. p. 508–19." href="/article/10.1186/s40537-016-0043-6#ref-CR77" id="ref-link-section-d43522e5130">77</a>] for software module defect prediction attempts to use labeled source data from one software project to train a classifier to predict unlabeled target data from another project. The source and target software projects collect different metrics making the source and target feature spaces heterogeneous. The proposed solution, referred to as the heterogeneous defect prediction (HDP) approach, is to first select the important features from the source domain using a feature selection method to eliminate redundant and irrelevant features. Feature selection methods used include gain ratio, Chi square, relief-F, and significance attribute evaluation (see Gao [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 39" title="Gao K, Khoshgoftaar TM, Wang H, Seliya N. Choosing software metrics for defect prediction: an investigation on feature selection techniques. J Softw Pract Exp. 2011;41(5):579–606." href="/article/10.1186/s40537-016-0043-6#ref-CR39" id="ref-link-section-d43522e5133">39</a>] and Shivaji [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 108" title="Shivaji S, Whitehead EJ, Akella R, Kim S. Reducing features to improve code change-based bug prediction. IEEE Trans Softw Eng. 2013;39(4):552–69." href="/article/10.1186/s40537-016-0043-6#ref-CR108" id="ref-link-section-d43522e5136">108</a>]). The next step is to statistically match the selected source domain features to ones in the target using a Kolmogorov–Smirnov test that measures the closeness of the empirical distribution between the two sources. A learner is trained with the source features that exhibit a close statistical match to the corresponding target features. The target data is tested with the trained classifier using the corresponding matched features of the target. Even though the approach by Nam [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 77" title="Nam J, Kim S (2015) Heterogeneous defect prediction. In: Proceedings of the 2015 10th joint meeting on foundations of software engineering. 2015. p. 508–19." href="/article/10.1186/s40537-016-0043-6#ref-CR77" id="ref-link-section-d43522e5139">77</a>] is applied directly to the application of software module defect prediction, this method can be used for other applications. Experiments are performed using five different software defect data sets with heterogeneous features. The proposed method by Nam [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 77" title="Nam J, Kim S (2015) Heterogeneous defect prediction. In: Proceedings of the 2015 10th joint meeting on foundations of software engineering. 2015. p. 508–19." href="/article/10.1186/s40537-016-0043-6#ref-CR77" id="ref-link-section-d43522e5142">77</a>] uses logistic regression as the base learner. The other approaches tested include a within project defect prediction (WPDP) approach where the learner is trained on labeled target data, a cross project defect prediction (CPDP-CM) approach where the source and target represent different software projects but have homogeneous features, and a cross project defect prediction approach with heterogeneous features (CPDP-IFS) as proposed by He [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 47" title="He P, Li B, Ma Y (2014) Towards cross-project defect prediction with imbalanced feature sets. &#xA;                    http://arxiv.org/abs/1411.4228&#xA;                    &#xA;                  ." href="/article/10.1186/s40537-016-0043-6#ref-CR47" id="ref-link-section-d43522e5146">47</a>]. The results of the experiment show the Nam [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 77" title="Nam J, Kim S (2015) Heterogeneous defect prediction. In: Proceedings of the 2015 10th joint meeting on foundations of software engineering. 2015. p. 508–19." href="/article/10.1186/s40537-016-0043-6#ref-CR77" id="ref-link-section-d43522e5149">77</a>] method significantly outperformed all other approaches with respect to area under the curve measurement. The WPDP approach is next best followed by the CPDP-CM approach and the CPDP-IFS approach. These results can be misleading as the Nam [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 77" title="Nam J, Kim S (2015) Heterogeneous defect prediction. In: Proceedings of the 2015 10th joint meeting on foundations of software engineering. 2015. p. 508–19." href="/article/10.1186/s40537-016-0043-6#ref-CR77" id="ref-link-section-d43522e5152">77</a>] approach could only match at least one or more input features between the source and target domains in 37 % of the tests. Therefore, in 63 % of the cases, the Nam [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 77" title="Nam J, Kim S (2015) Heterogeneous defect prediction. In: Proceedings of the 2015 10th joint meeting on foundations of software engineering. 2015. p. 508–19." href="/article/10.1186/s40537-016-0043-6#ref-CR77" id="ref-link-section-d43522e5155">77</a>] method could not be used and these cases are not counted. The WPDP method represents an upper bound and it is an unexpected result that the Nam [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 77" title="Nam J, Kim S (2015) Heterogeneous defect prediction. In: Proceedings of the 2015 10th joint meeting on foundations of software engineering. 2015. p. 508–19." href="/article/10.1186/s40537-016-0043-6#ref-CR77" id="ref-link-section-d43522e5158">77</a>] approach would outperform the WPDP method.</p><p>The paper by Zhou [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 144" title="Zhou JT, Pan S, Tsang IW, Yan Y. Hybrid heterogeneous transfer learning through deep learning. In: Proceedings of the national conference on artificial intelligence, vol. 3. 2014. p. 2213–20." href="/article/10.1186/s40537-016-0043-6#ref-CR144" id="ref-link-section-d43522e5165">144</a>] claims that previous heterogeneous solutions assume the instance correspondence between the source and target domains are statistically representative (distributions are equal), which may not always be the case. An example of this claim is in the application of text sentiment classification where the word bias problem previously discussed causes distribution differences between the source and target domains. The paper by Zhou [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 144" title="Zhou JT, Pan S, Tsang IW, Yan Y. Hybrid heterogeneous transfer learning through deep learning. In: Proceedings of the national conference on artificial intelligence, vol. 3. 2014. p. 2213–20." href="/article/10.1186/s40537-016-0043-6#ref-CR144" id="ref-link-section-d43522e5168">144</a>] proposes a solution called the hybrid heterogeneous transfer learning (HHTL) method for a heterogeneous environment with abundant labeled source data and abundant unlabeled target data. The idea is to first learn an asymmetric transformation from the target to the source domain, which reduces the problem to a homogeneous domain adaptation issue. The next step is to discover a common latent feature space using the transformed data (from the previous step) to reduce the distribution bias between the transformed unlabeled target domain and the labeled source domain. Finally, a classifier is trained using the common latent feature space from the labeled source data. This solution is realized using a deep learning method employing a marginalized stacked denoised autoencoder as proposed by Chen [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 16" title="Chen M, Xu ZE, Weinberger KQ, Sha F (2012) Marginalized denoising autoencoders for domain adaptation. ICML. arXiv preprintarXiv:1206.4683." href="/article/10.1186/s40537-016-0043-6#ref-CR16" id="ref-link-section-d43522e5171">16</a>] to learn the asymmetric transformation and the mapping to a common latent feature space. The previous surveyed paper by Glorot [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 41" title="Glorot X, Bordes A, Bengio Y. Domain adaptation for large-scale sentiment classification: A deep learning approach. In: Proceedings of the twenty-eight international conference on machine learning, vol. 27. 2011. p. 97–110." href="/article/10.1186/s40537-016-0043-6#ref-CR41" id="ref-link-section-d43522e5174">41</a>] demonstrated a deep learning approach finding a common latent feature space for homogeneous source and target feature set. The experiments focused on multiple language text sentiment classification where English is used in the source and three other languages are separately used in the target. Classification accuracy is measured as the performance metric. Other methods tested include a heterogeneous spectral mapping approach proposed by Shi [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 105" title="Shi X, Liu Q, Fan W, Yu PS, Zhu R. Transfer learning on heterogeneous feature spaces via spectral transformation. In: 2010 IEEE international conference on data mining. 2010. p. 1049–1054." href="/article/10.1186/s40537-016-0043-6#ref-CR105" id="ref-link-section-d43522e5177">105</a>], a method proposed by Vinokourov [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 120" title="Vinokourov A, Shawe-Taylor J, Cristianini N. Inferring a semantic representation of text via crosslanguage correlation analysis. Adv Neural Inf Proces Syst. 2002;15:1473–80." href="/article/10.1186/s40537-016-0043-6#ref-CR120" id="ref-link-section-d43522e5181">120</a>], and a multimodal deep learning approach proposed by Ngiam [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 79" title="Ngiam J, Khosla A, Kim M, Nam J, Lee H, Ng AY. Multimodal deep learning. In: The 28th international conference on machine learning. 2011. p. 689–96." href="/article/10.1186/s40537-016-0043-6#ref-CR79" id="ref-link-section-d43522e5184">79</a>]. An SVM learner is used as the base classifier for all methods. The results of the experiment from best to worst performance are Zhou [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 144" title="Zhou JT, Pan S, Tsang IW, Yan Y. Hybrid heterogeneous transfer learning through deep learning. In: Proceedings of the national conference on artificial intelligence, vol. 3. 2014. p. 2213–20." href="/article/10.1186/s40537-016-0043-6#ref-CR144" id="ref-link-section-d43522e5187">144</a>], Ngiam [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 79" title="Ngiam J, Khosla A, Kim M, Nam J, Lee H, Ng AY. Multimodal deep learning. In: The 28th international conference on machine learning. 2011. p. 689–96." href="/article/10.1186/s40537-016-0043-6#ref-CR79" id="ref-link-section-d43522e5190">79</a>], Vinokourov [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 120" title="Vinokourov A, Shawe-Taylor J, Cristianini N. Inferring a semantic representation of text via crosslanguage correlation analysis. Adv Neural Inf Proces Syst. 2002;15:1473–80." href="/article/10.1186/s40537-016-0043-6#ref-CR120" id="ref-link-section-d43522e5193">120</a>], and Shi [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 105" title="Shi X, Liu Q, Fan W, Yu PS, Zhu R. Transfer learning on heterogeneous feature spaces via spectral transformation. In: 2010 IEEE international conference on data mining. 2010. p. 1049–1054." href="/article/10.1186/s40537-016-0043-6#ref-CR105" id="ref-link-section-d43522e5196">105</a>].</p><h3 class="c-article__sub-heading" id="Sec14">Improvements to heterogeneous solutions</h3><p>The paper by Yang [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 136" title="Yang L, Jing L, Yu J, Ng MK. Learning transferred weights from co-occurrence data for heterogeneous transfer learning. IEEE Trans Neural Netw Learn Syst. 2015;PP(99):1–14." href="/article/10.1186/s40537-016-0043-6#ref-CR136" id="ref-link-section-d43522e5208">136</a>] proposes to quantify the amount of knowledge that can be transferred between domains in a heterogeneous transfer learning environment. In other words, it attempts to measure the “relatedness” of the domains. This is accomplished by first building a co-occurrence matrix for each domain. The co-occurrence matrix contains the set of instances represented in every domain. For example, if one particular text document is an instance in the co-occurrence matrix, that text document is required to be represented in every domain. Next, principal component analysis is used to select the most important features in each domain and assign the principal component coefficient to those features. The principal component coefficients are used to form a directed cyclic network (DCN) where each node represents a domain (either source or target) and each node connection (edge weight) is the conditional dependence from one domain to another. The DCN is built using a Markov Chain Monte Carlo method. The edge weights represent the potential amount of knowledge that can be transferred between domains where a higher value means higher knowledge transfer. These edge weights are then used as tuning parameters in different heterogeneous transfer learning solutions, which include works from Yang [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 137" title="Yang Q, Chen Y, Xue GR, Dai W, Yu Y. Heterogeneous transfer learning for image clustering via the social web. In: Proceedings of the joint conference of the 47th annual meeting of the ACL, vol. 1. 2009. p. 1–9." href="/article/10.1186/s40537-016-0043-6#ref-CR137" id="ref-link-section-d43522e5211">137</a>], Ng [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 78" title="Ng MK, Wu Q, Ye Y. Co-transfer learning via joint transition probability graph based method. In: Proceedings of the 1st international workshop on cross domain knowledge discovery in web and social network mining. 2012. p. 1–9." href="/article/10.1186/s40537-016-0043-6#ref-CR78" id="ref-link-section-d43522e5214">78</a>], and Zhu [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 146" title="Zhu Y, Chen Y, Lu Z, Pan S, Xue G, Yu Y, Yang Q. Heterogeneous transfer learning for image classification. In: Proceedings of the national conference on artificial intelligence, vol. 2. 2011. p. 1304–9." href="/article/10.1186/s40537-016-0043-6#ref-CR146" id="ref-link-section-d43522e5217">146</a>] (the weights are calculated first using Yang [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 136" title="Yang L, Jing L, Yu J, Ng MK. Learning transferred weights from co-occurrence data for heterogeneous transfer learning. IEEE Trans Neural Netw Learn Syst. 2015;PP(99):1–14." href="/article/10.1186/s40537-016-0043-6#ref-CR136" id="ref-link-section-d43522e5220">136</a>] and then applied as tuning values in the other solutions). Note, that integrating the edge weight values into a particular approach is specific to the implementation of the solution and cannot be generically applied. The experiments are run on the three different learning solutions comparing the original solution against the solution using the weighted edges of the DCN as the tuned parameters. In all three solutions, the classification accuracy is improved using the DCN tuned parameters. One potential issue with this approach is the construction of the co-occurrence matrix. The co-occurrence matrix contains many instances; however, each instance must be represented in each domain. This may be an unrealistic constraint in many real-world applications.</p><h3 class="c-article__sub-heading" id="Sec15">Experiment results</h3><p>In reviewing the experiment results of the previous surveyed papers, there are instances where one solution can show varying results over a range of different experiments. There are many reasons why this can happen which include varying test environments, different test implementations, different applications being tested, and different data sets being used. An interesting area of future work is to evaluate the solutions presented to determine the best performing solutions as a function of specific datasets. To facilitate that goal, a repository of open-source software containing the software implementations for solutions used in each paper would be extremely beneficial.</p><p>Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1186/s40537-016-0043-6#Tab3">3</a> lists a compilation of head-to-head results for the most commonly tested solutions contained in the “<a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1186/s40537-016-0043-6#Sec11">Heterogeneous transfer learning</a>” section. The results listed in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1186/s40537-016-0043-6#Tab3">3</a> represent a win, loss, and tie performance record of the head-to-head solution comparisons. Note, these results are compiled directly from the surveyed papers. It is difficult to draw exact conclusions from this information because of the reasons just outlined; however, it provides some interesting insight into the comparative performances of the solutions.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-3"><figure><figcaption class="c-article-table__figcaption"><b id="Tab3" data-test="table-caption">Table 3 Lists the head-to-head results of experiments performed in the heterogeneous transfer learning works surveyed</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1186/s40537-016-0043-6/tables/3"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <h3 class="c-article__sub-heading" id="Sec16">Discussion of heterogeneous solutions</h3><p>The previous surveyed heterogeneous transfer learning works demonstrate many different characteristics and attributes. Which heterogeneous transfer learning solution is best for a particular application? The heterogeneous transfer learning solutions use either a symmetric transformation or an asymmetric transformation process in an attempt to resolve the differences between the input feature space (as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1186/s40537-016-0043-6#Fig1">1</a>). The asymmetrical transformation approach is best used when the same class instances in the source and target domains can be transformed without context feature bias. Many of the surveyed heterogeneous transfer learning solutions only address the issue of the input feature space being different between the source and target domains and do not address other domain adaptation steps needed for marginal and/or conditional distribution differences. If further domain adaptation needs to be performed after the input feature spaces are aligned, then an appropriate homogeneous solution should be used. To further help determine which solution is best for a given transfer learning application, the information in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1186/s40537-016-0043-6#Tab4">4</a> should be used to match the characteristics of the solution to that of the desired application environment. None of the surveyed heterogeneous transfer learning solutions have a means to guard against negative transfer effects. However, the paper by Yang [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 136" title="Yang L, Jing L, Yu J, Ng MK. Learning transferred weights from co-occurrence data for heterogeneous transfer learning. IEEE Trans Neural Netw Learn Syst. 2015;PP(99):1–14." href="/article/10.1186/s40537-016-0043-6#ref-CR136" id="ref-link-section-d43522e5597">136</a>] demonstrates that negative transfer guards can benefit heterogeneous transfer learning solutions. It seems likely that future heterogeneous transfer learning works will integrate means for negative transfer protection. Many of the same heterogeneous transfer learning solutions are tested in the surveyed solution experiments. These head-to-head comparisons are summarized in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1186/s40537-016-0043-6#Tab3">3</a> and can be used as a starting point to understand the relative performance between the solutions. As observed as a trend in the previous homogeneous solutions, the recent heterogeneous solution by Duan [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 30" title="Duan L, Xu D, Tsang IW. Learning with augmented features for heterogeneous domain adaptation. IEEE Trans Pattern Anal Mach Intell. 2012;36(6):1134–48." href="/article/10.1186/s40537-016-0043-6#ref-CR30" id="ref-link-section-d43522e5603">30</a>] employs a one-stage solution that simultaneously performs the feature input space alignment process while learning the final classifier. As is the case for the surveyed homogeneous transfer learning works, the surveyed heterogeneous transfer learning works are not specifically applied to big data solutions; however, there is nothing to preclude their use in a big data environment.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-4"><figure><figcaption class="c-article-table__figcaption"><b id="Tab4" data-test="table-caption">Table 4 Heterogeneous transfer learning approaches surveyed in “<a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1186/s40537-016-0043-6#Sec7">Heterogeneous transfer learning</a>” section listing various characteristics of each approach</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1186/s40537-016-0043-6/tables/4"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        </div></div></section><section aria-labelledby="Sec17" data-title="Negative transfer"><div class="c-article-section" id="Sec17-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec17">Negative transfer</h2><div class="c-article-section__content" id="Sec17-content"><p>The high-level concept of transfer learning is to improve a target learner by using data from a related source domain. But what happens if the source domain is not well-related to the target? In this case, the target learner can be negatively impacted by this weak relation, which is referred to as negative transfer. In a big data environment, there may be a large dataset where only a portion of the data is related to a target domain of interest. For this case, there is a need to divide the dataset into multiple sources and employ negative transfer methods when using transfer learning algorithm. In the scenario where multiple datasets are available that initially appear to be related to the target domain of interest, it is desired to select the datasets that provide the best information transfer and avoid the datasets that cause negative transfer. This allows for the best use of the available large datasets. How related do the source and target domains need to be for transfer learning to be advantageous? The area of negative transfer has not been widely researched, but the following papers begin to address this issue.</p><p>An early paper by Rosenstein [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 98" title="Rosenstein MT, Marx Z, Kaelbling LP, Dietterich TG. To transfer or not to transfer. In: Proceedings NIPS’05 workshop, inductive transfer. 10 years later. 2005. p. 1–4." href="/article/10.1186/s40537-016-0043-6#ref-CR98" id="ref-link-section-d43522e6141">98</a>] discusses the concept of negative transfer in transfer learning and claims that the source domain needs to be sufficiently related to the target domain; otherwise, the attempt to transfer knowledge from the source can have a negative impact on the target learner. Cases of negative transfer are demonstrated by Rosenstein [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 98" title="Rosenstein MT, Marx Z, Kaelbling LP, Dietterich TG. To transfer or not to transfer. In: Proceedings NIPS’05 workshop, inductive transfer. 10 years later. 2005. p. 1–4." href="/article/10.1186/s40537-016-0043-6#ref-CR98" id="ref-link-section-d43522e6144">98</a>] in experiments using a hierarchical Naive Bayes classifier. The author also demonstrates the chance of negative transfer goes down as the number of labeled target training samples goes up.</p><p>The paper by Eaton [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 31" title="Eaton E, des Jardins M, Lane T. Modeling transfer relationships between learning tasks for improved inductive transfer. Proc Mach Learn Knowl Disc Database. 2008;5211:317–32." href="/article/10.1186/s40537-016-0043-6#ref-CR31" id="ref-link-section-d43522e6150">31</a>] proposes to build a target learner based on a transferability measure from multiple related source domains. The approach first builds a logistic regression learner for each source domain. Next, a model transfer graph is constructed to represent the transferability between each source learner. In this case, transferability from a first learner to a second learner is defined as the performance of the second learner with learning from the first learner minus the performance of the second learner without learning from the first learner. Next, the model transfer graph is modified by adding the transferability measures between the target learner and all the source learners. Using spectral graph theory [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 17" title="Chung FRK. Spectral graph theory. In: CBMS regional conference series in mathematics, no. 92. Providence: American Mathematical Society; 1994." href="/article/10.1186/s40537-016-0043-6#ref-CR17" id="ref-link-section-d43522e6153">17</a>] on the model transfer graph, a transfer function is derived that maintains the geometry of the model transfer graph and is used in the final target learner to determine the level of transfer from each source. Experiments are performed in the applications of document classification and alphabet classification. Source domains are identified that are either related or unrelated to the target domain. The method by Eaton [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 31" title="Eaton E, des Jardins M, Lane T. Modeling transfer relationships between learning tasks for improved inductive transfer. Proc Mach Learn Knowl Disc Database. 2008;5211:317–32." href="/article/10.1186/s40537-016-0043-6#ref-CR31" id="ref-link-section-d43522e6156">31</a>] is tested along with a handpicked method where the source domains are manually selected to be related to the target, an average method that uses all sources available, and a baseline method that does not use transfer learning. Classification accuracy is the performance metric measured in the experiments. The source and target domains are represented by a homogeneous feature input space. The results of the experiments are mixed. Overall, the Eaton [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 31" title="Eaton E, des Jardins M, Lane T. Modeling transfer relationships between learning tasks for improved inductive transfer. Proc Mach Learn Knowl Disc Database. 2008;5211:317–32." href="/article/10.1186/s40537-016-0043-6#ref-CR31" id="ref-link-section-d43522e6159">31</a>] approach performs the best; however, there are certain instances where Eaton [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 31" title="Eaton E, des Jardins M, Lane T. Modeling transfer relationships between learning tasks for improved inductive transfer. Proc Mach Learn Knowl Disc Database. 2008;5211:317–32." href="/article/10.1186/s40537-016-0043-6#ref-CR31" id="ref-link-section-d43522e6162">31</a>] performed worse than the handpicked, average, and baseline methods. In the implementation of the algorithm, the transferability measure between two sources is required to be the same; however, the transferability from source 1 to source 2 is not always equal to the transferability from source 2 to source 1. A suggestion for future improvement is to use directed graphs to specify the bidirectional nature of the transferability measure between two sources.</p><p>The paper by Ge [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 40" title="Ge L, Gao J, Ngo H, Li K, Zhang A. On handling negative transfer and imbalanced distributions in multiple source transfer learning. In: Proceedings of the 2013 SIAM international conference on data mining. 2013. p. 254–71." href="/article/10.1186/s40537-016-0043-6#ref-CR40" id="ref-link-section-d43522e6168">40</a>] claims that knowledge transfer can be inhibited due to the existence of unrelated or irrelevant source domains. Further, current transfer learning solutions are focused on transferring knowledge from source domains to a target domain, but are not concerned about different source domains that could potentially be irrelevant and cause negative transfer. In the model presented by Ge [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 40" title="Ge L, Gao J, Ngo H, Li K, Zhang A. On handling negative transfer and imbalanced distributions in multiple source transfer learning. In: Proceedings of the 2013 SIAM international conference on data mining. 2013. p. 254–71." href="/article/10.1186/s40537-016-0043-6#ref-CR40" id="ref-link-section-d43522e6171">40</a>], there is a single target domain with limited labeled data and multiple labeled source domains for knowledge transfer. To reduce negative transfer effects from unrelated source domains, each source is assigned a weight (called the Supervised Local Weight) corresponding to how related the source is with the target (the higher the weight the more it is related). The supervised local weight is found by first using a spectral clustering algorithm (Chung [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 17" title="Chung FRK. Spectral graph theory. In: CBMS regional conference series in mathematics, no. 92. Providence: American Mathematical Society; 1994." href="/article/10.1186/s40537-016-0043-6#ref-CR17" id="ref-link-section-d43522e6174">17</a>]) on the unlabeled target information and propagating labels to the clusters from the labeled target information. Next, each source is separately clustered and labels assigned to the clusters from the labeled source. The supervised local weight of each source cluster is computed by comparing the source and target clusters. This solution further addresses the issue of imbalanced class distribution in source domains by preventing a high-weight class assignment in the case of high-accuracy predictions in a minority target class. The final target learner uses the supervised local weights to attenuate the effects of negative transfer. Experiments are performed in three application areas including cardiac arrhythmia detection, spam email filtering, and intrusion detection. Area under the curve is measured as the performance metric. The source and target domains are represented by a homogeneous feature input space. The method presented in this paper is compared against methods by Luo [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 71" title="Luo P, Zhuang F, Xiong H, Xiong Y, He Q. Transfer learning from multiple source domains via consensus regularization. In: Proceedings of the 17th ACM conference on information and knowledge management. 2008. p. 103–12." href="/article/10.1186/s40537-016-0043-6#ref-CR71" id="ref-link-section-d43522e6177">71</a>], by Gao [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 38" title="Gao J, Liang F, Fan W, Sun Y, Han J. Graph based consensus maximization among multiple supervised and unsupervised models. Adv Neural Inf Process Syst. 2009;22:1–9." href="/article/10.1186/s40537-016-0043-6#ref-CR38" id="ref-link-section-d43522e6180">38</a>], by Chattopadhyay [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="Chattopadhyay R, Ye J, Panchanathan S, Fan W, Davidson I. Multi-source domain adaptation and its application to early detection of fatigue. ACM Trans Knowl Dis Data (Best of SIGKDD 2011 TKDD Homepage archive) 2011; 6(4) (Article 18)." href="/article/10.1186/s40537-016-0043-6#ref-CR14" id="ref-link-section-d43522e6184">14</a>], and by Gao [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 37" title="Gao J, Fan W, Jiang J, Han J (2008) Knowledge transfer via multiple model local structure mapping. In: Proceedings of the 14th ACM SIGKDD international conference on knowledge discovery and data mining. p. 283–91." href="/article/10.1186/s40537-016-0043-6#ref-CR37" id="ref-link-section-d43522e6187">37</a>]. The Luo [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 71" title="Luo P, Zhuang F, Xiong H, Xiong Y, He Q. Transfer learning from multiple source domains via consensus regularization. In: Proceedings of the 17th ACM conference on information and knowledge management. 2008. p. 103–12." href="/article/10.1186/s40537-016-0043-6#ref-CR71" id="ref-link-section-d43522e6190">71</a>] and Gao [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 38" title="Gao J, Liang F, Fan W, Sun Y, Han J. Graph based consensus maximization among multiple supervised and unsupervised models. Adv Neural Inf Process Syst. 2009;22:1–9." href="/article/10.1186/s40537-016-0043-6#ref-CR38" id="ref-link-section-d43522e6193">38</a>] methods are the worst performing, most likely due to the fact that these solutions do not attempt to combat negative transfer effects. The Chattopadhyay [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="Chattopadhyay R, Ye J, Panchanathan S, Fan W, Davidson I. Multi-source domain adaptation and its application to early detection of fatigue. ACM Trans Knowl Dis Data (Best of SIGKDD 2011 TKDD Homepage archive) 2011; 6(4) (Article 18)." href="/article/10.1186/s40537-016-0043-6#ref-CR14" id="ref-link-section-d43522e6196">14</a>] and Gao [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 37" title="Gao J, Fan W, Jiang J, Han J (2008) Knowledge transfer via multiple model local structure mapping. In: Proceedings of the 14th ACM SIGKDD international conference on knowledge discovery and data mining. p. 283–91." href="/article/10.1186/s40537-016-0043-6#ref-CR37" id="ref-link-section-d43522e6199">37</a>] methods are the next best performing, which have means in place to reduce the effects of negative transfer from the source domains. The Chattopadhyay [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="Chattopadhyay R, Ye J, Panchanathan S, Fan W, Davidson I. Multi-source domain adaptation and its application to early detection of fatigue. ACM Trans Knowl Dis Data (Best of SIGKDD 2011 TKDD Homepage archive) 2011; 6(4) (Article 18)." href="/article/10.1186/s40537-016-0043-6#ref-CR14" id="ref-link-section-d43522e6203">14</a>] and Gao [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 37" title="Gao J, Fan W, Jiang J, Han J (2008) Knowledge transfer via multiple model local structure mapping. In: Proceedings of the 14th ACM SIGKDD international conference on knowledge discovery and data mining. p. 283–91." href="/article/10.1186/s40537-016-0043-6#ref-CR37" id="ref-link-section-d43522e6206">37</a>] methods do address the negative transfer problem but do not address the imbalanced distribution issue. The Ge [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 40" title="Ge L, Gao J, Ngo H, Li K, Zhang A. On handling negative transfer and imbalanced distributions in multiple source transfer learning. In: Proceedings of the 2013 SIAM international conference on data mining. 2013. p. 254–71." href="/article/10.1186/s40537-016-0043-6#ref-CR40" id="ref-link-section-d43522e6209">40</a>] method does exhibit the best overall performance due to the handling of negative transfer and imbalanced class distribution.</p><p>The paper by Seah [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 102" title="Seah CW, Ong YS, Tsang IW. Combating negative transfer from predictive distribution differences. IEEE Trans Cybern. 2013;43(4):1153–65." href="/article/10.1186/s40537-016-0043-6#ref-CR102" id="ref-link-section-d43522e6216">102</a>] claims the root cause of negative transfer is mainly due to conditional distribution differences between source domains <span class="mathjax-tex">\(\left( {{\text{P}}_{{{\text{S}}1}} ({\text{y}}\left| {\text{x}} \right.)\,\, \ne \,{\text{P}}_{{{\text{S}}2}} \left( {{\text{y}}\left| {\text{x}} \right.} \right)} \right)\)</span> and a difference in class distribution (class imbalance) between the source and target <span class="mathjax-tex">\(\left( {{\text{P}}_{\text{S}} ({\text{y}})\,\, \ne \,{\text{P}}_{\text{T}} \left( {\text{y}} \right)} \right)\)</span>. Because the target domain usually contains a small number of labeled instances, it is difficult to find the true class distribution of the target domain. A predictive distribution matching (PDM) framework is proposed to align the conditional distributions of the source domains and target domain in an attempt to minimize negative transfer effects. A positive transferability measure is defined that measures the transferability of instance pairs with the same label from the source and target domains. The first step in the PDM framework is to assign pseudo labels to the unlabeled target data. This is accomplished by an iterative process that forces source and target instances which are similar (as defined by the positive transferability measure) to have the same label. Next, irrelevant source data are removed by identifying data that does not align with the conditional distribution of the pseudo labeled target data for each class. Both logistic regression and SVM classifiers are implemented using the PDM framework. Experiments are performed on document classification using the PDM method described in this paper, the approach from Daumé [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 22" title="Daumé H III. Frustratingly easy domain adaptation. In: Proceedings of ACL. 2007. p. 256–63." href="/article/10.1186/s40537-016-0043-6#ref-CR22" id="ref-link-section-d43522e6366">22</a>], the approach from Huang [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 51" title="Huang J, Smola A, Gretton A, Borgwardt KM, Schölkopf B. Correcting sample selection bias by unlabeled data. In: Proceedings of the 2006 conference. Adv Neural Inf Process Syst. 2006. p. 601–8." href="/article/10.1186/s40537-016-0043-6#ref-CR51" id="ref-link-section-d43522e6369">51</a>], and the approach from Bruzzone [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 11" title="Bruzzone L, Marconcini M. Domain adaptation problems: a DASVM classification technique and a circular validation strategy. IEEE Trans Pattern Anal Mach Intell. 2010;32(5):770–87." href="/article/10.1186/s40537-016-0043-6#ref-CR11" id="ref-link-section-d43522e6373">11</a>]. Classification accuracy is measured as the performance metric. The source and target domains are represented by a homogeneous feature input space. The PDM approach demonstrates better performance as compared to the other approaches tested as these solutions do not attempt to account for negative transfer effects.</p><p>A select number of previously surveyed papers contain solutions addressing negative transfer. The paper by Yang [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 136" title="Yang L, Jing L, Yu J, Ng MK. Learning transferred weights from co-occurrence data for heterogeneous transfer learning. IEEE Trans Neural Netw Learn Syst. 2015;PP(99):1–14." href="/article/10.1186/s40537-016-0043-6#ref-CR136" id="ref-link-section-d43522e6379">136</a>] addresses the negative transfer issue, which is presented in the “<a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1186/s40537-016-0043-6#Sec11">Heterogeneous transfer learning</a>” section. The homogeneous solution by Gong [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 42" title="Gong B, Shi Y, Sha F, Grauman K. Geodesic flow kernel for unsupervised domain adaptation. In: Proceedings of the 2012 IEEE conference on computer vision and pattern recognition. 2012. p. 2066–73." href="/article/10.1186/s40537-016-0043-6#ref-CR42" id="ref-link-section-d43522e6385">42</a>] defines an ROD value that measures the relatedness between a source and target domain. The work presented in Chattopadhyay [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="Chattopadhyay R, Ye J, Panchanathan S, Fan W, Davidson I. Multi-source domain adaptation and its application to early detection of fatigue. ACM Trans Knowl Dis Data (Best of SIGKDD 2011 TKDD Homepage archive) 2011; 6(4) (Article 18)." href="/article/10.1186/s40537-016-0043-6#ref-CR14" id="ref-link-section-d43522e6388">14</a>] is a multiple source transfer learning approach that calculates the source weights as a function of conditional probability differences between the source and target domains attempting to give the most related sources the highest weights. Duan [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 28" title="Duan L, Xu D, Chang SF. Exploiting web images for event recognition in consumer videos: a multiple source domain adaptation approach. In: IEEE 2012 conference on computer vision and pattern recognition. 2012. p. 1338–45." href="/article/10.1186/s40537-016-0043-6#ref-CR28" id="ref-link-section-d43522e6391">28</a>] proposes a transfer learning approach that only uses source domains that are deemed relevant and test data demonstrates better performance compared to methods with no negative transfer protection.</p><p>The previous papers attempt to measure how related source data is to the target data in a transfer learning environment and then selectively transfer the information that is highly related. The experiments in the above papers demonstrate that accounting for negative transfer effects from source domain data can improve target learner performance. However, most transfer learning solutions do not attempt to account for negative transfer effects. Robust negative transfer measurements are difficult to define. Since the target domain typically has limited labeled data, it is inherently difficult to find a true measure of the relatedness between the source and target domains. Further, by selectively transferring information that seems related to the limited labeled target domain, a risk of overfitting in the target learner is a concern. The topic of negative transfer is a fertile area for further research.</p></div></div></section><section aria-labelledby="Sec18" data-title="Transfer learning applications"><div class="c-article-section" id="Sec18-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec18">Transfer learning applications</h2><div class="c-article-section__content" id="Sec18-content"><p>The surveyed works in this paper demonstrate that transfer learning has been applied to many real-world applications. There are a number of application examples pertaining to natural language processing, more specifically in the areas of sentiment classification, text classification, spam email detection, and multiple language text classification. Other well-represented transfer learning applications include image classification and video concept classification. Applications that are more selectively addressed in the previous papers include WiFi localization classification, muscle fatigue classification, drug efficacy classification, human activity classification, software defect classification, and cardiac arrhythmia classification.</p><p>The majority of the solutions surveyed are generic, meaning the solution can be easily applied to applications other than the ones implemented and tested in the papers. The application-specific solutions tend to be related to the field of natural language processing and image processing. In the literature, there are a number of transfer learning solutions that are specific to the application of recommendation systems. Recommendation systems provide users with recommendations or ratings for a particular domain (e.g. movies, books, etc.), which are based on historical information. However, when the system does not have sufficient historical information (referred to as the data sparsity issue presented in Moreno [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 76" title="Moreno O, Shapira B, Rokach L, Shani G (2012) TALMUD—transfer learning for multiple domains. In: Proceedings of the 21st ACM international conference on information and knowledge management. 2012. p. 425–34." href="/article/10.1186/s40537-016-0043-6#ref-CR76" id="ref-link-section-d43522e6409">76</a>]), then the recommendations are not reliable. In the cases where the system does not have sufficient domain data to make reliable predictions (for example when a movie is just released), there is a need to use previously collected information from a different domain (using books for example). The aforementioned problem has been directly addressed using transfer learning methodologies and captured in papers by Moreno [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 76" title="Moreno O, Shapira B, Rokach L, Shani G (2012) TALMUD—transfer learning for multiple domains. In: Proceedings of the 21st ACM international conference on information and knowledge management. 2012. p. 425–34." href="/article/10.1186/s40537-016-0043-6#ref-CR76" id="ref-link-section-d43522e6412">76</a>], Cao [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 12" title="Cao B, Liu N, Yang Q. Transfer learning for collective link prediction in multiple heterogeneous domains. In: Proceedings of the 27th international conference on machine learning. 2010. p. 159–66." href="/article/10.1186/s40537-016-0043-6#ref-CR12" id="ref-link-section-d43522e6415">12</a>], Li [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 60" title="Li B, Yang Q, Xue X. Can movies and books collaborate? Cross-domain collaborative filtering for sparsity reduction. In: Proceedings of the 21st international joint conference on artificial intelligence. 2009. p. 2052–57." href="/article/10.1186/s40537-016-0043-6#ref-CR60" id="ref-link-section-d43522e6418">60</a>], Li [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 61" title="Li B, Yang Q, Xue X. Transfer learning for collaborative filtering via a rating-matrix generative model. In: Proceedings of the 26th annual international conference on machine learning. 2009. p. 617–24." href="/article/10.1186/s40537-016-0043-6#ref-CR61" id="ref-link-section-d43522e6421">61</a>], Pan [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 86" title="Pan W. Xiang EW, Liu NN, Yang Q. Transfer learning in collaborative filtering for sparsity reduction. In: Twenty-fourth AAAI conference on artificial intelligence, vol. 1. 2010. p. 230–235." href="/article/10.1186/s40537-016-0043-6#ref-CR86" id="ref-link-section-d43522e6425">86</a>], Zhang [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 140" title="Zhang Y, Cao B, Yeung D. Multi-domain collaborative filtering. In: Proceedings of the 26th conference on uncertainty in artificial intelligence. 2010. p. 725–32." href="/article/10.1186/s40537-016-0043-6#ref-CR140" id="ref-link-section-d43522e6428">140</a>], Pan [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 85" title="Pan W, Liu NN, Xiang EW, Yang Q. Transfer learning to predict missing ratings via heterogeneous user feedbacks. In: Proceedings of the 22nd international joint conference on artificial intelligence. 2011. p. 2318–23." href="/article/10.1186/s40537-016-0043-6#ref-CR85" id="ref-link-section-d43522e6431">85</a>], Roy [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 99" title="Roy S.D., Mei T., Zeng W., Li S. Social transfer: cross-domain transfer learning from social streams for media applications. In: Proceedings of the 20th ACM international conference on multimedia. p. 649–58." href="/article/10.1186/s40537-016-0043-6#ref-CR99" id="ref-link-section-d43522e6434">99</a>], Jiang [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 54" title="Jiang M, Cui P, Wang F, Yang Q, Zhu W, Yang S. Social recommendation across multiple relational domains. In: Proceedings of the 21st ACM international conference on information and knowledge management. 2012. p. 1422–31." href="/article/10.1186/s40537-016-0043-6#ref-CR54" id="ref-link-section-d43522e6437">54</a>], and Zhao [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 142" title="Zhao L, Pan SJ, Xiang EW, Zhong E, Lu Z, Yang Q. Active transfer learning for cross-system recommendation. In: Proceedings of the 27th AAAI conference on artificial intelligence. 2013. p. 1205–11." href="/article/10.1186/s40537-016-0043-6#ref-CR142" id="ref-link-section-d43522e6440">142</a>].</p><p>Transfer learning solutions continue to be applied to a diverse number of real-world applications, and in some cases the applications are quite obscure. The application of head pose classification finds a learner trained with previously captured labeled head positions to predict a new head position. Head pose classification is used for determining the attentiveness of drivers, analyzing social behavior, and human interaction with robots. Head positions captured in source training data will have different head tilt ranges and angles than that of the predicted target. The paper by Rajagopal [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 96" title="Rajagopal AN, Subramanian R, Ricci E, Vieriu RL, Lanz O, Ramakrishnan KR, Sebe N. Exploring transfer learning approaches for head pose classification from multi-view surveillance images. Int J Comput Vis. 2014;109(1–2):146–67." href="/article/10.1186/s40537-016-0043-6#ref-CR96" id="ref-link-section-d43522e6446">96</a>] addresses the head pose classification issues using transfer learning solutions.</p><p>Other transfer learning applications include the paper by Ma [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 72" title="Ma Y, Gong W, Mao F. Transfer learning used to analyze the dynamic evolution of the dust aerosol. J Quant Spectrosc Radiat Transf. 2015;153:119–30." href="/article/10.1186/s40537-016-0043-6#ref-CR72" id="ref-link-section-d43522e6452">72</a>] that uses transfer learning for atmospheric dust aerosol particle classification to enhance global climate models. Here the TrAdaBoost algorithm proposed by Dai [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 21" title="Dai W, Yang Q, Xue GR, Yu Y (2007) Boosting for transfer learning. In: Proceedings of the 24th international conference on machine learning. p. 193–200." href="/article/10.1186/s40537-016-0043-6#ref-CR21" id="ref-link-section-d43522e6455">21</a>] is used in conjunction with an SVM classifier to improve on classification results. Being able to identify areas of low income in developing countries is important for disaster relief efforts, food security, and achieving sustainable growth. To better predict poverty mapping, Xie [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 134" title="Xie M, Jean N, Burke M, Lobell D, Ermon S. Transfer learning from deep features for remote sensing and poverty mapping. In: Proceedings 30th AAAI conference on artificial intelligence. 2015. p. 1–10." href="/article/10.1186/s40537-016-0043-6#ref-CR134" id="ref-link-section-d43522e6458">134</a>] proposes an approach similar to Oquab [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 81" title="Oquab M, Bottou L, Laptev I, Sivic J. Learning and transferring mid-level image representations using convolutional neural networks. In: Proceedings of the 2014 IEEE conference on computer vision and pattern recognition. 2013. p. 1717–24." href="/article/10.1186/s40537-016-0043-6#ref-CR81" id="ref-link-section-d43522e6461">81</a>] that uses a convolution neural network model. The first prediction model is trained to predict night time light intensity from source image data. The final target prediction model predicts the poverty mapping from source night time light intensity data. In the paper by Ogoe [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 80" title="Ogoe HA, Visweswaran S, Lu X, Gopalakrishnan V. Knowledge transfer via classification rules using functional mapping for integrative modeling of gene expression data. BMC Bioinform. 2015. p. 1–15." href="/article/10.1186/s40537-016-0043-6#ref-CR80" id="ref-link-section-d43522e6464">80</a>], transfer learning in used to enhance disease prediction. In this solution, a rule-based learning approach is formulated to use abstract source domain data to perform modeling of multiple types of gene expression data. Online display web advertising is a growing industry where transfer learning is used to optimally predict targeted ads. In the paper by Perlich [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 90" title="Perlich C, Dalessandro B, Raeder T, Stitelman O, Provost F. Machine learning for targeted display advertising: transfer learning in action. Mach Learn. 2014;95:103–27." href="/article/10.1186/s40537-016-0043-6#ref-CR90" id="ref-link-section-d43522e6468">90</a>], a transfer learning approach is employed that uses the weighted outputs of multiple source classifiers to enhance a target classifier trained to predict targeted online display advertising results. The paper by Kan [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 56" title="Kan M, Wu J, Shan S, Chen X. Domain adaptation for face recognition: targetize source domain bridged by common subspace. Int J Comput Vis. 2014;109(1–2):94–109." href="/article/10.1186/s40537-016-0043-6#ref-CR56" id="ref-link-section-d43522e6471">56</a>] addresses the field of facial recognition and is able to use face image information from one ethnic group to improve the learning of a classifier for a different ethnic group. The paper by Farhadi [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 35" title="Farhadi A, Forsyth D, White R. Transfer learning in sign language. In: IEEE 2007 conference on computer vision and pattern recognition. 2007. p. 1–8." href="/article/10.1186/s40537-016-0043-6#ref-CR35" id="ref-link-section-d43522e6474">35</a>] is focused on the application of sign language recognition where the model is able to learn from different people signing at various angles. Transfer learning is applied to the field of biology in the paper by Widmer [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 127" title="Widmer C, Ratsch G. Multitask learning in computational biology. JMLR. 2012;27:207–16." href="/article/10.1186/s40537-016-0043-6#ref-CR127" id="ref-link-section-d43522e6477">127</a>]. Specifically, a multi-task learning approach is used in the prediction of splice sites in genome biology. Predicting if patients will contract particular bacteria when admitted to a hospital is addressed in the paper by Wiens [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 128" title="Wiens J, Guttag J, Horvitz EJ. A study in transfer learning: leveraging data from multiple hospitals to enhance hospital-specific predictions. J Am Med Inform Assoc. 2013;21(4):699–706." href="/article/10.1186/s40537-016-0043-6#ref-CR128" id="ref-link-section-d43522e6480">128</a>]. Information taken from different hospitals is used to predict the infection rate for a different hospital. In the paper by Romera-Paredes [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 97" title="Romera-Paredes B, Aung MSH, Pontil M, Bianchi-Berthouze N, Williams AC de C, Watson P. Transfer learning to account for idiosyncrasy in face and body expressions. In: Proceedings of the 10th international conference on automatic face and gesture recognition (FG). 2013. p. 1–6." href="/article/10.1186/s40537-016-0043-6#ref-CR97" id="ref-link-section-d43522e6483">97</a>], a multi-task transfer learning approach is used to predict pain levels from an individual’s facial expression by using labeled source facial images from other individuals. The paper by Deng [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 25" title="Deng J, Zhang Z, Marchi E, Schuller B. Sparse autoencoder based feature transfer learning for speech emotion recognition. In: Humaine association conference on affective computing and intelligent interaction. 2013. p. 511–6." href="/article/10.1186/s40537-016-0043-6#ref-CR25" id="ref-link-section-d43522e6487">25</a>] applies transfer learning to the application of speech emotion recognition where information is transferred from multiple labeled speech sources. The application of wine quality classification is implemented in Zhang [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 141" title="Zhang Y, Yeung DY. Transfer metric learning by learning task relationships. In: Proceedings of the 16th ACM SIGKDD international conference on knowledge discovery and data mining. 2010. p. 1199–208." href="/article/10.1186/s40537-016-0043-6#ref-CR141" id="ref-link-section-d43522e6490">141</a>] using a multi-task transfer learning approach. As a reference, the survey paper by Cook [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 19" title="Cook DJ, Feuz KD, Krishnan NC. Transfer learning for activity recognition: a survey. Knowl Inf Syst. 2012;36(3):537–56." href="/article/10.1186/s40537-016-0043-6#ref-CR19" id="ref-link-section-d43522e6493">19</a>] covers transfer learning for the application of activity recognition and the survey papers by Patel [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 89" title="Patel VM, Gopalan R, Li R, Chellappa R. Visual domain adaptation: a survey of recent advances. IEEE Signal Process Mag. 2014;32(3):53–69." href="/article/10.1186/s40537-016-0043-6#ref-CR89" id="ref-link-section-d43522e6496">89</a>] and Shao [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 103" title="Shao L, Zhu F, Li X. Transfer learning for visual categorization: a survey. IEEE Trans Neural Netw Learn Syst. 2014;26(5):1019–34." href="/article/10.1186/s40537-016-0043-6#ref-CR103" id="ref-link-section-d43522e6499">103</a>] address transfer learning in the domain of image recognition.</p></div></div></section><section aria-labelledby="Sec19" data-title="Conclusion and discussion"><div class="c-article-section" id="Sec19-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec19">Conclusion and discussion</h2><div class="c-article-section__content" id="Sec19-content"><p>The subject of transfer learning is a well-researched area as evidenced with more than 700 academic papers addressing the topic in the last 5 years. This survey paper presents solutions from the literature representing current trends in transfer learning. Homogeneous transfer learning papers are surveyed that demonstrate instance-based, feature-based, parameter-based, and relational-based information transfer techniques. Solutions having various requirements for labeled and unlabeled data are also presented as a key attribute. The relatively new area of heterogeneous transfer learning is surveyed showing the two dominant approaches for domain adaptation being asymmetric and symmetric transformations. Many real-world applications that transfer learning is applied to are listed and discussed in this survey paper. In some cases, the proposed transfer learning solutions are very specific to the underlying application and cannot be generically used for other applications. A list of software downloads implementing a portion of the solutions surveyed is presented in the appendix of this paper. A great benefit to researchers is to have software available from previous solutions so experiments can be performed more efficiently and more reliably. A single open-source software repository for published transfer learning solutions would be a great asset to the research community.</p><p>In many transfer learning solutions, the domain adaptation process performed is focused either on correcting the marginal distribution differences or the conditional distribution differences between the source and target domains. Correcting the conditional distribution differences is a challenging problem due to the lack of labeled target data. To address the lack of labeled target data, some solutions estimate the labels for the target data (called pseudo labels), which are then used to correct the conditional distribution differences. This method is problematic because the conditional distribution corrections are being made with the aid of pseudo labels. Improved methods for correcting the conditional distribution differences is a potential area of future research. A number of more recent works attempt to correct both the marginal distribution differences and the conditional distribution differences during the domain adaptation process. An area of future work is to quantify the advantage of correcting both distributions and in what scenarios it is most effective. Further, Long [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 68" title="Long M, Wang J, Ding G, Pan SJ, Yu PS. Adaptation regularization: a general framework for transfer learning. IEEE Trans Knowl Data Eng. 2014;26(5):1076–89." href="/article/10.1186/s40537-016-0043-6#ref-CR68" id="ref-link-section-d43522e6513">68</a>] states that the simultaneous solving of marginal and conditional distribution differences is preferred over serial alignment as it reduces the risk of overfitting. Another area of future work is to quantify any performance gains for simultaneously solving both distribution differences. In addition to solving for distribution differences in the domain adaptation process, exploring possible data preprocessing steps using heuristic knowledge of the domain features can be used as a method to improve the target learner performance. The heuristic knowledge would represent a set of complex rules or relations that standard transfer learning techniques cannot account for. In most cases, this heuristic knowledge would be specific to each domain, which would not lead to a generic solution. However, if such a preprocessing step leads to improved target learner performance, it is likely worth the effort.</p><p>A trend observed in the formulation of transfer learning solutions is in the implementation of a one-stage process as opposed to a two-stage process. A two-stage solution first performs the domain adaptation process and then independently learns the final classifier. A one-stage process simultaneously performs the domain adaptation process while learning the final classifier. Recent solutions employing a one-stage solution include Long [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 68" title="Long M, Wang J, Ding G, Pan SJ, Yu PS. Adaptation regularization: a general framework for transfer learning. IEEE Trans Knowl Data Eng. 2014;26(5):1076–89." href="/article/10.1186/s40537-016-0043-6#ref-CR68" id="ref-link-section-d43522e6519">68</a>], Duan [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 27" title="Duan L, Tsang IW, Xu D. Domain transfer multiple kernel learning. IEEE Trans Pattern Anal Mach Intell. 2012;34(3):465–79." href="/article/10.1186/s40537-016-0043-6#ref-CR27" id="ref-link-section-d43522e6522">27</a>], Shi [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 106" title="Shi Y, Sha F. Information-theoretical learning of discriminative clusters for unsupervised domain adaptation. In: Proceedings of the 29th international conference on machine learning. 2012. p. 1–8." href="/article/10.1186/s40537-016-0043-6#ref-CR106" id="ref-link-section-d43522e6525">106</a>], Xia [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 132" title="Xia R, Zong C, Hu X, Cambria E. Feature ensemble plus sample selection: domain adaptation for sentiment classification. IEEE Intell Syst. 2013;28(3):10–8." href="/article/10.1186/s40537-016-0043-6#ref-CR132" id="ref-link-section-d43522e6528">132</a>], and Duan [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 30" title="Duan L, Xu D, Tsang IW. Learning with augmented features for heterogeneous domain adaptation. IEEE Trans Pattern Anal Mach Intell. 2012;36(6):1134–48." href="/article/10.1186/s40537-016-0043-6#ref-CR30" id="ref-link-section-d43522e6531">30</a>]. With respect to the one-stage solution, Long [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 68" title="Long M, Wang J, Ding G, Pan SJ, Yu PS. Adaptation regularization: a general framework for transfer learning. IEEE Trans Knowl Data Eng. 2014;26(5):1076–89." href="/article/10.1186/s40537-016-0043-6#ref-CR68" id="ref-link-section-d43522e6535">68</a>] claims the simultaneous solving of domain adaptation and the classifier establishes mutual reinforcement for enhanced performance. An area of future work is to better quantify the effects of a one-stage approach over a two-stage approach.</p><p>This paper surveys a number of works addressing the topic of negative transfer. The subject of negative transfer is still a lightly researched area. The expanded integration of negative transfer techniques into transfer learning solutions is a natural extension for future research. Solutions supporting multiple source domains enabling the splitting of larger source domains into smaller domains to more easily discriminate against unrelated source data are a logical area for continued research. Additionally, optimal transfer is another fertile area for future research. Negative transfer is defined as a source domain having a negative impact on a target learner. The concept of optimal transfer is when select information from a source domain is transferred to achieve the highest possible performance in a target learner. There is overlap between the concepts of negative transfer and optimal transfer; however, optimal transfer attempts to find the best performing target learner, which goes well beyond the negative transfer concept.</p><p>With the recent proliferation of sensors being deployed in cell phones, vehicles, buildings, roadways, and computers, larger and more diverse information is being collected. The diversity in data collection makes heterogeneous transfer learning solutions more important moving forward. Larger data collection sizes highlight the potential for big data solutions being deployed concurrent with current transfer learning solutions. How the diversity and large size of sensor data integrates into transfer learning solutions is an interesting topic of future research. Another area of future work pertains to the scenario where the output label space is different between domains. With new data sets being captured and being made available, this topic could be a needed area of focus for the future. Lastly, the literature has very few transfer learning solutions addressing the scenario of unlabeled source and unlabeled target data, which is certainly an area for expanded research.</p></div></div></section>
                        
                    

                    <section aria-labelledby="Bib1" data-title="References"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="A literature survey on domain adaptation of statistical classifiers. http://sifaka.cs.uiuc.edu/jiang4/domain_a" /><span class="c-article-references__counter">1.</span><p class="c-article-references__text" id="ref-CR1">A literature survey on domain adaptation of statistical classifiers. <a href="http://sifaka.cs.uiuc.edu/jiang4/domain_adaptation/survey/da_survey.html">http://sifaka.cs.uiuc.edu/jiang4/domain_adaptation/survey/da_survey.html</a>. Accessed 4 Mar 2016.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="RK. Ando, T. Zhang, " /><meta itemprop="datePublished" content="2005" /><meta itemprop="headline" content="Ando RK, Zhang T. A framework for learning predictive structures from multiple tasks and unlabeled data. J Mac" /><span class="c-article-references__counter">2.</span><p class="c-article-references__text" id="ref-CR2">Ando RK, Zhang T. A framework for learning predictive structures from multiple tasks and unlabeled data. J Mach Learn Res. 2005;6:1817–53.</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.ams.org/mathscinet-getitem?mr=2249873" aria-label="View reference 2 on MathSciNet">MathSciNet</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.emis.de/MATH-item?1222.68133" aria-label="View reference 2 on MATH">MATH</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 2 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20framework%20for%20learning%20predictive%20structures%20from%20multiple%20tasks%20and%20unlabeled%20data&amp;journal=J%20Mach%20Learn%20Res&amp;volume=6&amp;pages=1817-1853&amp;publication_year=2005&amp;author=Ando%2CRK&amp;author=Zhang%2CT">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="H. Bay, T. Tuytelaars, LV. Gool, " /><meta itemprop="datePublished" content="2006" /><meta itemprop="headline" content="Bay H, Tuytelaars T, Gool LV. Surf: speeded up robust features. Comput Vis Image Underst. 2006;110(3):346–59." /><span class="c-article-references__counter">3.</span><p class="c-article-references__text" id="ref-CR3">Bay H, Tuytelaars T, Gool LV. Surf: speeded up robust features. Comput Vis Image Underst. 2006;110(3):346–59.</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.cviu.2007.09.014" aria-label="View reference 3">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 3 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Surf%3A%20speeded%20up%20robust%20features&amp;journal=Comput%20Vis%20Image%20Underst&amp;volume=110&amp;issue=3&amp;pages=346-359&amp;publication_year=2006&amp;author=Bay%2CH&amp;author=Tuytelaars%2CT&amp;author=Gool%2CLV">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="M. Belkin, P. Niyogi, V. Sindhwani, " /><meta itemprop="datePublished" content="2006" /><meta itemprop="headline" content="Belkin M, Niyogi P, Sindhwani V. Manifold regularization: a geometric framework for learning from examples. J " /><span class="c-article-references__counter">4.</span><p class="c-article-references__text" id="ref-CR4">Belkin M, Niyogi P, Sindhwani V. Manifold regularization: a geometric framework for learning from examples. J Mach Learn Res Arch. 2006;7:2399–434.</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.ams.org/mathscinet-getitem?mr=2274444" aria-label="View reference 4 on MathSciNet">MathSciNet</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.emis.de/MATH-item?1222.68144" aria-label="View reference 4 on MATH">MATH</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 4 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Manifold%20regularization%3A%20a%20geometric%20framework%20for%20learning%20from%20examples&amp;journal=J%20Mach%20Learn%20Res%20Arch&amp;volume=7&amp;pages=2399-2434&amp;publication_year=2006&amp;author=Belkin%2CM&amp;author=Niyogi%2CP&amp;author=Sindhwani%2CV">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Blitzer, J, McDonald R, Pereira F. Domain adaptation with structural correspondence learning. In: Proceedings " /><span class="c-article-references__counter">5.</span><p class="c-article-references__text" id="ref-CR5">Blitzer, J, McDonald R, Pereira F. Domain adaptation with structural correspondence learning. In: Proceedings of the 2006 conference on empirical methods in natural language processing. 2006;120–8.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="BoChen90 Update TrAdaBoost.m. https://github.com/BoChen90/machine-learning-matlab/blob/master/TrAdaBoost.m. Ac" /><span class="c-article-references__counter">6.</span><p class="c-article-references__text" id="ref-CR6">BoChen90 Update TrAdaBoost.m. <a href="https://github.com/BoChen90/machine-learning-matlab/blob/master/TrAdaBoost.m">https://github.com/BoChen90/machine-learning-matlab/blob/master/TrAdaBoost.m</a>. Accessed 4 Mar 2016.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Bolt online learning toolbox. http://pprett.github.com/bolt/. Accessed 4 Mar 2016." /><span class="c-article-references__counter">7.</span><p class="c-article-references__text" id="ref-CR7">Bolt online learning toolbox. <a href="http://pprett.github.com/bolt/">http://pprett.github.com/bolt/</a>. Accessed 4 Mar 2016.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Bonilla E, Chai KM, Williams C. Multi-task Gaussian process prediction. In: Proceedings of the 20th annual con" /><span class="c-article-references__counter">8.</span><p class="c-article-references__text" id="ref-CR8">Bonilla E, Chai KM, Williams C. Multi-task Gaussian process prediction. In: Proceedings of the 20th annual conference of neural information processing systems. 2008. 153–60.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Gong B. http://www-scf.usc.edu/~boqinggo/. Accessed 4 Mar 2016." /><span class="c-article-references__counter">9.</span><p class="c-article-references__text" id="ref-CR9">Gong B. <a href="http://www-scf.usc.edu/%7eboqinggo/">http://www-scf.usc.edu/~boqinggo/</a>. Accessed 4 Mar 2016.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="KM. Borgwardt, A. Gretton, MJ. Rasch, HP. Kriegel, B. Schölkopf, AJ. Smola, " /><meta itemprop="datePublished" content="2006" /><meta itemprop="headline" content="Borgwardt KM, Gretton A, Rasch MJ, Kriegel HP, Schölkopf B, Smola AJ. Integrating structured biological data b" /><span class="c-article-references__counter">10.</span><p class="c-article-references__text" id="ref-CR10">Borgwardt KM, Gretton A, Rasch MJ, Kriegel HP, Schölkopf B, Smola AJ. Integrating structured biological data by kernel maximum mean discrepancy. Bioinformatics. 2006;22(4):49–57.</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1093%2Fbioinformatics%2Fbtl242" aria-label="View reference 10">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 10 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Integrating%20structured%20biological%20data%20by%20kernel%20maximum%20mean%20discrepancy&amp;journal=Bioinformatics&amp;volume=22&amp;issue=4&amp;pages=49-57&amp;publication_year=2006&amp;author=Borgwardt%2CKM&amp;author=Gretton%2CA&amp;author=Rasch%2CMJ&amp;author=Kriegel%2CHP&amp;author=Sch%C3%B6lkopf%2CB&amp;author=Smola%2CAJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="L. Bruzzone, M. Marconcini, " /><meta itemprop="datePublished" content="2010" /><meta itemprop="headline" content="Bruzzone L, Marconcini M. Domain adaptation problems: a DASVM classification technique and a circular validati" /><span class="c-article-references__counter">11.</span><p class="c-article-references__text" id="ref-CR11">Bruzzone L, Marconcini M. Domain adaptation problems: a DASVM classification technique and a circular validation strategy. IEEE Trans Pattern Anal Mach Intell. 2010;32(5):770–87.</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FTPAMI.2009.57" aria-label="View reference 11">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 11 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Domain%20adaptation%20problems%3A%20a%20DASVM%20classification%20technique%20and%20a%20circular%20validation%20strategy&amp;journal=IEEE%20Trans%20Pattern%20Anal%20Mach%20Intell&amp;volume=32&amp;issue=5&amp;pages=770-787&amp;publication_year=2010&amp;author=Bruzzone%2CL&amp;author=Marconcini%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Cao B, Liu N, Yang Q. Transfer learning for collective link prediction in multiple heterogeneous domains. In: " /><span class="c-article-references__counter">12.</span><p class="c-article-references__text" id="ref-CR12">Cao B, Liu N, Yang Q. Transfer learning for collective link prediction in multiple heterogeneous domains. In: Proceedings of the 27th international conference on machine learning. 2010. p. 159–66.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Cawley G. Leave-one-out cross-validation based model selection criteria for weighted LS-SVMs. In: IEEE 2006 in" /><span class="c-article-references__counter">13.</span><p class="c-article-references__text" id="ref-CR13">Cawley G. Leave-one-out cross-validation based model selection criteria for weighted LS-SVMs. In: IEEE 2006 international joint conference on neural network proceedings 2006. p. 1661–68.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Chattopadhyay R, Ye J, Panchanathan S, Fan W, Davidson I. Multi-source domain adaptation and its application t" /><span class="c-article-references__counter">14.</span><p class="c-article-references__text" id="ref-CR14">Chattopadhyay R, Ye J, Panchanathan S, Fan W, Davidson I. Multi-source domain adaptation and its application to early detection of fatigue. ACM Trans Knowl Dis Data (Best of SIGKDD 2011 TKDD Homepage archive) 2011; 6(4) <b>(Article 18)</b>.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="C. Chelba, A. Acero, " /><meta itemprop="datePublished" content="2004" /><meta itemprop="headline" content="Chelba C, Acero A. Adaptation of maximum entropy classifier: little data can help a lot. Comput Speech Lang. 2" /><span class="c-article-references__counter">15.</span><p class="c-article-references__text" id="ref-CR15">Chelba C, Acero A. Adaptation of maximum entropy classifier: little data can help a lot. Comput Speech Lang. 2004;20(4):382–99.</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.csl.2005.05.005" aria-label="View reference 15">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 15 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Adaptation%20of%20maximum%20entropy%20classifier%3A%20little%20data%20can%20help%20a%20lot&amp;journal=Comput%20Speech%20Lang&amp;volume=20&amp;issue=4&amp;pages=382-399&amp;publication_year=2004&amp;author=Chelba%2CC&amp;author=Acero%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Chen M, Xu ZE, Weinberger KQ, Sha F (2012) Marginalized denoising autoencoders for domain adaptation. ICML. ar" /><span class="c-article-references__counter">16.</span><p class="c-article-references__text" id="ref-CR16">Chen M, Xu ZE, Weinberger KQ, Sha F (2012) Marginalized denoising autoencoders for domain adaptation. ICML. arXiv preprintarXiv:1206.4683.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Chung FRK. Spectral graph theory. In: CBMS regional conference series in mathematics, no. 92. Providence: Amer" /><span class="c-article-references__counter">17.</span><p class="c-article-references__text" id="ref-CR17">Chung FRK. Spectral graph theory. In: CBMS regional conference series in mathematics, no. 92. Providence: American Mathematical Society; 1994.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Computer Vision and Learning Group. http://vision.cs.uml.edu/adaptation.html. Accessed 4 Mar 2016." /><span class="c-article-references__counter">18.</span><p class="c-article-references__text" id="ref-CR18">Computer Vision and Learning Group. <a href="http://vision.cs.uml.edu/adaptation.html">http://vision.cs.uml.edu/adaptation.html</a>. Accessed 4 Mar 2016.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="DJ. Cook, KD. Feuz, NC. Krishnan, " /><meta itemprop="datePublished" content="2012" /><meta itemprop="headline" content="Cook DJ, Feuz KD, Krishnan NC. Transfer learning for activity recognition: a survey. Knowl Inf Syst. 2012;36(3" /><span class="c-article-references__counter">19.</span><p class="c-article-references__text" id="ref-CR19">Cook DJ, Feuz KD, Krishnan NC. Transfer learning for activity recognition: a survey. Knowl Inf Syst. 2012;36(3):537–56.</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs10115-013-0665-3" aria-label="View reference 19">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 19 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Transfer%20learning%20for%20activity%20recognition%3A%20a%20survey&amp;journal=Knowl%20Inf%20Syst&amp;volume=36&amp;issue=3&amp;pages=537-556&amp;publication_year=2012&amp;author=Cook%2CDJ&amp;author=Feuz%2CKD&amp;author=Krishnan%2CNC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="W. Dai, Y. Chen, GR. Xue, Q. Yang, Y. Yu, " /><meta itemprop="datePublished" content="2008" /><meta itemprop="headline" content="Dai W, Chen Y, Xue GR, Yang Q, Yu Y. Translated learning: transfer learning across different feature spaces. A" /><span class="c-article-references__counter">20.</span><p class="c-article-references__text" id="ref-CR20">Dai W, Chen Y, Xue GR, Yang Q, Yu Y. Translated learning: transfer learning across different feature spaces. Adv Neural Inform Process Syst. 2008;21:353–60.</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 20 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Translated%20learning%3A%20transfer%20learning%20across%20different%20feature%20spaces&amp;journal=Adv%20Neural%20Inform%20Process%20Syst&amp;volume=21&amp;pages=353-360&amp;publication_year=2008&amp;author=Dai%2CW&amp;author=Chen%2CY&amp;author=Xue%2CGR&amp;author=Yang%2CQ&amp;author=Yu%2CY">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Dai W, Yang Q, Xue GR, Yu Y (2007) Boosting for transfer learning. In: Proceedings of the 24th international c" /><span class="c-article-references__counter">21.</span><p class="c-article-references__text" id="ref-CR21">Dai W, Yang Q, Xue GR, Yu Y (2007) Boosting for transfer learning. In: Proceedings of the 24th international conference on machine learning. p. 193–200.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Daumé H III. Frustratingly easy domain adaptation. In: Proceedings of ACL. 2007. p. 256–63." /><span class="c-article-references__counter">22.</span><p class="c-article-references__text" id="ref-CR22">Daumé H III. Frustratingly easy domain adaptation. In: Proceedings of ACL. 2007. p. 256–63.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Davis J, Kulis B, Jain P, Sra S, Dhillon I. Information theoretic metric learning. In: Proceedings of the 24th" /><span class="c-article-references__counter">23.</span><p class="c-article-references__text" id="ref-CR23">Davis J, Kulis B, Jain P, Sra S, Dhillon I. Information theoretic metric learning. In: Proceedings of the 24th international conference on machine learning. 2007. p. 209–16.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="S. Deerwester, ST. Dumais, GW. Furnas, TK. Landauer, R. Harshman, " /><meta itemprop="datePublished" content="1990" /><meta itemprop="headline" content="Deerwester S, Dumais ST, Furnas GW, Landauer TK, Harshman R. Indexing by latent semantic analysis. J Am Soc In" /><span class="c-article-references__counter">24.</span><p class="c-article-references__text" id="ref-CR24">Deerwester S, Dumais ST, Furnas GW, Landauer TK, Harshman R. Indexing by latent semantic analysis. J Am Soc Inf Sci. 1990;41:391–407.</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1002%2F%28SICI%291097-4571%28199009%2941%3A6%3C391%3A%3AAID-ASI1%3E3.0.CO%3B2-9" aria-label="View reference 24">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 24 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Indexing%20by%20latent%20semantic%20analysis&amp;journal=J%20Am%20Soc%20Inf%20Sci&amp;volume=41&amp;pages=391-407&amp;publication_year=1990&amp;author=Deerwester%2CS&amp;author=Dumais%2CST&amp;author=Furnas%2CGW&amp;author=Landauer%2CTK&amp;author=Harshman%2CR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Deng J, Zhang Z, Marchi E, Schuller B. Sparse autoencoder based feature transfer learning for speech emotion r" /><span class="c-article-references__counter">25.</span><p class="c-article-references__text" id="ref-CR25">Deng J, Zhang Z, Marchi E, Schuller B. Sparse autoencoder based feature transfer learning for speech emotion recognition. In: Humaine association conference on affective computing and intelligent interaction. 2013. p. 511–6.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Domain adaptation project. https://www.eecs.berkeley.edu/~jhoffman/domainadapt/. Accessed 4 Mar 2016." /><span class="c-article-references__counter">26.</span><p class="c-article-references__text" id="ref-CR26">Domain adaptation project. <a href="https://www.eecs.berkeley.edu/%7ejhoffman/domainadapt/">https://www.eecs.berkeley.edu/~jhoffman/domainadapt/</a>. Accessed 4 Mar 2016.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="L. Duan, IW. Tsang, D. Xu, " /><meta itemprop="datePublished" content="2012" /><meta itemprop="headline" content="Duan L, Tsang IW, Xu D. Domain transfer multiple kernel learning. IEEE Trans Pattern Anal Mach Intell. 2012;34" /><span class="c-article-references__counter">27.</span><p class="c-article-references__text" id="ref-CR27">Duan L, Tsang IW, Xu D. Domain transfer multiple kernel learning. IEEE Trans Pattern Anal Mach Intell. 2012;34(3):465–79.</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FTPAMI.2011.114" aria-label="View reference 27">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 27 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Domain%20transfer%20multiple%20kernel%20learning&amp;journal=IEEE%20Trans%20Pattern%20Anal%20Mach%20Intell&amp;volume=34&amp;issue=3&amp;pages=465-479&amp;publication_year=2012&amp;author=Duan%2CL&amp;author=Tsang%2CIW&amp;author=Xu%2CD">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Duan L, Xu D, Chang SF. Exploiting web images for event recognition in consumer videos: a multiple source doma" /><span class="c-article-references__counter">28.</span><p class="c-article-references__text" id="ref-CR28">Duan L, Xu D, Chang SF. Exploiting web images for event recognition in consumer videos: a multiple source domain adaptation approach. In: IEEE 2012 conference on computer vision and pattern recognition. 2012. p. 1338–45.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="L. Duan, D. Xu, IW. Tsang, " /><meta itemprop="datePublished" content="2012" /><meta itemprop="headline" content="Duan L, Xu D, Tsang IW. Domain adaptation from multiple sources: a domain-dependent regularization approach. I" /><span class="c-article-references__counter">29.</span><p class="c-article-references__text" id="ref-CR29">Duan L, Xu D, Tsang IW. Domain adaptation from multiple sources: a domain-dependent regularization approach. IEEE Trans Neural Netw Learn Syst. 2012;23(3):504–18.</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FTNNLS.2011.2178556" aria-label="View reference 29">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 29 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Domain%20adaptation%20from%20multiple%20sources%3A%20a%20domain-dependent%20regularization%20approach&amp;journal=IEEE%20Trans%20Neural%20Netw%20Learn%20Syst&amp;volume=23&amp;issue=3&amp;pages=504-518&amp;publication_year=2012&amp;author=Duan%2CL&amp;author=Xu%2CD&amp;author=Tsang%2CIW">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="L. Duan, D. Xu, IW. Tsang, " /><meta itemprop="datePublished" content="2012" /><meta itemprop="headline" content="Duan L, Xu D, Tsang IW. Learning with augmented features for heterogeneous domain adaptation. IEEE Trans Patte" /><span class="c-article-references__counter">30.</span><p class="c-article-references__text" id="ref-CR30">Duan L, Xu D, Tsang IW. Learning with augmented features for heterogeneous domain adaptation. IEEE Trans Pattern Anal Mach Intell. 2012;36(6):1134–48.</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 30 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Learning%20with%20augmented%20features%20for%20heterogeneous%20domain%20adaptation&amp;journal=IEEE%20Trans%20Pattern%20Anal%20Mach%20Intell&amp;volume=36&amp;issue=6&amp;pages=1134-1148&amp;publication_year=2012&amp;author=Duan%2CL&amp;author=Xu%2CD&amp;author=Tsang%2CIW">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="E. Eaton, M. des Jardins, T. Lane, " /><meta itemprop="datePublished" content="2008" /><meta itemprop="headline" content="Eaton E, des Jardins M, Lane T. Modeling transfer relationships between learning tasks for improved inductive " /><span class="c-article-references__counter">31.</span><p class="c-article-references__text" id="ref-CR31">Eaton E, des Jardins M, Lane T. Modeling transfer relationships between learning tasks for improved inductive transfer. Proc Mach Learn Knowl Disc Database. 2008;5211:317–32.</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2F978-3-540-87479-9_39" aria-label="View reference 31">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 31 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Modeling%20transfer%20relationships%20between%20learning%20tasks%20for%20improved%20inductive%20transfer&amp;journal=Proc%20Mach%20Learn%20Knowl%20Disc%20Database&amp;volume=5211&amp;pages=317-332&amp;publication_year=2008&amp;author=Eaton%2CE&amp;author=des%20Jardins%2CM&amp;author=Lane%2CT">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="EasyAdapt.pl.gz (Download). http://hal3.name/easyadapt.pl.gz Accessed 4 Mar 2016." /><span class="c-article-references__counter">32.</span><p class="c-article-references__text" id="ref-CR32">EasyAdapt.pl.gz (Download). <a href="http://hal3.name/easyadapt.pl.gz">http://hal3.name/easyadapt.pl.gz</a> Accessed 4 Mar 2016.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Evgeniou T, Pontil M (2004) Regularized multi-task learning. In: Proceedings of the 10th ACM SIGKDD internatio" /><span class="c-article-references__counter">33.</span><p class="c-article-references__text" id="ref-CR33">Evgeniou T, Pontil M (2004) Regularized multi-task learning. In: Proceedings of the 10th ACM SIGKDD international conference on knowledge discovery and data mining. p. 109–17.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Exploiting web images for event recognition in consumer videos: a multiple source domain adaptation approach. " /><span class="c-article-references__counter">34.</span><p class="c-article-references__text" id="ref-CR34">Exploiting web images for event recognition in consumer videos: a multiple source domain adaptation approach. <a href="http://lxduan.info/papers/DuanCVPR2012_poster.pdf">http://lxduan.info/papers/DuanCVPR2012_poster.pdf</a>. Accessed 4 Mar 2016.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Farhadi A, Forsyth D, White R. Transfer learning in sign language. In: IEEE 2007 conference on computer vision" /><span class="c-article-references__counter">35.</span><p class="c-article-references__text" id="ref-CR35">Farhadi A, Forsyth D, White R. Transfer learning in sign language. In: IEEE 2007 conference on computer vision and pattern recognition. 2007. p. 1–8.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Feuz KD, Cook DJ. Transfer learning across feature-rich heterogeneous feature spaces via feature-space remappi" /><span class="c-article-references__counter">36.</span><p class="c-article-references__text" id="ref-CR36">Feuz KD, Cook DJ. Transfer learning across feature-rich heterogeneous feature spaces via feature-space remapping (FSR). J ACM Trans Intell Syst Technol. 2014;6(1):1–27 <b>(Article 3)</b>.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Gao J, Fan W, Jiang J, Han J (2008) Knowledge transfer via multiple model local structure mapping. In: Proceed" /><span class="c-article-references__counter">37.</span><p class="c-article-references__text" id="ref-CR37">Gao J, Fan W, Jiang J, Han J (2008) Knowledge transfer via multiple model local structure mapping. In: Proceedings of the 14th ACM SIGKDD international conference on knowledge discovery and data mining. p. 283–91.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="J. Gao, F. Liang, W. Fan, Y. Sun, J. Han, " /><meta itemprop="datePublished" content="2009" /><meta itemprop="headline" content="Gao J, Liang F, Fan W, Sun Y, Han J. Graph based consensus maximization among multiple supervised and unsuperv" /><span class="c-article-references__counter">38.</span><p class="c-article-references__text" id="ref-CR38">Gao J, Liang F, Fan W, Sun Y, Han J. Graph based consensus maximization among multiple supervised and unsupervised models. Adv Neural Inf Process Syst. 2009;22:1–9.</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 38 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Graph%20based%20consensus%20maximization%20among%20multiple%20supervised%20and%20unsupervised%20models&amp;journal=Adv%20Neural%20Inf%20Process%20Syst&amp;volume=22&amp;pages=1-9&amp;publication_year=2009&amp;author=Gao%2CJ&amp;author=Liang%2CF&amp;author=Fan%2CW&amp;author=Sun%2CY&amp;author=Han%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="K. Gao, TM. Khoshgoftaar, H. Wang, N. Seliya, " /><meta itemprop="datePublished" content="2011" /><meta itemprop="headline" content="Gao K, Khoshgoftaar TM, Wang H, Seliya N. Choosing software metrics for defect prediction: an investigation on" /><span class="c-article-references__counter">39.</span><p class="c-article-references__text" id="ref-CR39">Gao K, Khoshgoftaar TM, Wang H, Seliya N. Choosing software metrics for defect prediction: an investigation on feature selection techniques. J Softw Pract Exp. 2011;41(5):579–606.</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1002%2Fspe.1043" aria-label="View reference 39">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 39 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Choosing%20software%20metrics%20for%20defect%20prediction%3A%20an%20investigation%20on%20feature%20selection%20techniques&amp;journal=J%20Softw%20Pract%20Exp&amp;volume=41&amp;issue=5&amp;pages=579-606&amp;publication_year=2011&amp;author=Gao%2CK&amp;author=Khoshgoftaar%2CTM&amp;author=Wang%2CH&amp;author=Seliya%2CN">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ge L, Gao J, Ngo H, Li K, Zhang A. On handling negative transfer and imbalanced distributions in multiple sour" /><span class="c-article-references__counter">40.</span><p class="c-article-references__text" id="ref-CR40">Ge L, Gao J, Ngo H, Li K, Zhang A. On handling negative transfer and imbalanced distributions in multiple source transfer learning. In: Proceedings of the 2013 SIAM international conference on data mining. 2013. p. 254–71.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Glorot X, Bordes A, Bengio Y. Domain adaptation for large-scale sentiment classification: A deep learning appr" /><span class="c-article-references__counter">41.</span><p class="c-article-references__text" id="ref-CR41">Glorot X, Bordes A, Bengio Y. Domain adaptation for large-scale sentiment classification: A deep learning approach. In: Proceedings of the twenty-eight international conference on machine learning, vol. 27. 2011. p. 97–110.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Gong B, Shi Y, Sha F, Grauman K. Geodesic flow kernel for unsupervised domain adaptation. In: Proceedings of t" /><span class="c-article-references__counter">42.</span><p class="c-article-references__text" id="ref-CR42">Gong B, Shi Y, Sha F, Grauman K. Geodesic flow kernel for unsupervised domain adaptation. In: Proceedings of the 2012 IEEE conference on computer vision and pattern recognition. 2012. p. 2066–73.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Gopalan R, Li R, Chellappa R. Domain adaptation for object recognition: an unsupervised approach. In: 2011 int" /><span class="c-article-references__counter">43.</span><p class="c-article-references__text" id="ref-CR43">Gopalan R, Li R, Chellappa R. Domain adaptation for object recognition: an unsupervised approach. In: 2011 international conference on computer vision. 2011. p. 999–1006.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Guo-Jun Qi’s publication list. http://www.eecs.ucf.edu/~gqi/publications.html. Accessed 4 Mar 2016." /><span class="c-article-references__counter">44.</span><p class="c-article-references__text" id="ref-CR44">Guo-Jun Qi’s publication list. <a href="http://www.eecs.ucf.edu/%7egqi/publications.html">http://www.eecs.ucf.edu/~gqi/publications.html</a>. Accessed 4 Mar 2016.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ham JH, Lee DD, Saul LK. Learning high dimensional correspondences from low dimensional manifolds. In: Proceed" /><span class="c-article-references__counter">45.</span><p class="c-article-references__text" id="ref-CR45">Ham JH, Lee DD, Saul LK. Learning high dimensional correspondences from low dimensional manifolds. In: Proceedings of the twentieth international conference on machine learning. 2003. p. 1–8.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Harel M, Mannor S. Learning from multiple outlooks. In: Proceedings of the 28th international conference on ma" /><span class="c-article-references__counter">46.</span><p class="c-article-references__text" id="ref-CR46">Harel M, Mannor S. Learning from multiple outlooks. In: Proceedings of the 28th international conference on machine learning. 2011. p. 401–8.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="He P, Li B, Ma Y (2014) Towards cross-project defect prediction with imbalanced feature sets. http://arxiv.org" /><span class="c-article-references__counter">47.</span><p class="c-article-references__text" id="ref-CR47">He P, Li B, Ma Y (2014) Towards cross-project defect prediction with imbalanced feature sets. <a href="http://arxiv.org/abs/1411.4228">http://arxiv.org/abs/1411.4228</a>.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Heterogeneous defect prediction. http://www.slideshare.net/hunkim/heterogeneous-defect-prediction-esecfse-2015" /><span class="c-article-references__counter">48.</span><p class="c-article-references__text" id="ref-CR48">Heterogeneous defect prediction. <a href="http://www.slideshare.net/hunkim/heterogeneous-defect-prediction-esecfse-2015">http://www.slideshare.net/hunkim/heterogeneous-defect-prediction-esecfse-2015</a>. Accessed 4 Mar 2016.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="HFA_release_0315.rar (Download). https://sites.google.com/site/xyzliwen/publications/HFA_release_0315.rar. Acc" /><span class="c-article-references__counter">49.</span><p class="c-article-references__text" id="ref-CR49">HFA_release_0315.rar (Download). <a href="https://sites.google.com/site/xyzliwen/publications/HFA_release_0315.rar">https://sites.google.com/site/xyzliwen/publications/HFA_release_0315.rar</a>. Accessed 4 Mar 2016.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Hu M, Liu B. Mining and summarizing customer reviews. In: Proceedings of the 10th ACM SIGKDD international con" /><span class="c-article-references__counter">50.</span><p class="c-article-references__text" id="ref-CR50">Hu M, Liu B. Mining and summarizing customer reviews. In: Proceedings of the 10th ACM SIGKDD international conference on Knowledge discovery and data mining. 2004. p. 168–77.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Huang J, Smola A, Gretton A, Borgwardt KM, Schölkopf B. Correcting sample selection bias by unlabeled data. In" /><span class="c-article-references__counter">51.</span><p class="c-article-references__text" id="ref-CR51">Huang J, Smola A, Gretton A, Borgwardt KM, Schölkopf B. Correcting sample selection bias by unlabeled data. In: Proceedings of the 2006 conference. Adv Neural Inf Process Syst. 2006. p. 601–8.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Jakob N, Gurevych I. Extracting opinion targets in a single and cross-domain setting with conditional random f" /><span class="c-article-references__counter">52.</span><p class="c-article-references__text" id="ref-CR52">Jakob N, Gurevych I. Extracting opinion targets in a single and cross-domain setting with conditional random fields. In: Proceedings of the 2010 conference on empirical methods in NLP. 2010. p. 1035–45.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Jiang J, Zhai C. Instance weighting for domain adaptation in NLP. In: Proceedings of the 45th annual meeting o" /><span class="c-article-references__counter">53.</span><p class="c-article-references__text" id="ref-CR53">Jiang J, Zhai C. Instance weighting for domain adaptation in NLP. In: Proceedings of the 45th annual meeting of the association of computational linguistics. 2007. p. 264–71.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Jiang M, Cui P, Wang F, Yang Q, Zhu W, Yang S. Social recommendation across multiple relational domains. In: P" /><span class="c-article-references__counter">54.</span><p class="c-article-references__text" id="ref-CR54">Jiang M, Cui P, Wang F, Yang Q, Zhu W, Yang S. Social recommendation across multiple relational domains. In: Proceedings of the 21st ACM international conference on information and knowledge management. 2012. p. 1422–31.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Jiang W, Zavesky E, Chang SF, Loui A. Cross-domain learning methods for high-level visual concept classificati" /><span class="c-article-references__counter">55.</span><p class="c-article-references__text" id="ref-CR55">Jiang W, Zavesky E, Chang SF, Loui A. Cross-domain learning methods for high-level visual concept classification. In: IEEE 2008 15th international conference on image processing. 2008. p. 161–4.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="M. Kan, J. Wu, S. Shan, X. Chen, " /><meta itemprop="datePublished" content="2014" /><meta itemprop="headline" content="Kan M, Wu J, Shan S, Chen X. Domain adaptation for face recognition: targetize source domain bridged by common" /><span class="c-article-references__counter">56.</span><p class="c-article-references__text" id="ref-CR56">Kan M, Wu J, Shan S, Chen X. Domain adaptation for face recognition: targetize source domain bridged by common subspace. Int J Comput Vis. 2014;109(1–2):94–109.</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs11263-013-0693-1" aria-label="View reference 56">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.emis.de/MATH-item?1328.68244" aria-label="View reference 56 on MATH">MATH</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 56 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Domain%20adaptation%20for%20face%20recognition%3A%20targetize%20source%20domain%20bridged%20by%20common%20subspace&amp;journal=Int%20J%20Comput%20Vis&amp;volume=109&amp;issue=1%E2%80%932&amp;pages=94-109&amp;publication_year=2014&amp;author=Kan%2CM&amp;author=Wu%2CJ&amp;author=Shan%2CS&amp;author=Chen%2CX">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="M. Kloft, U. Brefeld, S. Sonnenburg, A. Zien, " /><meta itemprop="datePublished" content="2011" /><meta itemprop="headline" content="Kloft M, Brefeld U, Sonnenburg S, Zien A. Lp-norm multiple kernel learning. J Mach Learn Res. 2011;12:953–97." /><span class="c-article-references__counter">57.</span><p class="c-article-references__text" id="ref-CR57">Kloft M, Brefeld U, Sonnenburg S, Zien A. Lp-norm multiple kernel learning. J Mach Learn Res. 2011;12:953–97.</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.ams.org/mathscinet-getitem?mr=2786915" aria-label="View reference 57 on MathSciNet">MathSciNet</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.emis.de/MATH-item?1280.68173" aria-label="View reference 57 on MATH">MATH</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 57 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Lp-norm%20multiple%20kernel%20learning&amp;journal=J%20Mach%20Learn%20Res&amp;volume=12&amp;pages=953-997&amp;publication_year=2011&amp;author=Kloft%2CM&amp;author=Brefeld%2CU&amp;author=Sonnenburg%2CS&amp;author=Zien%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kulis B, Saenko K, Darrell T. What you saw is not what you get: domain adaptation using asymmetric kernel tran" /><span class="c-article-references__counter">58.</span><p class="c-article-references__text" id="ref-CR58">Kulis B, Saenko K, Darrell T. What you saw is not what you get: domain adaptation using asymmetric kernel transforms. In: IEEE 2011 conference on computer vision and pattern recognition. 2011. p. 1785–92.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="LeCun Y, Bottou L, HuangFu J. Learning methods for generic object recognition with invariance to pose and ligh" /><span class="c-article-references__counter">59.</span><p class="c-article-references__text" id="ref-CR59">LeCun Y, Bottou L, HuangFu J. Learning methods for generic object recognition with invariance to pose and lighting. In: Proceedings of the 2004 IEEE computer society conference on computer vision and pattern recognition, vol. 2. 2004. p. 97–104.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Li B, Yang Q, Xue X. Can movies and books collaborate? Cross-domain collaborative filtering for sparsity reduc" /><span class="c-article-references__counter">60.</span><p class="c-article-references__text" id="ref-CR60">Li B, Yang Q, Xue X. Can movies and books collaborate? Cross-domain collaborative filtering for sparsity reduction. In: Proceedings of the 21st international joint conference on artificial intelligence. 2009. p. 2052–57.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Li B, Yang Q, Xue X. Transfer learning for collaborative filtering via a rating-matrix generative model. In: P" /><span class="c-article-references__counter">61.</span><p class="c-article-references__text" id="ref-CR61">Li B, Yang Q, Xue X. Transfer learning for collaborative filtering via a rating-matrix generative model. In: Proceedings of the 26th annual international conference on machine learning. 2009. p. 617–24.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Li F, Pan SJ, Jin O, Yang Q, Zhu X. Cross-domain co-extraction of sentiment and topic lexicons. In: Proceeding" /><span class="c-article-references__counter">62.</span><p class="c-article-references__text" id="ref-CR62">Li F, Pan SJ, Jin O, Yang Q, Zhu X. Cross-domain co-extraction of sentiment and topic lexicons. In: Proceedings of the 50th annual meeting of the association for computational linguistics long papers, vol. 1. 2012. p. 410–19.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Li S, Zong C. Multi-domain adaptation for sentiment classification: Using multiple classifier combining method" /><span class="c-article-references__counter">63.</span><p class="c-article-references__text" id="ref-CR63">Li S, Zong C. Multi-domain adaptation for sentiment classification: Using multiple classifier combining methods. In: Proceedings of the conference on natural language processing and knowledge engineering. 2008. p. 1–8.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="W. Li, L. Duan, D. Xu, IW. Tsang, " /><meta itemprop="datePublished" content="2014" /><meta itemprop="headline" content="Li W, Duan L, Xu D, Tsang IW. Learning with augmented features for supervised and semi-supervised heterogeneou" /><span class="c-article-references__counter">64.</span><p class="c-article-references__text" id="ref-CR64">Li W, Duan L, Xu D, Tsang IW. Learning with augmented features for supervised and semi-supervised heterogeneous domain adaptation. IEEE Trans Pattern Anal Mach Intell. 2014;36(6):1134–48.</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FTPAMI.2013.167" aria-label="View reference 64">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 64 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Learning%20with%20augmented%20features%20for%20supervised%20and%20semi-supervised%20heterogeneous%20domain%20adaptation&amp;journal=IEEE%20Trans%20Pattern%20Anal%20Mach%20Intell&amp;volume=36&amp;issue=6&amp;pages=1134-1148&amp;publication_year=2014&amp;author=Li%2CW&amp;author=Duan%2CL&amp;author=Xu%2CD&amp;author=Tsang%2CIW">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="LIBSVM (2016) A library for support vector machines. http://www.csie.ntu.edu.tw/~cjlin/libsvm. Accessed 4 Mar " /><span class="c-article-references__counter">65.</span><p class="c-article-references__text" id="ref-CR65">LIBSVM (2016) A library for support vector machines. <a href="http://www.csie.ntu.edu.tw/%7ecjlin/libsvm">http://www.csie.ntu.edu.tw/~cjlin/libsvm</a>. Accessed 4 Mar 2016.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ling X, Dai W, Xue GR, Yang Q, Yu Y. Spectral domain-transfer learning. In: Proceedings of the 14th ACM SIGKDD" /><span class="c-article-references__counter">66.</span><p class="c-article-references__text" id="ref-CR66">Ling X, Dai W, Xue GR, Yang Q, Yu Y. Spectral domain-transfer learning. In: Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining. 2008. p. 488–96.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Lixin Duan. http://www.lxduan.info/#sourcecode_hfa. Accessed 4 Mar 2016." /><span class="c-article-references__counter">67.</span><p class="c-article-references__text" id="ref-CR67">Lixin Duan. <a href="http://www.lxduan.info/%23sourcecode_hfa">http://www.lxduan.info/#sourcecode_hfa</a>. Accessed 4 Mar 2016.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="M. Long, J. Wang, G. Ding, SJ. Pan, PS. Yu, " /><meta itemprop="datePublished" content="2014" /><meta itemprop="headline" content="Long M, Wang J, Ding G, Pan SJ, Yu PS. Adaptation regularization: a general framework for transfer learning. I" /><span class="c-article-references__counter">68.</span><p class="c-article-references__text" id="ref-CR68">Long M, Wang J, Ding G, Pan SJ, Yu PS. Adaptation regularization: a general framework for transfer learning. IEEE Trans Knowl Data Eng. 2014;26(5):1076–89.</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FTKDE.2013.111" aria-label="View reference 68">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 68 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Adaptation%20regularization%3A%20a%20general%20framework%20for%20transfer%20learning&amp;journal=IEEE%20Trans%20Knowl%20Data%20Eng&amp;volume=26&amp;issue=5&amp;pages=1076-1089&amp;publication_year=2014&amp;author=Long%2CM&amp;author=Wang%2CJ&amp;author=Ding%2CG&amp;author=Pan%2CSJ&amp;author=Yu%2CPS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Long M, Wang J, Ding G, Sun J, Yu PS. Transfer feature learning with joint distribution adaptation. In: Procee" /><span class="c-article-references__counter">69.</span><p class="c-article-references__text" id="ref-CR69">Long M, Wang J, Ding G, Sun J, Yu PS. Transfer feature learning with joint distribution adaptation. In: Proceedings of the 2013 IEEE international conference on computer vision. 2013. p. 2200–07.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="DG. Lowe, " /><meta itemprop="datePublished" content="2004" /><meta itemprop="headline" content="Lowe DG. Distinctive image features from scale-invariant keypoints. Int Comput Vis. 2004;60(2):91–110." /><span class="c-article-references__counter">70.</span><p class="c-article-references__text" id="ref-CR70">Lowe DG. Distinctive image features from scale-invariant keypoints. Int Comput Vis. 2004;60(2):91–110.</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1023%2FB%3AVISI.0000029664.99615.94" aria-label="View reference 70">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 70 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Distinctive%20image%20features%20from%20scale-invariant%20keypoints&amp;journal=Int%20Comput%20Vis&amp;volume=60&amp;issue=2&amp;pages=91-110&amp;publication_year=2004&amp;author=Lowe%2CDG">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Luo P, Zhuang F, Xiong H, Xiong Y, He Q. Transfer learning from multiple source domains via consensus regulari" /><span class="c-article-references__counter">71.</span><p class="c-article-references__text" id="ref-CR71">Luo P, Zhuang F, Xiong H, Xiong Y, He Q. Transfer learning from multiple source domains via consensus regularization. In: Proceedings of the 17th ACM conference on information and knowledge management. 2008. p. 103–12.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="Y. Ma, W. Gong, F. Mao, " /><meta itemprop="datePublished" content="2015" /><meta itemprop="headline" content="Ma Y, Gong W, Mao F. Transfer learning used to analyze the dynamic evolution of the dust aerosol. J Quant Spec" /><span class="c-article-references__counter">72.</span><p class="c-article-references__text" id="ref-CR72">Ma Y, Gong W, Mao F. Transfer learning used to analyze the dynamic evolution of the dust aerosol. J Quant Spectrosc Radiat Transf. 2015;153:119–30.</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.jqsrt.2014.09.025" aria-label="View reference 72">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 72 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Transfer%20learning%20used%20to%20analyze%20the%20dynamic%20evolution%20of%20the%20dust%20aerosol&amp;journal=J%20Quant%20Spectrosc%20Radiat%20Transf&amp;volume=153&amp;pages=119-130&amp;publication_year=2015&amp;author=Ma%2CY&amp;author=Gong%2CW&amp;author=Mao%2CF">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Marszalek M, Schmid C, Harzallah H, Van de Weijer J. Learning object representations for visual object class r" /><span class="c-article-references__counter">73.</span><p class="c-article-references__text" id="ref-CR73">Marszalek M, Schmid C, Harzallah H, Van de Weijer J. Learning object representations for visual object class recognition. In: Visual recognition challenge workshop ICCV. 2007. p. 1–10.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Mihalkova L, Mooney RJ. Transfer learning by mapping with minimal target data. In: Proc. assoc. for the advanc" /><span class="c-article-references__counter">74.</span><p class="c-article-references__text" id="ref-CR74">Mihalkova L, Mooney RJ. Transfer learning by mapping with minimal target data. In: Proc. assoc. for the advancement of artificial intelligence workshop transfer learning for complex tasks. 2008. p. 31–6.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Long M. http://ise.thss.tsinghua.edu.cn/~mlong/. Accessed 4 Mar 2016." /><span class="c-article-references__counter">75.</span><p class="c-article-references__text" id="ref-CR75">Long M. <a href="http://ise.thss.tsinghua.edu.cn/%7emlong/">http://ise.thss.tsinghua.edu.cn/~mlong/</a>. Accessed 4 Mar 2016.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Moreno O, Shapira B, Rokach L, Shani G (2012) TALMUD—transfer learning for multiple domains. In: Proceedings o" /><span class="c-article-references__counter">76.</span><p class="c-article-references__text" id="ref-CR76">Moreno O, Shapira B, Rokach L, Shani G (2012) TALMUD—transfer learning for multiple domains. In: Proceedings of the 21st ACM international conference on information and knowledge management. 2012. p. 425–34.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Nam J, Kim S (2015) Heterogeneous defect prediction. In: Proceedings of the 2015 10th joint meeting on foundat" /><span class="c-article-references__counter">77.</span><p class="c-article-references__text" id="ref-CR77">Nam J, Kim S (2015) Heterogeneous defect prediction. In: Proceedings of the 2015 10th joint meeting on foundations of software engineering. 2015. p. 508–19.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ng MK, Wu Q, Ye Y. Co-transfer learning via joint transition probability graph based method. In: Proceedings o" /><span class="c-article-references__counter">78.</span><p class="c-article-references__text" id="ref-CR78">Ng MK, Wu Q, Ye Y. Co-transfer learning via joint transition probability graph based method. In: Proceedings of the 1st international workshop on cross domain knowledge discovery in web and social network mining. 2012. p. 1–9.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ngiam J, Khosla A, Kim M, Nam J, Lee H, Ng AY. Multimodal deep learning. In: The 28th international conference" /><span class="c-article-references__counter">79.</span><p class="c-article-references__text" id="ref-CR79">Ngiam J, Khosla A, Kim M, Nam J, Lee H, Ng AY. Multimodal deep learning. In: The 28th international conference on machine learning. 2011. p. 689–96.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ogoe HA, Visweswaran S, Lu X, Gopalakrishnan V. Knowledge transfer via classification rules using functional m" /><span class="c-article-references__counter">80.</span><p class="c-article-references__text" id="ref-CR80">Ogoe HA, Visweswaran S, Lu X, Gopalakrishnan V. Knowledge transfer via classification rules using functional mapping for integrative modeling of gene expression data. BMC Bioinform. 2015. p. 1–15.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Oquab M, Bottou L, Laptev I, Sivic J. Learning and transferring mid-level image representations using convolut" /><span class="c-article-references__counter">81.</span><p class="c-article-references__text" id="ref-CR81">Oquab M, Bottou L, Laptev I, Sivic J. Learning and transferring mid-level image representations using convolutional neural networks. In: Proceedings of the 2014 IEEE conference on computer vision and pattern recognition. 2013. p. 1717–24.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Pan SJ, Kwok JT, Yang Q. Transfer learning via dimensionality reduction. In: Proceedings of the 23rd national " /><span class="c-article-references__counter">82.</span><p class="c-article-references__text" id="ref-CR82">Pan SJ, Kwok JT, Yang Q. Transfer learning via dimensionality reduction. In: Proceedings of the 23rd national conference on artificial intelligence, vol. 2. 2008. p. 677–82.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Pan SJ, Ni X, Sun JT, Yang Q, Chen Z. Cross-domain sentiment classification via spectral feature alignment. In" /><span class="c-article-references__counter">83.</span><p class="c-article-references__text" id="ref-CR83">Pan SJ, Ni X, Sun JT, Yang Q, Chen Z. Cross-domain sentiment classification via spectral feature alignment. In: Proceedings of the 19th international conference on world wide web. 2010. p. 751–60.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="SJ. Pan, Q. Yang, " /><meta itemprop="datePublished" content="2010" /><meta itemprop="headline" content="Pan SJ, Yang Q. A survey on transfer learning. IEEE Trans Knowl Data Eng. 2010;22(10):1345–59." /><span class="c-article-references__counter">84.</span><p class="c-article-references__text" id="ref-CR84">Pan SJ, Yang Q. A survey on transfer learning. IEEE Trans Knowl Data Eng. 2010;22(10):1345–59.</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FTKDE.2009.191" aria-label="View reference 84">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 84 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20survey%20on%20transfer%20learning&amp;journal=IEEE%20Trans%20Knowl%20Data%20Eng&amp;volume=22&amp;issue=10&amp;pages=1345-1359&amp;publication_year=2010&amp;author=Pan%2CSJ&amp;author=Yang%2CQ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Pan W, Liu NN, Xiang EW, Yang Q. Transfer learning to predict missing ratings via heterogeneous user feedbacks" /><span class="c-article-references__counter">85.</span><p class="c-article-references__text" id="ref-CR85">Pan W, Liu NN, Xiang EW, Yang Q. Transfer learning to predict missing ratings via heterogeneous user feedbacks. In: Proceedings of the 22nd international joint conference on artificial intelligence. 2011. p. 2318–23.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Pan W. Xiang EW, Liu NN, Yang Q. Transfer learning in collaborative filtering for sparsity reduction. In: Twen" /><span class="c-article-references__counter">86.</span><p class="c-article-references__text" id="ref-CR86">Pan W. Xiang EW, Liu NN, Yang Q. Transfer learning in collaborative filtering for sparsity reduction. In: Twenty-fourth AAAI conference on artificial intelligence, vol. 1. 2010. p. 230–235.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="SJ. Pan, IW. Tsang, JT. Kwok, Q. Yang, " /><meta itemprop="datePublished" content="2009" /><meta itemprop="headline" content="Pan SJ, Tsang IW, Kwok JT, Yang Q. Domain adaptation via transfer component analysis. IEEE Trans Neural Netw. " /><span class="c-article-references__counter">87.</span><p class="c-article-references__text" id="ref-CR87">Pan SJ, Tsang IW, Kwok JT, Yang Q. Domain adaptation via transfer component analysis. IEEE Trans Neural Netw. 2009;22(2):199–210.</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 87 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Domain%20adaptation%20via%20transfer%20component%20analysis&amp;journal=IEEE%20Trans%20Neural%20Netw&amp;volume=22&amp;issue=2&amp;pages=199-210&amp;publication_year=2009&amp;author=Pan%2CSJ&amp;author=Tsang%2CIW&amp;author=Kwok%2CJT&amp;author=Yang%2CQ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Papers:oquab-2014. http://leon.bottou.org/papers/oquab-2014. Accessed 4 Mar 2016." /><span class="c-article-references__counter">88.</span><p class="c-article-references__text" id="ref-CR88">Papers:oquab-2014. <a href="http://leon.bottou.org/papers/oquab-2014">http://leon.bottou.org/papers/oquab-2014</a>. Accessed 4 Mar 2016.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="VM. Patel, R. Gopalan, R. Li, R. Chellappa, " /><meta itemprop="datePublished" content="2014" /><meta itemprop="headline" content="Patel VM, Gopalan R, Li R, Chellappa R. Visual domain adaptation: a survey of recent advances. IEEE Signal Pro" /><span class="c-article-references__counter">89.</span><p class="c-article-references__text" id="ref-CR89">Patel VM, Gopalan R, Li R, Chellappa R. Visual domain adaptation: a survey of recent advances. IEEE Signal Process Mag. 2014;32(3):53–69.</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FMSP.2014.2347059" aria-label="View reference 89">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 89 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Visual%20domain%20adaptation%3A%20a%20survey%20of%20recent%20advances&amp;journal=IEEE%20Signal%20Process%20Mag&amp;volume=32&amp;issue=3&amp;pages=53-69&amp;publication_year=2014&amp;author=Patel%2CVM&amp;author=Gopalan%2CR&amp;author=Li%2CR&amp;author=Chellappa%2CR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="C. Perlich, B. Dalessandro, T. Raeder, O. Stitelman, F. Provost, " /><meta itemprop="datePublished" content="2014" /><meta itemprop="headline" content="Perlich C, Dalessandro B, Raeder T, Stitelman O, Provost F. Machine learning for targeted display advertising:" /><span class="c-article-references__counter">90.</span><p class="c-article-references__text" id="ref-CR90">Perlich C, Dalessandro B, Raeder T, Stitelman O, Provost F. Machine learning for targeted display advertising: transfer learning in action. Mach Learn. 2014;95:103–27.</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.ams.org/mathscinet-getitem?mr=3179981" aria-label="View reference 90 on MathSciNet">MathSciNet</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs10994-013-5375-2" aria-label="View reference 90">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 90 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Machine%20learning%20for%20targeted%20display%20advertising%3A%20transfer%20learning%20in%20action&amp;journal=Mach%20Learn&amp;volume=95&amp;pages=103-127&amp;publication_year=2014&amp;author=Perlich%2CC&amp;author=Dalessandro%2CB&amp;author=Raeder%2CT&amp;author=Stitelman%2CO&amp;author=Provost%2CF">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Prettenhofer P, Stein B. (2010) Cross-language text classification using structural correspondence learning. I" /><span class="c-article-references__counter">91.</span><p class="c-article-references__text" id="ref-CR91">Prettenhofer P, Stein B. (2010) Cross-language text classification using structural correspondence learning. In: Proceedings of the 48th annual meeting of the association for computational linguistics. 2010. p. 1118–27.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Qi GJ, Aggarwal C, Huang T. Towards semantic knowledge propagation from text corpus to Web images. In: Proceed" /><span class="c-article-references__counter">92.</span><p class="c-article-references__text" id="ref-CR92">Qi GJ, Aggarwal C, Huang T. Towards semantic knowledge propagation from text corpus to Web images. In: Proceedings of the 20th international conference on world wide web. p. 297–306.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Qiu G, Liu B, Bu J, Chen C. Expanding domain sentiment lexicon through double propagation. In: Proceedings of " /><span class="c-article-references__counter">93.</span><p class="c-article-references__text" id="ref-CR93">Qiu G, Liu B, Bu J, Chen C. Expanding domain sentiment lexicon through double propagation. In: Proceedings of the 21st international joint conference on artificial intelligence. p. 1199–204.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Quanz B, Huan J. Large margin transductive transfer learning. In: Proceedings of the 18th ACM conference on in" /><span class="c-article-references__counter">94.</span><p class="c-article-references__text" id="ref-CR94">Quanz B, Huan J. Large margin transductive transfer learning. In: Proceedings of the 18th ACM conference on information and knowledge management. 2009. p. 1327–36.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Raina R, Battle A, Lee H, Packer B, Ng AY. Self-taught learning: transfer learning from unlabeled data. In: Pr" /><span class="c-article-references__counter">95.</span><p class="c-article-references__text" id="ref-CR95">Raina R, Battle A, Lee H, Packer B, Ng AY. Self-taught learning: transfer learning from unlabeled data. In: Proceedings of the 24th international conference on machine learning. 2007. p. 759–66.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="AN. Rajagopal, R. Subramanian, E. Ricci, RL. Vieriu, O. Lanz, KR. Ramakrishnan, N. Sebe, " /><meta itemprop="datePublished" content="2014" /><meta itemprop="headline" content="Rajagopal AN, Subramanian R, Ricci E, Vieriu RL, Lanz O, Ramakrishnan KR, Sebe N. Exploring transfer learning " /><span class="c-article-references__counter">96.</span><p class="c-article-references__text" id="ref-CR96">Rajagopal AN, Subramanian R, Ricci E, Vieriu RL, Lanz O, Ramakrishnan KR, Sebe N. Exploring transfer learning approaches for head pose classification from multi-view surveillance images. Int J Comput Vis. 2014;109(1–2):146–67.</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs11263-013-0692-2" aria-label="View reference 96">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 96 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Exploring%20transfer%20learning%20approaches%20for%20head%20pose%20classification%20from%20multi-view%20surveillance%20images&amp;journal=Int%20J%20Comput%20Vis&amp;volume=109&amp;issue=1%E2%80%932&amp;pages=146-167&amp;publication_year=2014&amp;author=Rajagopal%2CAN&amp;author=Subramanian%2CR&amp;author=Ricci%2CE&amp;author=Vieriu%2CRL&amp;author=Lanz%2CO&amp;author=Ramakrishnan%2CKR&amp;author=Sebe%2CN">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Romera-Paredes B, Aung MSH, Pontil M, Bianchi-Berthouze N, Williams AC de C, Watson P. Transfer learning to ac" /><span class="c-article-references__counter">97.</span><p class="c-article-references__text" id="ref-CR97">Romera-Paredes B, Aung MSH, Pontil M, Bianchi-Berthouze N, Williams AC de C, Watson P. Transfer learning to account for idiosyncrasy in face and body expressions. In: Proceedings of the 10th international conference on automatic face and gesture recognition (FG). 2013. p. 1–6.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Rosenstein MT, Marx Z, Kaelbling LP, Dietterich TG. To transfer or not to transfer. In: Proceedings NIPS’05 wo" /><span class="c-article-references__counter">98.</span><p class="c-article-references__text" id="ref-CR98">Rosenstein MT, Marx Z, Kaelbling LP, Dietterich TG. To transfer or not to transfer. In: Proceedings NIPS’05 workshop, inductive transfer. 10 years later. 2005. p. 1–4.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Roy S.D., Mei T., Zeng W., Li S. Social transfer: cross-domain transfer learning from social streams for media" /><span class="c-article-references__counter">99.</span><p class="c-article-references__text" id="ref-CR99">Roy S.D., Mei T., Zeng W., Li S. Social transfer: cross-domain transfer learning from social streams for media applications. In: Proceedings of the 20th ACM international conference on multimedia. p. 649–58.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="K. Saenko, B. Kulis, M. Fritz, T. Darrell, " /><meta itemprop="datePublished" content="2010" /><meta itemprop="headline" content="Saenko K, Kulis B, Fritz M, Darrell T. Adapting visual category models to new domains. Comput Vision ECCV. 201" /><span class="c-article-references__counter">100.</span><p class="c-article-references__text" id="ref-CR100">Saenko K, Kulis B, Fritz M, Darrell T. Adapting visual category models to new domains. Comput Vision ECCV. 2010;6314:213–26.</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 100 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Adapting%20visual%20category%20models%20to%20new%20domains&amp;journal=Comput%20Vision%20ECCV&amp;volume=6314&amp;pages=213-226&amp;publication_year=2010&amp;author=Saenko%2CK&amp;author=Kulis%2CB&amp;author=Fritz%2CM&amp;author=Darrell%2CT">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="G. Schweikert, C. Widmer, B. Schölkopf, G. Rätsch, " /><meta itemprop="datePublished" content="2009" /><meta itemprop="headline" content="Schweikert G, Widmer C, Schölkopf B, Rätsch G. An empirical analysis of domain adaptation algorithms for genom" /><span class="c-article-references__counter">101.</span><p class="c-article-references__text" id="ref-CR101">Schweikert G, Widmer C, Schölkopf B, Rätsch G. An empirical analysis of domain adaptation algorithms for genomic sequence analysis. Adv Neural Inf Process Syst. 2009;21:1433–40.</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 101 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=An%20empirical%20analysis%20of%20domain%20adaptation%20algorithms%20for%20genomic%20sequence%20analysis&amp;journal=Adv%20Neural%20Inf%20Process%20Syst&amp;volume=21&amp;pages=1433-1440&amp;publication_year=2009&amp;author=Schweikert%2CG&amp;author=Widmer%2CC&amp;author=Sch%C3%B6lkopf%2CB&amp;author=R%C3%A4tsch%2CG">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="CW. Seah, YS. Ong, IW. Tsang, " /><meta itemprop="datePublished" content="2013" /><meta itemprop="headline" content="Seah CW, Ong YS, Tsang IW. Combating negative transfer from predictive distribution differences. IEEE Trans Cy" /><span class="c-article-references__counter">102.</span><p class="c-article-references__text" id="ref-CR102">Seah CW, Ong YS, Tsang IW. Combating negative transfer from predictive distribution differences. IEEE Trans Cybern. 2013;43(4):1153–65.</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FTSMCB.2012.2225102" aria-label="View reference 102">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 102 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Combating%20negative%20transfer%20from%20predictive%20distribution%20differences&amp;journal=IEEE%20Trans%20Cybern&amp;volume=43&amp;issue=4&amp;pages=1153-1165&amp;publication_year=2013&amp;author=Seah%2CCW&amp;author=Ong%2CYS&amp;author=Tsang%2CIW">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="L. Shao, F. Zhu, X. Li, " /><meta itemprop="datePublished" content="2014" /><meta itemprop="headline" content="Shao L, Zhu F, Li X. Transfer learning for visual categorization: a survey. IEEE Trans Neural Netw Learn Syst." /><span class="c-article-references__counter">103.</span><p class="c-article-references__text" id="ref-CR103">Shao L, Zhu F, Li X. Transfer learning for visual categorization: a survey. IEEE Trans Neural Netw Learn Syst. 2014;26(5):1019–34.</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.ams.org/mathscinet-getitem?mr=3454260" aria-label="View reference 103 on MathSciNet">MathSciNet</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FTNNLS.2014.2330900" aria-label="View reference 103">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 103 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Transfer%20learning%20for%20visual%20categorization%3A%20a%20survey&amp;journal=IEEE%20Trans%20Neural%20Netw%20Learn%20Syst&amp;volume=26&amp;issue=5&amp;pages=1019-1034&amp;publication_year=2014&amp;author=Shao%2CL&amp;author=Zhu%2CF&amp;author=Li%2CX">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Shawe-Taylor J, Cristianini N. Kernel methods for pattern analysis. Cambridge: Cambridge University Press; 200" /><span class="c-article-references__counter">104.</span><p class="c-article-references__text" id="ref-CR104">Shawe-Taylor J, Cristianini N. Kernel methods for pattern analysis. Cambridge: Cambridge University Press; 2004.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Shi X, Liu Q, Fan W, Yu PS, Zhu R. Transfer learning on heterogeneous feature spaces via spectral transformati" /><span class="c-article-references__counter">105.</span><p class="c-article-references__text" id="ref-CR105">Shi X, Liu Q, Fan W, Yu PS, Zhu R. Transfer learning on heterogeneous feature spaces via spectral transformation. In: 2010 IEEE international conference on data mining. 2010. p. 1049–1054.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Shi Y, Sha F. Information-theoretical learning of discriminative clusters for unsupervised domain adaptation. " /><span class="c-article-references__counter">106.</span><p class="c-article-references__text" id="ref-CR106">Shi Y, Sha F. Information-theoretical learning of discriminative clusters for unsupervised domain adaptation. In: Proceedings of the 29th international conference on machine learning. 2012. p. 1–8.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="H. Shimodaira, " /><meta itemprop="datePublished" content="2000" /><meta itemprop="headline" content="Shimodaira H. Improving predictive inference under covariate shift by weighting the log-likelihood function. J" /><span class="c-article-references__counter">107.</span><p class="c-article-references__text" id="ref-CR107">Shimodaira H. Improving predictive inference under covariate shift by weighting the log-likelihood function. J Stat Plan Inf. 2000;90(2):227–44.</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.ams.org/mathscinet-getitem?mr=1795598" aria-label="View reference 107 on MathSciNet">MathSciNet</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2FS0378-3758%2800%2900115-4" aria-label="View reference 107">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.emis.de/MATH-item?0958.62011" aria-label="View reference 107 on MATH">MATH</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 107 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Improving%20predictive%20inference%20under%20covariate%20shift%20by%20weighting%20the%20log-likelihood%20function&amp;journal=J%20Stat%20Plan%20Inf&amp;volume=90&amp;issue=2&amp;pages=227-244&amp;publication_year=2000&amp;author=Shimodaira%2CH">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="S. Shivaji, EJ. Whitehead, R. Akella, S. Kim, " /><meta itemprop="datePublished" content="2013" /><meta itemprop="headline" content="Shivaji S, Whitehead EJ, Akella R, Kim S. Reducing features to improve code change-based bug prediction. IEEE " /><span class="c-article-references__counter">108.</span><p class="c-article-references__text" id="ref-CR108">Shivaji S, Whitehead EJ, Akella R, Kim S. Reducing features to improve code change-based bug prediction. IEEE Trans Softw Eng. 2013;39(4):552–69.</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FTSE.2012.43" aria-label="View reference 108">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 108 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Reducing%20features%20to%20improve%20code%20change-based%20bug%20prediction&amp;journal=IEEE%20Trans%20Softw%20Eng&amp;volume=39&amp;issue=4&amp;pages=552-569&amp;publication_year=2013&amp;author=Shivaji%2CS&amp;author=Whitehead%2CEJ&amp;author=Akella%2CR&amp;author=Kim%2CS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="S. Si, D. Tao, B. Geng, " /><meta itemprop="datePublished" content="2010" /><meta itemprop="headline" content="Si S, Tao D, Geng B. Bregman divergence-based regularization for transfer subspace learning. IEEE Trans Knowl " /><span class="c-article-references__counter">109.</span><p class="c-article-references__text" id="ref-CR109">Si S, Tao D, Geng B. Bregman divergence-based regularization for transfer subspace learning. IEEE Trans Knowl Data Eng. 2010;22(7):929–42.</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FTKDE.2009.126" aria-label="View reference 109">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 109 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Bregman%20divergence-based%20regularization%20for%20transfer%20subspace%20learning&amp;journal=IEEE%20Trans%20Knowl%20Data%20Eng&amp;volume=22&amp;issue=7&amp;pages=929-942&amp;publication_year=2010&amp;author=Si%2CS&amp;author=Tao%2CD&amp;author=Geng%2CB">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="Z. Song, Q. Chen, Z. Huang, Y. Hua, S. Yan, " /><meta itemprop="datePublished" content="2011" /><meta itemprop="headline" content="Song Z, Chen Q, Huang Z, Hua Y, Yan S. Contextualizing object detection and classification. IEEE Trans Pattern" /><span class="c-article-references__counter">110.</span><p class="c-article-references__text" id="ref-CR110">Song Z, Chen Q, Huang Z, Hua Y, Yan S. Contextualizing object detection and classification. IEEE Trans Pattern Anal Mach Intell. 2011;37(1):13–27.</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 110 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Contextualizing%20object%20detection%20and%20classification&amp;journal=IEEE%20Trans%20Pattern%20Anal%20Mach%20Intell&amp;volume=37&amp;issue=1&amp;pages=13-27&amp;publication_year=2011&amp;author=Song%2CZ&amp;author=Chen%2CQ&amp;author=Huang%2CZ&amp;author=Hua%2CY&amp;author=Yan%2CS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="I. Steinwart, " /><meta itemprop="datePublished" content="2001" /><meta itemprop="headline" content="Steinwart I. On the influence of the kernel on the consistency of support vector machines. JMLR. 2001;2:67–93." /><span class="c-article-references__counter">111.</span><p class="c-article-references__text" id="ref-CR111">Steinwart I. On the influence of the kernel on the consistency of support vector machines. JMLR. 2001;2:67–93.</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.ams.org/mathscinet-getitem?mr=1883281" aria-label="View reference 111 on MathSciNet">MathSciNet</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.emis.de/MATH-item?1009.68143" aria-label="View reference 111 on MATH">MATH</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 111 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=On%20the%20influence%20of%20the%20kernel%20on%20the%20consistency%20of%20support%20vector%20machines&amp;journal=JMLR&amp;volume=2&amp;pages=67-93&amp;publication_year=2001&amp;author=Steinwart%2CI">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="ME. Taylor, P. Stone, " /><meta itemprop="datePublished" content="2009" /><meta itemprop="headline" content="Taylor ME, Stone P. Transfer learning for reinforcement learning domains: a survey. JMLR. 2009;10:1633–85." /><span class="c-article-references__counter">112.</span><p class="c-article-references__text" id="ref-CR112">Taylor ME, Stone P. Transfer learning for reinforcement learning domains: a survey. JMLR. 2009;10:1633–85.</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.ams.org/mathscinet-getitem?mr=2534874" aria-label="View reference 112 on MathSciNet">MathSciNet</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.emis.de/MATH-item?1235.68196" aria-label="View reference 112 on MATH">MATH</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 112 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Transfer%20learning%20for%20reinforcement%20learning%20domains%3A%20a%20survey&amp;journal=JMLR&amp;volume=10&amp;pages=1633-1685&amp;publication_year=2009&amp;author=Taylor%2CME&amp;author=Stone%2CP">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Tommasi T, Caputo B. The more you know, the less you learn: from knowledge transfer to one-shot learning of ob" /><span class="c-article-references__counter">113.</span><p class="c-article-references__text" id="ref-CR113">Tommasi T, Caputo B. The more you know, the less you learn: from knowledge transfer to one-shot learning of object categories. BMVC. 2009;1–11.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="T. Tommasi, F. Orabona, B. Caputo, " /><meta itemprop="datePublished" content="2010" /><meta itemprop="headline" content="Tommasi T, Orabona F, Caputo B. Safety in numbers: learning categories from few examples with multi model know" /><span class="c-article-references__counter">114.</span><p class="c-article-references__text" id="ref-CR114">Tommasi T, Orabona F, Caputo B. Safety in numbers: learning categories from few examples with multi model knowledge transfer. IEEE Conf Comput Vision Pattern Recog. 2010;2010:3081–8.</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 114 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Safety%20in%20numbers%3A%20learning%20categories%20from%20few%20examples%20with%20multi%20model%20knowledge%20transfer&amp;journal=IEEE%20Conf%20Comput%20Vision%20Pattern%20Recog&amp;volume=2010&amp;pages=3081-3088&amp;publication_year=2010&amp;author=Tommasi%2CT&amp;author=Orabona%2CF&amp;author=Caputo%2CB">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Transfer learning resources. http://www.cse.ust.hk/TL/. Accessed 4 Mar 2016." /><span class="c-article-references__counter">115.</span><p class="c-article-references__text" id="ref-CR115">Transfer learning resources. <a href="http://www.cse.ust.hk/TL/">http://www.cse.ust.hk/TL/</a>. Accessed 4 Mar 2016.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Tutorial on domain adaptation and transfer learning. http://tommasit.wix.com/datl14tutorial. Accessed 4 Mar 20" /><span class="c-article-references__counter">116.</span><p class="c-article-references__text" id="ref-CR116">Tutorial on domain adaptation and transfer learning. <a href="http://tommasit.wix.com/datl14tutorial">http://tommasit.wix.com/datl14tutorial</a>. Accessed 4 Mar 2016.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="V. Vapnik, " /><meta itemprop="datePublished" content="1992" /><meta itemprop="headline" content="Vapnik V. Principles of risk minimization for learning theory. Adv Neural Inf Process Syst. 1992;4:831–8." /><span class="c-article-references__counter">117.</span><p class="c-article-references__text" id="ref-CR117">Vapnik V. Principles of risk minimization for learning theory. Adv Neural Inf Process Syst. 1992;4:831–8.</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 117 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Principles%20of%20risk%20minimization%20for%20learning%20theory&amp;journal=Adv%20Neural%20Inf%20Process%20Syst&amp;volume=4&amp;pages=831-838&amp;publication_year=1992&amp;author=Vapnik%2CV">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Vedaldi A, Gulshan V, Varma M, Zisserman A. Multiple kernels for object detection. In: 2009 IEEE 12th internat" /><span class="c-article-references__counter">118.</span><p class="c-article-references__text" id="ref-CR118">Vedaldi A, Gulshan V, Varma M, Zisserman A. Multiple kernels for object detection. In: 2009 IEEE 12th international conference on computer vision. 2009. p. 606–13.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Vincent P, Larochelle H, Bengio Y, Manzagol PA. Extracting and composing robust features with denoising autoen" /><span class="c-article-references__counter">119.</span><p class="c-article-references__text" id="ref-CR119">Vincent P, Larochelle H, Bengio Y, Manzagol PA. Extracting and composing robust features with denoising autoencoders. In: Proceedings of the 25th international conference on machine learning. 2008. p. 1096–103.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Vinokourov, J. Shawe-Taylor, N. Cristianini, " /><meta itemprop="datePublished" content="2002" /><meta itemprop="headline" content="Vinokourov A, Shawe-Taylor J, Cristianini N. Inferring a semantic representation of text via crosslanguage cor" /><span class="c-article-references__counter">120.</span><p class="c-article-references__text" id="ref-CR120">Vinokourov A, Shawe-Taylor J, Cristianini N. Inferring a semantic representation of text via crosslanguage correlation analysis. Adv Neural Inf Proces Syst. 2002;15:1473–80.</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 120 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Inferring%20a%20semantic%20representation%20of%20text%20via%20crosslanguage%20correlation%20analysis&amp;journal=Adv%20Neural%20Inf%20Proces%20Syst&amp;volume=15&amp;pages=1473-1480&amp;publication_year=2002&amp;author=Vinokourov%2CA&amp;author=Shawe-Taylor%2CJ&amp;author=Cristianini%2CN">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Wang C, Mahadevan S. Heterogeneous domain adaptation using manifold alignment. In: Proceedings of the 22nd int" /><span class="c-article-references__counter">121.</span><p class="c-article-references__text" id="ref-CR121">Wang C, Mahadevan S. Heterogeneous domain adaptation using manifold alignment. In: Proceedings of the 22nd international joint conference on artificial intelligence, vol. 2. 2011. p. 541–46.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Wang G, Hoiem D, Forsyth DA. Building text Features for object image classification. In: 2009 IEEE conference " /><span class="c-article-references__counter">122.</span><p class="c-article-references__text" id="ref-CR122">Wang G, Hoiem D, Forsyth DA. Building text Features for object image classification. In: 2009 IEEE conference on computer vision and pattern recognition. 2009. p. 1367–74.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Wang H, Klaser A, Schmid C, Liu CL (2011) Action recognition by dense trajectories. In: IEEE 2011 conference o" /><span class="c-article-references__counter">123.</span><p class="c-article-references__text" id="ref-CR123">Wang H, Klaser A, Schmid C, Liu CL (2011) Action recognition by dense trajectories. In: IEEE 2011 conference on computer vision and pattern recognition. 2011. p. 3169–76.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Wei B, Pal C (2010) Cross-lingual adaptation: an experiment on sentiment classifications. In: Proceedings of t" /><span class="c-article-references__counter">124.</span><p class="c-article-references__text" id="ref-CR124">Wei B, Pal C (2010) Cross-lingual adaptation: an experiment on sentiment classifications. In: Proceedings of the ACL 2010 conference short papers. 2010. p. 258–62.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Wei B, Pal C (2011) Heterogeneous transfer learning with RBMs. In: Proceedings of the twenty-fifth AAAI confer" /><span class="c-article-references__counter">125.</span><p class="c-article-references__text" id="ref-CR125">Wei B, Pal C (2011) Heterogeneous transfer learning with RBMs. In: Proceedings of the twenty-fifth AAAI conference on artificial intelligence. 2011. p. 531–36.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="KQ. Weinberger, LK. Saul, " /><meta itemprop="datePublished" content="2009" /><meta itemprop="headline" content="Weinberger KQ, Saul LK. Distance metric learning for large margin nearest neighbor classification. JMLR. 2009;" /><span class="c-article-references__counter">126.</span><p class="c-article-references__text" id="ref-CR126">Weinberger KQ, Saul LK. Distance metric learning for large margin nearest neighbor classification. JMLR. 2009;10:207–44.</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.emis.de/MATH-item?1235.68204" aria-label="View reference 126 on MATH">MATH</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 126 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Distance%20metric%20learning%20for%20large%20margin%20nearest%20neighbor%20classification&amp;journal=JMLR&amp;volume=10&amp;pages=207-244&amp;publication_year=2009&amp;author=Weinberger%2CKQ&amp;author=Saul%2CLK">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="C. Widmer, G. Ratsch, " /><meta itemprop="datePublished" content="2012" /><meta itemprop="headline" content="Widmer C, Ratsch G. Multitask learning in computational biology. JMLR. 2012;27:207–16." /><span class="c-article-references__counter">127.</span><p class="c-article-references__text" id="ref-CR127">Widmer C, Ratsch G. Multitask learning in computational biology. JMLR. 2012;27:207–16.</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 127 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Multitask%20learning%20in%20computational%20biology&amp;journal=JMLR&amp;volume=27&amp;pages=207-216&amp;publication_year=2012&amp;author=Widmer%2CC&amp;author=Ratsch%2CG">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="J. Wiens, J. Guttag, EJ. Horvitz, " /><meta itemprop="datePublished" content="2013" /><meta itemprop="headline" content="Wiens J, Guttag J, Horvitz EJ. A study in transfer learning: leveraging data from multiple hospitals to enhanc" /><span class="c-article-references__counter">128.</span><p class="c-article-references__text" id="ref-CR128">Wiens J, Guttag J, Horvitz EJ. A study in transfer learning: leveraging data from multiple hospitals to enhance hospital-specific predictions. J Am Med Inform Assoc. 2013;21(4):699–706.</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1136%2Famiajnl-2013-002162" aria-label="View reference 128">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 128 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20study%20in%20transfer%20learning%3A%20leveraging%20data%20from%20multiple%20hospitals%20to%20enhance%20hospital-specific%20predictions&amp;journal=J%20Am%20Med%20Inform%20Assoc&amp;volume=21&amp;issue=4&amp;pages=699-706&amp;publication_year=2013&amp;author=Wiens%2CJ&amp;author=Guttag%2CJ&amp;author=Horvitz%2CEJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="IH. Witten, E. Frank, " /><meta itemprop="datePublished" content="2011" /><meta itemprop="headline" content="Witten IH, Frank E. Data mining, practical machine learning tools and techniques. 3rd ed. San Francisco: Morga" /><span class="c-article-references__counter">129.</span><p class="c-article-references__text" id="ref-CR129">Witten IH, Frank E. Data mining, practical machine learning tools and techniques. 3rd ed. San Francisco: Morgan Kaufmann Publishers; 2011.</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 129 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Data%20mining%2C%20practical%20machine%20learning%20tools%20and%20techniques&amp;publication_year=2011&amp;author=Witten%2CIH&amp;author=Frank%2CE">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Wu X, Xu D, Duan L, Luo J (2011) Action recognition using context and appearance distribution features. In: IE" /><span class="c-article-references__counter">130.</span><p class="c-article-references__text" id="ref-CR130">Wu X, Xu D, Duan L, Luo J (2011) Action recognition using context and appearance distribution features. In: IEEE 2011 conference on computer vision and pattern recognition. 2011. p. 489–96.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Xia R, Zong C. A POS-based ensemble model for cross-domain sentiment classification. In: Proceedings of the 5t" /><span class="c-article-references__counter">131.</span><p class="c-article-references__text" id="ref-CR131">Xia R, Zong C. A POS-based ensemble model for cross-domain sentiment classification. In: Proceedings of the 5th international joint conference on natural language processing. 2011. p. 614–22.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="R. Xia, C. Zong, X. Hu, E. Cambria, " /><meta itemprop="datePublished" content="2013" /><meta itemprop="headline" content="Xia R, Zong C, Hu X, Cambria E. Feature ensemble plus sample selection: domain adaptation for sentiment classi" /><span class="c-article-references__counter">132.</span><p class="c-article-references__text" id="ref-CR132">Xia R, Zong C, Hu X, Cambria E. Feature ensemble plus sample selection: domain adaptation for sentiment classification. IEEE Intell Syst. 2013;28(3):10–8.</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FMIS.2013.27" aria-label="View reference 132">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 132 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Feature%20ensemble%20plus%20sample%20selection%3A%20domain%20adaptation%20for%20sentiment%20classification&amp;journal=IEEE%20Intell%20Syst&amp;volume=28&amp;issue=3&amp;pages=10-18&amp;publication_year=2013&amp;author=Xia%2CR&amp;author=Zong%2CC&amp;author=Hu%2CX&amp;author=Cambria%2CE">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Xiao M, Guo Y. Semi-supervised kernel matching for domain adaptation. In: Proceedings of the twenty-sixth AAAI" /><span class="c-article-references__counter">133.</span><p class="c-article-references__text" id="ref-CR133">Xiao M, Guo Y. Semi-supervised kernel matching for domain adaptation. In: Proceedings of the twenty-sixth AAAI conference on artificial intelligence. 2012. p. 1183–89.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Xie M, Jean N, Burke M, Lobell D, Ermon S. Transfer learning from deep features for remote sensing and poverty" /><span class="c-article-references__counter">134.</span><p class="c-article-references__text" id="ref-CR134">Xie M, Jean N, Burke M, Lobell D, Ermon S. Transfer learning from deep features for remote sensing and poverty mapping. In: Proceedings 30th AAAI conference on artificial intelligence. 2015. p. 1–10.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Yang J, Yan R, Hauptmann AG. Cross-domain video concept detection using adaptive SVMs. In: Proceedings of the " /><span class="c-article-references__counter">135.</span><p class="c-article-references__text" id="ref-CR135">Yang J, Yan R, Hauptmann AG. Cross-domain video concept detection using adaptive SVMs. In: Proceedings of the 15th ACM international conference on multimedia. 2007. p. 188–97.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Yang L, Jing L, Yu J, Ng MK. Learning transferred weights from co-occurrence data for heterogeneous transfer l" /><span class="c-article-references__counter">136.</span><p class="c-article-references__text" id="ref-CR136">Yang L, Jing L, Yu J, Ng MK. Learning transferred weights from co-occurrence data for heterogeneous transfer learning. IEEE Trans Neural Netw Learn Syst. 2015;PP(99):1–14.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Yang Q, Chen Y, Xue GR, Dai W, Yu Y. Heterogeneous transfer learning for image clustering via the social web. " /><span class="c-article-references__counter">137.</span><p class="c-article-references__text" id="ref-CR137">Yang Q, Chen Y, Xue GR, Dai W, Yu Y. Heterogeneous transfer learning for image clustering via the social web. In: Proceedings of the joint conference of the 47th annual meeting of the ACL, vol. 1. 2009. p. 1–9.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Yao Y, Doretto G. Boosting for transfer learning with multiple sources. In: Proceedings of the IEEE computer s" /><span class="c-article-references__counter">138.</span><p class="c-article-references__text" id="ref-CR138">Yao Y, Doretto G. Boosting for transfer learning with multiple sources. In: Proceedings of the IEEE computer society conference on computer vision and pattern recognition. 2010. p. 1855–62.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Yin Z. http://www.cse.ust.hk/~yinz/. Accessed 4 Mar 2016." /><span class="c-article-references__counter">139.</span><p class="c-article-references__text" id="ref-CR139">Yin Z. <a href="http://www.cse.ust.hk/%7eyinz/">http://www.cse.ust.hk/~yinz/</a>. Accessed 4 Mar 2016.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Zhang Y, Cao B, Yeung D. Multi-domain collaborative filtering. In: Proceedings of the 26th conference on uncer" /><span class="c-article-references__counter">140.</span><p class="c-article-references__text" id="ref-CR140">Zhang Y, Cao B, Yeung D. Multi-domain collaborative filtering. In: Proceedings of the 26th conference on uncertainty in artificial intelligence. 2010. p. 725–32.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Zhang Y, Yeung DY. Transfer metric learning by learning task relationships. In: Proceedings of the 16th ACM SI" /><span class="c-article-references__counter">141.</span><p class="c-article-references__text" id="ref-CR141">Zhang Y, Yeung DY. Transfer metric learning by learning task relationships. In: Proceedings of the 16th ACM SIGKDD international conference on knowledge discovery and data mining. 2010. p. 1199–208.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Zhao L, Pan SJ, Xiang EW, Zhong E, Lu Z, Yang Q. Active transfer learning for cross-system recommendation. In:" /><span class="c-article-references__counter">142.</span><p class="c-article-references__text" id="ref-CR142">Zhao L, Pan SJ, Xiang EW, Zhong E, Lu Z, Yang Q. Active transfer learning for cross-system recommendation. In: Proceedings of the 27th AAAI conference on artificial intelligence. 2013. p. 1205–11.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Zhong E, Fan W, Peng J, Zhang K, Ren J, Turaga D, Verscheure O. Cross domain distribution adaptation via kerne" /><span class="c-article-references__counter">143.</span><p class="c-article-references__text" id="ref-CR143">Zhong E, Fan W, Peng J, Zhang K, Ren J, Turaga D, Verscheure O. Cross domain distribution adaptation via kernel mapping. In: Proceedings of the 15th ACM SIGKDD. 2009. p. 1027–36.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Zhou JT, Pan S, Tsang IW, Yan Y. Hybrid heterogeneous transfer learning through deep learning. In: Proceedings" /><span class="c-article-references__counter">144.</span><p class="c-article-references__text" id="ref-CR144">Zhou JT, Pan S, Tsang IW, Yan Y. Hybrid heterogeneous transfer learning through deep learning. In: Proceedings of the national conference on artificial intelligence, vol. 3. 2014. p. 2213–20.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Zhou JT, Tsang IW, Pan SJ Tan M. Heterogeneous domain adaptation for multiple classes. In: International confe" /><span class="c-article-references__counter">145.</span><p class="c-article-references__text" id="ref-CR145">Zhou JT, Tsang IW, Pan SJ Tan M. Heterogeneous domain adaptation for multiple classes. In: International conference on artificial intelligence and statistics. 2014. p. 1095–103.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Zhu Y, Chen Y, Lu Z, Pan S, Xue G, Yu Y, Yang Q. Heterogeneous transfer learning for image classification. In:" /><span class="c-article-references__counter">146.</span><p class="c-article-references__text" id="ref-CR146">Zhu Y, Chen Y, Lu Z, Pan S, Xue G, Yu Y, Yang Q. Heterogeneous transfer learning for image classification. In: Proceedings of the national conference on artificial intelligence, vol. 2. 2011. p. 1304–9.</p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1186/s40537-016-0043-6-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1" data-title="Authors’ contributions"><div class="c-article-section" id="Ack1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Ack1">Authors’ contributions</h2><div class="c-article-section__content" id="Ack1-content"><p>TMK introduced this topic to KW and provided technical guidance throughout the research and writing phases of the manuscript. KW performed the literature review, analysis, and writing of the manuscript. DDW provided a full review of the final manuscript. All authors read and approved the final manuscript.</p>
                <h3 class="c-article__sub-heading" id="FPar1">Competing interests</h3>
                <p>The authors declare that they have no competing interests.</p>
              </div></div></section><section aria-labelledby="author-information" data-title="Author information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">Florida Atlantic University, 777 Glades Road, Boca Raton, FL, 33431, USA</p><p class="c-article-author-affiliation__authors-list">Karl Weiss, Taghi M. Khoshgoftaar &amp; DingDing Wang</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Karl-Weiss"><span class="c-article-authors-search__title u-h3 js-search-name">Karl Weiss</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Karl+Weiss&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Karl+Weiss" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Karl+Weiss%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Taghi_M_-Khoshgoftaar"><span class="c-article-authors-search__title u-h3 js-search-name">Taghi M. Khoshgoftaar</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Taghi M.+Khoshgoftaar&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Taghi M.+Khoshgoftaar" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Taghi M.+Khoshgoftaar%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-DingDing-Wang"><span class="c-article-authors-search__title u-h3 js-search-name">DingDing Wang</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;DingDing+Wang&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=DingDing+Wang" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22DingDing+Wang%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1186/s40537-016-0043-6/email/correspondent/c1/new">Karl Weiss</a>.</p></div></div></section><section aria-labelledby="appendices"><div class="c-article-section" id="appendices-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="appendices">Appendix</h2><div class="c-article-section__content" id="appendices-content"><h3 class="c-article__sub-heading u-visually-hidden" id="App1">Appendix</h3><p>The majority of transfer learning solutions surveyed are complex and implemented with non-trivial software. It is a great advantage for a researcher to have access to software implementations of transfer learning solutions so comparisons with competing solutions are facilitated more quickly and fairly. Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1186/s40537-016-0043-6#Tab5">5</a> provides a list of available software downloads for a number of the solutions surveyed in this paper. Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1186/s40537-016-0043-6#Tab6">6</a> provides a resource for useful links that point to transfer learning tutorials and other interesting articles on the topic of transfer learning.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-5"><figure><figcaption class="c-article-table__figcaption"><b id="Tab5" data-test="table-caption">Table 5 Software downloads for various transfer learning solutions</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1186/s40537-016-0043-6/tables/5"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-6"><figure><figcaption class="c-article-table__figcaption"><b id="Tab6" data-test="table-caption">Table 6 Useful links for transfer learning information</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1186/s40537-016-0043-6/tables/6"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        </div></div></section><section aria-labelledby="rightslink" data-title="Rights and permissions"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content">
                <p>
                           <b>Open Access</b> This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (<a href="http://creativecommons.org/licenses/by/4.0/" rel="license" itemprop="license">http://creativecommons.org/licenses/by/4.0/</a>), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made.</p>
              <p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=A%20survey%20of%20transfer%20learning&amp;author=Karl%20Weiss%20et%20al&amp;contentID=10.1186%2Fs40537-016-0043-6&amp;copyright=The%20Author%28s%29&amp;publication=2196-1115&amp;publicationDate=2016-05-28&amp;publisherName=SpringerNature&amp;orderBeanReset=true&amp;oa=CC%20BY">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info" data-title="About this article"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="u-hide-print c-bibliographic-information__column c-bibliographic-information__column--border"><a data-crossmark="10.1186/s40537-016-0043-6" target="_blank" rel="noopener" href="https://crossmark.crossref.org/dialog/?doi=10.1186/s40537-016-0043-6" data-track="click" data-track-action="Click Crossmark" data-track-label="link" data-test="crossmark"><img width="57" height="81" alt="Verify currency and authenticity via CrossMark" src="data:image/svg+xml;base64,<svg height="81" width="57" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="m17.35 35.45 21.3-14.2v-17.03h-21.3" fill="#989898"/><path d="m38.65 35.45-21.3-14.2v-17.03h21.3" fill="#747474"/><path d="m28 .5c-12.98 0-23.5 10.52-23.5 23.5s10.52 23.5 23.5 23.5 23.5-10.52 23.5-23.5c0-6.23-2.48-12.21-6.88-16.62-4.41-4.4-10.39-6.88-16.62-6.88zm0 41.25c-9.8 0-17.75-7.95-17.75-17.75s7.95-17.75 17.75-17.75 17.75 7.95 17.75 17.75c0 4.71-1.87 9.22-5.2 12.55s-7.84 5.2-12.55 5.2z" fill="#535353"/><path d="m41 36c-5.81 6.23-15.23 7.45-22.43 2.9-7.21-4.55-10.16-13.57-7.03-21.5l-4.92-3.11c-4.95 10.7-1.19 23.42 8.78 29.71 9.97 6.3 23.07 4.22 30.6-4.86z" fill="#9c9c9c"/><path d="m.2 58.45c0-.75.11-1.42.33-2.01s.52-1.09.91-1.5c.38-.41.83-.73 1.34-.94.51-.22 1.06-.32 1.65-.32.56 0 1.06.11 1.51.35.44.23.81.5 1.1.81l-.91 1.01c-.24-.24-.49-.42-.75-.56-.27-.13-.58-.2-.93-.2-.39 0-.73.08-1.05.23-.31.16-.58.37-.81.66-.23.28-.41.63-.53 1.04-.13.41-.19.88-.19 1.39 0 1.04.23 1.86.68 2.46.45.59 1.06.88 1.84.88.41 0 .77-.07 1.07-.23s.59-.39.85-.68l.91 1c-.38.43-.8.76-1.28.99-.47.22-1 .34-1.58.34-.59 0-1.13-.1-1.64-.31-.5-.2-.94-.51-1.31-.91-.38-.4-.67-.9-.88-1.48-.22-.59-.33-1.26-.33-2.02zm8.4-5.33h1.61v2.54l-.05 1.33c.29-.27.61-.51.96-.72s.76-.31 1.24-.31c.73 0 1.27.23 1.61.71.33.47.5 1.14.5 2.02v4.31h-1.61v-4.1c0-.57-.08-.97-.25-1.21-.17-.23-.45-.35-.83-.35-.3 0-.56.08-.79.22-.23.15-.49.36-.78.64v4.8h-1.61zm7.37 6.45c0-.56.09-1.06.26-1.51.18-.45.42-.83.71-1.14.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.36c.07.62.29 1.1.65 1.44.36.33.82.5 1.38.5.29 0 .57-.04.83-.13s.51-.21.76-.37l.55 1.01c-.33.21-.69.39-1.09.53-.41.14-.83.21-1.26.21-.48 0-.92-.08-1.34-.25-.41-.16-.76-.4-1.07-.7-.31-.31-.55-.69-.72-1.13-.18-.44-.26-.95-.26-1.52zm4.6-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.07.45-.31.29-.5.73-.58 1.3zm2.5.62c0-.57.09-1.08.28-1.53.18-.44.43-.82.75-1.13s.69-.54 1.1-.71c.42-.16.85-.24 1.31-.24.45 0 .84.08 1.17.23s.61.34.85.57l-.77 1.02c-.19-.16-.38-.28-.56-.37-.19-.09-.39-.14-.61-.14-.56 0-1.01.21-1.35.63-.35.41-.52.97-.52 1.67 0 .69.17 1.24.51 1.66.34.41.78.62 1.32.62.28 0 .54-.06.78-.17.24-.12.45-.26.64-.42l.67 1.03c-.33.29-.69.51-1.08.65-.39.15-.78.23-1.18.23-.46 0-.9-.08-1.31-.24-.4-.16-.75-.39-1.05-.7s-.53-.69-.7-1.13c-.17-.45-.25-.96-.25-1.53zm6.91-6.45h1.58v6.17h.05l2.54-3.16h1.77l-2.35 2.8 2.59 4.07h-1.75l-1.77-2.98-1.08 1.23v1.75h-1.58zm13.69 1.27c-.25-.11-.5-.17-.75-.17-.58 0-.87.39-.87 1.16v.75h1.34v1.27h-1.34v5.6h-1.61v-5.6h-.92v-1.2l.92-.07v-.72c0-.35.04-.68.13-.98.08-.31.21-.57.4-.79s.42-.39.71-.51c.28-.12.63-.18 1.04-.18.24 0 .48.02.69.07.22.05.41.1.57.17zm.48 5.18c0-.57.09-1.08.27-1.53.17-.44.41-.82.72-1.13.3-.31.65-.54 1.04-.71.39-.16.8-.24 1.23-.24s.84.08 1.24.24c.4.17.74.4 1.04.71s.54.69.72 1.13c.19.45.28.96.28 1.53s-.09 1.08-.28 1.53c-.18.44-.42.82-.72 1.13s-.64.54-1.04.7-.81.24-1.24.24-.84-.08-1.23-.24-.74-.39-1.04-.7c-.31-.31-.55-.69-.72-1.13-.18-.45-.27-.96-.27-1.53zm1.65 0c0 .69.14 1.24.43 1.66.28.41.68.62 1.18.62.51 0 .9-.21 1.19-.62.29-.42.44-.97.44-1.66 0-.7-.15-1.26-.44-1.67-.29-.42-.68-.63-1.19-.63-.5 0-.9.21-1.18.63-.29.41-.43.97-.43 1.67zm6.48-3.44h1.33l.12 1.21h.05c.24-.44.54-.79.88-1.02.35-.24.7-.36 1.07-.36.32 0 .59.05.78.14l-.28 1.4-.33-.09c-.11-.01-.23-.02-.38-.02-.27 0-.56.1-.86.31s-.55.58-.77 1.1v4.2h-1.61zm-47.87 15h1.61v4.1c0 .57.08.97.25 1.2.17.24.44.35.81.35.3 0 .57-.07.8-.22.22-.15.47-.39.73-.73v-4.7h1.61v6.87h-1.32l-.12-1.01h-.04c-.3.36-.63.64-.98.86-.35.21-.76.32-1.24.32-.73 0-1.27-.24-1.61-.71-.33-.47-.5-1.14-.5-2.02zm9.46 7.43v2.16h-1.61v-9.59h1.33l.12.72h.05c.29-.24.61-.45.97-.63.35-.17.72-.26 1.1-.26.43 0 .81.08 1.15.24.33.17.61.4.84.71.24.31.41.68.53 1.11.13.42.19.91.19 1.44 0 .59-.09 1.11-.25 1.57-.16.47-.38.85-.65 1.16-.27.32-.58.56-.94.73-.35.16-.72.25-1.1.25-.3 0-.6-.07-.9-.2s-.59-.31-.87-.56zm0-2.3c.26.22.5.37.73.45.24.09.46.13.66.13.46 0 .84-.2 1.15-.6.31-.39.46-.98.46-1.77 0-.69-.12-1.22-.35-1.61-.23-.38-.61-.57-1.13-.57-.49 0-.99.26-1.52.77zm5.87-1.69c0-.56.08-1.06.25-1.51.16-.45.37-.83.65-1.14.27-.3.58-.54.93-.71s.71-.25 1.08-.25c.39 0 .73.07 1 .2.27.14.54.32.81.55l-.06-1.1v-2.49h1.61v9.88h-1.33l-.11-.74h-.06c-.25.25-.54.46-.88.64-.33.18-.69.27-1.06.27-.87 0-1.56-.32-2.07-.95s-.76-1.51-.76-2.65zm1.67-.01c0 .74.13 1.31.4 1.7.26.38.65.58 1.15.58.51 0 .99-.26 1.44-.77v-3.21c-.24-.21-.48-.36-.7-.45-.23-.08-.46-.12-.7-.12-.45 0-.82.19-1.13.59-.31.39-.46.95-.46 1.68zm6.35 1.59c0-.73.32-1.3.97-1.71.64-.4 1.67-.68 3.08-.84 0-.17-.02-.34-.07-.51-.05-.16-.12-.3-.22-.43s-.22-.22-.38-.3c-.15-.06-.34-.1-.58-.1-.34 0-.68.07-1 .2s-.63.29-.93.47l-.59-1.08c.39-.24.81-.45 1.28-.63.47-.17.99-.26 1.54-.26.86 0 1.51.25 1.93.76s.63 1.25.63 2.21v4.07h-1.32l-.12-.76h-.05c-.3.27-.63.48-.98.66s-.73.27-1.14.27c-.61 0-1.1-.19-1.48-.56-.38-.36-.57-.85-.57-1.46zm1.57-.12c0 .3.09.53.27.67.19.14.42.21.71.21.28 0 .54-.07.77-.2s.48-.31.73-.56v-1.54c-.47.06-.86.13-1.18.23-.31.09-.57.19-.76.31s-.33.25-.41.4c-.09.15-.13.31-.13.48zm6.29-3.63h-.98v-1.2l1.06-.07.2-1.88h1.34v1.88h1.75v1.27h-1.75v3.28c0 .8.32 1.2.97 1.2.12 0 .24-.01.37-.04.12-.03.24-.07.34-.11l.28 1.19c-.19.06-.4.12-.64.17-.23.05-.49.08-.76.08-.4 0-.74-.06-1.02-.18-.27-.13-.49-.3-.67-.52-.17-.21-.3-.48-.37-.78-.08-.3-.12-.64-.12-1.01zm4.36 2.17c0-.56.09-1.06.27-1.51s.41-.83.71-1.14c.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.37c.08.62.29 1.1.65 1.44.36.33.82.5 1.38.5.3 0 .58-.04.84-.13.25-.09.51-.21.76-.37l.54 1.01c-.32.21-.69.39-1.09.53s-.82.21-1.26.21c-.47 0-.92-.08-1.33-.25-.41-.16-.77-.4-1.08-.7-.3-.31-.54-.69-.72-1.13-.17-.44-.26-.95-.26-1.52zm4.61-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.08.45-.31.29-.5.73-.57 1.3zm3.01 2.23c.31.24.61.43.92.57.3.13.63.2.98.2.38 0 .65-.08.83-.23s.27-.35.27-.6c0-.14-.05-.26-.13-.37-.08-.1-.2-.2-.34-.28-.14-.09-.29-.16-.47-.23l-.53-.22c-.23-.09-.46-.18-.69-.3-.23-.11-.44-.24-.62-.4s-.33-.35-.45-.55c-.12-.21-.18-.46-.18-.75 0-.61.23-1.1.68-1.49.44-.38 1.06-.57 1.83-.57.48 0 .91.08 1.29.25s.71.36.99.57l-.74.98c-.24-.17-.49-.32-.73-.42-.25-.11-.51-.16-.78-.16-.35 0-.6.07-.76.21-.17.15-.25.33-.25.54 0 .14.04.26.12.36s.18.18.31.26c.14.07.29.14.46.21l.54.19c.23.09.47.18.7.29s.44.24.64.4c.19.16.34.35.46.58.11.23.17.5.17.82 0 .3-.06.58-.17.83-.12.26-.29.48-.51.68-.23.19-.51.34-.84.45-.34.11-.72.17-1.15.17-.48 0-.95-.09-1.41-.27-.46-.19-.86-.41-1.2-.68z" fill="#535353"/></g></svg>" /></a></div><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Weiss, K., Khoshgoftaar, T.M. &amp; Wang, D. A survey of transfer learning.
                    <i>J Big Data</i> <b>3, </b>9 (2016). https://doi.org/10.1186/s40537-016-0043-6</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" data-track-external="" href="/article/10.1186/s40537-016-0043-6.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2016-03-04">04 March 2016</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2016-05-17">17 May 2016</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2016-05-28">28 May 2016</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1186/s40537-016-0043-6" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1186/s40537-016-0043-6</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">Transfer learning</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Survey</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Domain adaptation</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Machine learning</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Data mining</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1186/s40537-016-0043-6.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button" data-track-external  download>
            
                <span>Download PDF</span>
                <svg aria-hidden="true" focusable="false" width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>

                </div>

                <div data-test="collections">
                    <div id="SpringerLinkArticleCollections">
    
</div>

                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/40537/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=43;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 87.4.11.85</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Not affiliated
        </p>

    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2021 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
        <symbol id="icon-info" viewBox="0 0 18 18">
            <path d="m9 0c4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9zm0 7h-1.5l-.11662113.00672773c-.49733868.05776511-.88337887.48043643-.88337887.99327227 0 .47338693.32893365.86994729.77070917.97358929l.1126697.01968298.11662113.00672773h.5v3h-.5l-.11662113.0067277c-.42082504.0488782-.76196299.3590206-.85696816.7639815l-.01968298.1126697-.00672773.1166211.00672773.1166211c.04887817.4208251.35902055.761963.76398144.8569682l.1126697.019683.11662113.0067277h3l.1166211-.0067277c.4973387-.0577651.8833789-.4804365.8833789-.9932723 0-.4733869-.3289337-.8699473-.7707092-.9735893l-.1126697-.019683-.1166211-.0067277h-.5v-4l-.00672773-.11662113c-.04887817-.42082504-.35902055-.76196299-.76398144-.85696816l-.1126697-.01968298zm0-3.25c-.69035594 0-1.25.55964406-1.25 1.25s.55964406 1.25 1.25 1.25 1.25-.55964406 1.25-1.25-.55964406-1.25-1.25-1.25z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="icon-chevron-down" viewBox="0 0 16 16">
            <path d="m5.58578644 3-3.29289322-3.29289322c-.39052429-.39052429-.39052429-1.02368927 0-1.41421356s1.02368927-.39052429 1.41421356 0l4 4c.39052429.39052429.39052429 1.02368927 0 1.41421356l-4 4c-.39052429.39052429-1.02368927.39052429-1.41421356 0s-.39052429-1.02368927 0-1.41421356z" fill-rule="evenodd" transform="matrix(0 1 -1 0 11 1)"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

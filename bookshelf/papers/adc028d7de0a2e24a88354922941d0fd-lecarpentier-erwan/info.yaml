abstract: 'This work tackles the problem of robust zero-shot planning in non-stationary
  stochastic environments. We study Markov Decision Processes (MDPs) evolving over
  time and consider Model-Based Reinforcement Learning algorithms in this setting.
  We make two hypotheses: 1) the environment evolves continuously with a bounded evolution
  rate; 2) a current model is known at each decision epoch but not its evolution.
  Our contribution can be presented in four points. 1) we define a specific class
  of MDPs that we call Non-Stationary MDPs (NSMDPs). We introduce the notion of regular
  evolution by making an hypothesis of Lipschitz-Continuity on the transition and
  reward functions w.r.t. time; 2) we consider a planning agent using the current
  model of the environment but unaware of its future evolution. This leads us to consider
  a worst-case method where the environment is seen as an adversarial agent; 3) following
  this approach, we propose the Risk-Averse Tree-Search (RATS) algorithm, a zero-shot
  Model-Based method similar to Minimax search; 4) we illustrate the benefits brought
  by RATS empirically and compare its performance with reference Model-Based algorithms.'
archiveprefix: arXiv
author: Lecarpentier, Erwan and Rachelson, Emmanuel
author_list:
- family: Lecarpentier
  given: Erwan
- family: Rachelson
  given: Emmanuel
eprint: 1904.10090v4
file: 1904.10090v4.pdf
files:
- lecarpentier-erwan-and-rachelson-emmanuelnon-stationary-markov-decision-processes-a-worst-case-approach-using-model-based-reinforcement-learning.pdf
month: Apr
note: 'year: 2019; page range: 7214--7223'
primaryclass: cs.LG
ref: 1904.10090v4
time-added: 2021-04-11-10:26:58
title: Non-Stationary Markov Decision Processes, a Worst-Case Approach using   Model-Based
  Reinforcement Learning, Extended version
type: article
url: http://arxiv.org/abs/1904.10090v4
year: '2019'

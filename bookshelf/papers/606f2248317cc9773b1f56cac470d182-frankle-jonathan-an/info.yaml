abstract: 'Neural network pruning techniques can reduce the parameter counts of trained
  networks by over 90%, decreasing storage requirements and improving computational
  performance of inference without compromising accuracy. However, contemporary experience
  is that the sparse architectures produced by pruning are difficult to train from
  the start, which would similarly improve training performance.   We find that a
  standard pruning technique naturally uncovers subnetworks whose initializations
  made them capable of training effectively. Based on these results, we articulate
  the "lottery ticket hypothesis:" dense, randomly-initialized, feed-forward networks
  contain subnetworks ("winning tickets") that - when trained in isolation - reach
  test accuracy comparable to the original network in a similar number of iterations.
  The winning tickets we find have won the initialization lottery: their connections
  have initial weights that make training particularly effective.   We present an
  algorithm to identify winning tickets and a series of experiments that support the
  lottery ticket hypothesis and the importance of these fortuitous initializations.
  We consistently find winning tickets that are less than 10-20% of the size of several
  fully-connected and convolutional feed-forward architectures for MNIST and CIFAR10.
  Above this size, the winning tickets that we find learn faster than the original
  network and reach higher test accuracy.'
archiveprefix: arXiv
author: Frankle, Jonathan and Carbin, Michael
author_list:
- family: Frankle
  given: Jonathan
- family: Carbin
  given: Michael
eprint: 1803.03635v5
file: 1803.03635v5.pdf
files:
- frankle-jonathan-and-carbin-michaelthe-lottery-ticket-hypothesis-finding-sparse-trainable-neural-networks2018.pdf
month: Mar
note: ICLR 2019
primaryclass: cs.LG
ref: 1803.03635v5
time-added: 2020-06-24-00:11:17
title: 'The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks'
type: article
url: http://arxiv.org/abs/1803.03635v5
year: '2018'

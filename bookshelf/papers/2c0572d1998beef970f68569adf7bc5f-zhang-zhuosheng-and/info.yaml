abstract: Large language models (LLMs) have shown impressive performance on complex
  reasoning by leveraging chain-of-thought (CoT) prompting to generate intermediate
  reasoning chains as the rationale to infer the answer. However, existing CoT studies
  have focused on the language modality. We propose Multimodal-CoT that incorporates
  language (text) and vision (images) modalities into a two-stage framework that separates
  rationale generation and answer inference. In this way, answer inference can leverage
  better generated rationales that are based on multimodal information. With Multimodal-CoT,
  our model under 1 billion parameters outperforms the previous state-of-the-art LLM
  (GPT-3.5) by 16 percentage points (75.17%->91.68% accuracy) on the ScienceQA benchmark
  and even surpasses human performance. Code is publicly available available at https://github.com/amazon-science/mm-cot.
archiveprefix: arXiv
author: Zhang, Zhuosheng and Zhang, Aston and Li, Mu and Zhao, Hai and Karypis, George
  and Smola, Alex
author_list:
- family: Zhang
  given: Zhuosheng
- family: Zhang
  given: Aston
- family: Li
  given: Mu
- family: Zhao
  given: Hai
- family: Karypis
  given: George
- family: Smola
  given: Alex
eprint: 2302.00923v4
file: 2302.00923v4.pdf
files:
- zhang-zhuosheng-and-zhang-aston-and-li-mu-and-zhao-hai-and-karypis-george-and-smola-alexmultimodal-chain-of-thought-reasoning-in-language-models.pdf
month: Feb
primaryclass: cs.CL
ref: 2302.00923v4
time-added: 2023-02-21-10:08:09
title: Multimodal Chain-of-Thought Reasoning in Language Models
type: article
url: http://arxiv.org/abs/2302.00923v4
year: '2023'

abstract: 'We compare the elementary theories of Shannon information and Kolmogorov
  complexity, the extent to which they have a common purpose, and where they are fundamentally
  different. We discuss and relate the basic notions of both theories: Shannon entropy
  versus Kolmogorov complexity, the relation of both to universal coding, Shannon
  mutual information versus Kolmogorov (`algorithmic'') mutual information, probabilistic
  sufficient statistic versus algorithmic sufficient statistic (related to lossy compression
  in the Shannon theory versus meaningful information in the Kolmogorov theory), and
  rate distortion theory versus Kolmogorov''s structure function. Part of the material
  has appeared in print before, scattered through various publications, but this is
  the first comprehensive systematic comparison. The last mentioned relations are
  new.'
archiveprefix: arXiv
author: Grunwald, Peter and Vitanyi, Paul
author_list:
- family: Grunwald
  given: Peter
- family: Vitanyi
  given: Paul
eprint: cs/0410002v1
file: cs/0410002v1.pdf
files:
- grunwald-peter-and-vitanyi-paulshannon-information-and-kolmogorov-complexity2004.pdf
month: Oct
note: There are some errors in this paper draft; when in doubt see the   textbook
  Li, Vitanyi, An Introduction to Kolmogorov Complexity and Its   Applications, Springer,
  1993, 1997, 2008, 2019
primaryclass: cs.IT
ref: cs/0410002v1
time-added: 2021-01-23-14:02:33
title: Shannon Information and Kolmogorov Complexity
type: article
url: http://arxiv.org/abs/cs/0410002v1
year: '2004'

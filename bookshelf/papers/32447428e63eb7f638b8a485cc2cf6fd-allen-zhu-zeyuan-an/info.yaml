abstract: Recurrent Neural Networks (RNNs) are among the most popular models in sequential
  data analysis. Yet, in the foundational PAC learning language, what concept class
  can it learn? Moreover, how can the same recurrent unit simultaneously learn functions
  from different input tokens to different output tokens, without affecting each other?
  Existing generalization bounds for RNN scale exponentially with the input length,
  significantly limiting their practical implications.   In this paper, we show using
  the vanilla stochastic gradient descent (SGD), RNN can actually learn some notable
  concept class efficiently, meaning that both time and sample complexity scale polynomially
  in the input length (or almost polynomially, depending on the concept). This concept
  class at least includes functions where each output token is generated from inputs
  of earlier tokens using a smooth two-layer neural network.
archiveprefix: arXiv
author: Allen-Zhu, Zeyuan and Li, Yuanzhi
author_list:
- family: Allen-Zhu
  given: Zeyuan
- family: Li
  given: Yuanzhi
eprint: 1902.01028v2
file: 1902.01028v2.pdf
files:
- allen-zhu-zeyuan-and-li-yuanzhican-sgd-learn-recurrent-neural-networks-with-provable-generalization-2019.pdf
month: Feb
primaryclass: cs.LG
ref: 1902.01028v2
time-added: 2023-02-06-10:23:55
title: Can SGD Learn Recurrent Neural Networks with Provable Generalization?
type: article
url: http://arxiv.org/abs/1902.01028v2
year: '2019'

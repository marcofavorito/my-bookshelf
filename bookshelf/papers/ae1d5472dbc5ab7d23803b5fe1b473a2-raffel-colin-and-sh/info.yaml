abstract: Transfer learning, where a model is first pre-trained on a data-rich task
  before being fine-tuned on a downstream task, has emerged as a powerful technique
  in natural language processing (NLP). The effectiveness of transfer learning has
  given rise to a diversity of approaches, methodology, and practice. In this paper,
  we explore the landscape of transfer learning techniques for NLP by introducing
  a unified framework that converts all text-based language problems into a text-to-text
  format. Our systematic study compares pre-training objectives, architectures, unlabeled
  data sets, transfer approaches, and other factors on dozens of language understanding
  tasks. By combining the insights from our exploration with scale and our new ``Colossal
  Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering
  summarization, question answering, text classification, and more. To facilitate
  future work on transfer learning for NLP, we release our data set, pre-trained models,
  and code.
archiveprefix: arXiv
author: Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang,
  Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.
author_list:
- family: Raffel
  given: Colin
- family: Shazeer
  given: Noam
- family: Roberts
  given: Adam
- family: Lee
  given: Katherine
- family: Narang
  given: Sharan
- family: Matena
  given: Michael
- family: Zhou
  given: Yanqi
- family: Li
  given: Wei
- family: Liu
  given: Peter J.
eprint: 1910.10683v3
file: 1910.10683v3.pdf
files:
- raffel-colin-and-shazeer-noam-and-roberts-adam-and-lee-katherine-and-narang-sharan-and-matena-michael-and-zhou-yanqi-and-li-wei-and-liu-peter.pdf
month: Oct
primaryclass: cs.LG
ref: 1910.10683v3
time-added: 2022-05-24-19:20:33
title: Exploring the Limits of Transfer Learning with a Unified Text-to-Text   Transformer
type: article
url: http://arxiv.org/abs/1910.10683v3
year: '2019'

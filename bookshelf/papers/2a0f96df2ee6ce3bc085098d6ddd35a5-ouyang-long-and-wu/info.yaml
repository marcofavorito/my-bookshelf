abstract: Making language models bigger does not inherently make them better at following
  a user's intent. For example, large language models can generate outputs that are
  untruthful, toxic, or simply not helpful to the user. In other words, these models
  are not aligned with their users. In this paper, we show an avenue for aligning
  language models with user intent on a wide range of tasks by fine-tuning with human
  feedback. Starting with a set of labeler-written prompts and prompts submitted through
  the OpenAI API, we collect a dataset of labeler demonstrations of the desired model
  behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect
  a dataset of rankings of model outputs, which we use to further fine-tune this supervised
  model using reinforcement learning from human feedback. We call the resulting models
  InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B
  parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite
  having 100x fewer parameters. Moreover, InstructGPT models show improvements in
  truthfulness and reductions in toxic output generation while having minimal performance
  regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes,
  our results show that fine-tuning with human feedback is a promising direction for
  aligning language models with human intent.
archiveprefix: arXiv
author: Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright,
  Carroll L. and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama,
  Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and
  Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano,
  Paul and Leike, Jan and Lowe, Ryan
author_list:
- family: Ouyang
  given: Long
- family: Wu
  given: Jeff
- family: Jiang
  given: Xu
- family: Almeida
  given: Diogo
- family: Wainwright
  given: Carroll L.
- family: Mishkin
  given: Pamela
- family: Zhang
  given: Chong
- family: Agarwal
  given: Sandhini
- family: Slama
  given: Katarina
- family: Ray
  given: Alex
- family: Schulman
  given: John
- family: Hilton
  given: Jacob
- family: Kelton
  given: Fraser
- family: Miller
  given: Luke
- family: Simens
  given: Maddie
- family: Askell
  given: Amanda
- family: Welinder
  given: Peter
- family: Christiano
  given: Paul
- family: Leike
  given: Jan
- family: Lowe
  given: Ryan
eprint: 2203.02155v1
file: 2203.02155v1.pdf
files:
- ouyang-long-and-wu-jeff-and-jiang-xu-and-almeida-diogo-and-wainwright-carroll-l.-and-mishkin-pamela-and-zhang-chong-and-agarwal-sandhini-and-s.pdf
month: Mar
primaryclass: cs.CL
ref: 2203.02155v1
time-added: 2023-02-28-21:23:32
title: Training language models to follow instructions with human feedback
type: article
url: http://arxiv.org/abs/2203.02155v1
year: '2022'

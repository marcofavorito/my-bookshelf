abstract: Automatic differentiation (AD) in reverse mode (RAD) is a central component
  of deep learning and other uses of large-scale optimization. Commonly used RAD algorithms
  such as backpropagation, however, are complex and stateful, hindering deep understanding,
  improvement, and parallel execution. This paper develops a simple, generalized AD
  algorithm calculated from a simple, natural specification. The general algorithm
  is then specialized by varying the representation of derivatives. In particular,
  applying well-known constructions to a naive representation yields two RAD algorithms
  that are far simpler than previously known. In contrast to commonly used RAD implementations,
  the algorithms defined here involve no graphs, tapes, variables, partial derivatives,
  or mutation. They are inherently parallel-friendly, correct by construction, and
  usable directly from an existing programming language with no need for new data
  types or programming style, thanks to use of an AD-agnostic compiler plugin.
archiveprefix: arXiv
author: Elliott, Conal
author_list:
- family: Elliott
  given: Conal
eprint: 1804.00746v4
file: 1804.00746v4.pdf
files:
- elliott-conalthe-simple-essence-of-automatic-differentiation2018.pdf
month: Apr
primaryclass: cs.PL
ref: 1804.00746v4
time-added: 2020-06-20-14:59:51
title: The simple essence of automatic differentiation
type: article
url: http://arxiv.org/abs/1804.00746v4
year: '2018'

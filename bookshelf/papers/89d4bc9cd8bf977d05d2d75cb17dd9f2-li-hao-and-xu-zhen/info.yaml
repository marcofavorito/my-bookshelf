abstract: Neural network training relies on our ability to find "good" minimizers
  of highly non-convex loss functions. It is well-known that certain network architecture
  designs (e.g., skip connections) produce loss functions that train easier, and well-chosen
  training parameters (batch size, learning rate, optimizer) produce minimizers that
  generalize better. However, the reasons for these differences, and their effects
  on the underlying loss landscape, are not well understood. In this paper, we explore
  the structure of neural loss functions, and the effect of loss landscapes on generalization,
  using a range of visualization methods. First, we introduce a simple "filter normalization"
  method that helps us visualize loss function curvature and make meaningful side-by-side
  comparisons between loss functions. Then, using a variety of visualizations, we
  explore how network architecture affects the loss landscape, and how training parameters
  affect the shape of minimizers.
archiveprefix: arXiv
author: Li, Hao and Xu, Zheng and Taylor, Gavin and Studer, Christoph and Goldstein,
  Tom
author_list:
- family: Li
  given: Hao
- family: Xu
  given: Zheng
- family: Taylor
  given: Gavin
- family: Studer
  given: Christoph
- family: Goldstein
  given: Tom
eprint: 1712.09913v3
file: 1712.09913v3.pdf
files:
- li-hao-and-xu-zheng-and-taylor-gavin-and-studer-christoph-and-goldstein-tomvisualizing-the-loss-landscape-of-neural-nets2017.pdf
month: Dec
primaryclass: cs.LG
ref: 1712.09913v3
time-added: 2022-08-16-14:06:17
title: Visualizing the Loss Landscape of Neural Nets
type: article
url: http://arxiv.org/abs/1712.09913v3
year: '2017'

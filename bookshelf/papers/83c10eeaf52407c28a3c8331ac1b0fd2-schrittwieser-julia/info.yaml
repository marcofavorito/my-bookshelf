author: Schrittwieser, Julian and Antonoglou, Ioannis and Hubert, Thomas and Simonyan,
  Karen and Sifre, Laurent and Schmitt, Simon and Guez, Arthur and Lockhart, Edward
  and Hassabis, Demis and Graepel, Thore and Lillicrap, Timothy and Silver, David
author_list:
- affiliation: []
  family: Schrittwieser
  given: Julian
- affiliation: []
  family: Antonoglou
  given: Ioannis
- affiliation: []
  family: Hubert
  given: Thomas
- affiliation: []
  family: Simonyan
  given: Karen
- affiliation: []
  family: Sifre
  given: Laurent
- affiliation: []
  family: Schmitt
  given: Simon
- affiliation: []
  family: Guez
  given: Arthur
- affiliation: []
  family: Lockhart
  given: Edward
- affiliation: []
  family: Hassabis
  given: Demis
- affiliation: []
  family: Graepel
  given: Thore
- affiliation: []
  family: Lillicrap
  given: Timothy
- affiliation: []
  family: Silver
  given: David
citations:
- author: M Campbell
  doi: 10.1016/S0004-3702(01)00129-1
  first-page: '57'
  journal-title: Artif. Intell.
  unstructured: Campbell, M., Hoane, A. J. Jr & Hsu, F.-h. Deep Blue. Artif. Intell.
    134, 57–83 (2002).
  volume: '134'
  year: '2002'
- author: D Silver
  doi: 10.1038/nature16961
  first-page: '484'
  journal-title: Nature
  unstructured: Silver, D. et al. Mastering the game of Go with deep neural networks
    and tree search. Nature 529, 484–489 (2016).
  volume: '529'
  year: '2016'
- author: MG Bellemare
  doi: 10.1613/jair.3912
  first-page: '253'
  journal-title: J. Artif. Intell. Res.
  unstructured: 'Bellemare, M. G., Naddaf, Y., Veness, J. & Bowling, M. The arcade
    learning environment: an evaluation platform for general agents. J. Artif. Intell.
    Res. 47, 253–279 (2013).'
  volume: '47'
  year: '2013'
- author: M Machado
  doi: 10.1613/jair.5699
  first-page: '523'
  journal-title: J. Artif. Intell. Res.
  unstructured: 'Machado, M. et al. Revisiting the arcade learning environment: evaluation
    protocols and open problems for general agents. J. Artif. Intell. Res. 61, 523–562
    (2018).'
  volume: '61'
  year: '2018'
- author: D Silver
  doi: 10.1126/science.aar6404
  first-page: '1140'
  journal-title: Science
  unstructured: Silver, D. et al. A general reinforcement learning algorithm that
    masters chess, shogi, and Go through self-play. Science 362, 1140–1144 (2018).
  volume: '362'
  year: '2018'
- author: J Schaeffer
  doi: 10.1016/0004-3702(92)90074-8
  first-page: '273'
  journal-title: Artif. Intell.
  unstructured: Schaeffer, J. et al. A world championship caliber checkers program.
    Artif. Intell. 53, 273–289 (1992).
  volume: '53'
  year: '1992'
- author: N Brown
  doi: 10.1126/science.aao1733
  first-page: '418'
  journal-title: Science
  unstructured: 'Brown, N. & Sandholm, T. Superhuman AI for heads-up no-limit poker:
    Libratus beats top professionals. Science 359, 418–424 (2018).'
  volume: '359'
  year: '2018'
- author: M Moravčík
  doi: 10.1126/science.aam6960
  first-page: '508'
  journal-title: Science
  unstructured: 'Moravčík, M. et al. Deepstack: expert-level artificial intelligence
    in heads-up no-limit poker. Science 356, 508–513 (2017).'
  volume: '356'
  year: '2017'
- unstructured: Vlahavas, I. & Refanidis, I. Planning and Scheduling Technical Report
    (EETN, 2013).
- author: MH Segler
  doi: 10.1038/nature25978
  first-page: '604'
  journal-title: Nature
  unstructured: Segler, M. H., Preuss, M. & Waller, M. P. Planning chemical syntheses
    with deep neural networks and symbolic AI. Nature 555, 604–610 (2018).
  volume: '555'
  year: '2018'
- unstructured: 'Sutton, R. S. & Barto, A. G. Reinforcement Learning: An Introduction
    2nd edn (MIT Press, 2018).'
- unstructured: 'Deisenroth, M. & Rasmussen, C. PILCO: a model-based and data-efficient
    approach to policy search. In Proc. 28th International Conference on Machine Learning,
    ICML 2011 465–472 (Omnipress, 2011).'
- unstructured: 'Heess, N. et al. Learning continuous control policies by stochastic
    value gradients. In NIPS’15: Proc. 28th International Conference on Neural Information
    Processing Systems Vol. 2 (eds Cortes, C. et al.) 2944–2952 (MIT Press, 2015).'
- author: S Levine
  first-page: '1071'
  journal-title: Adv. Neural Inf. Process. Syst.
  unstructured: Levine, S. & Abbeel, P. Learning neural network policies with guided
    policy search under unknown dynamics. Adv. Neural Inf. Process. Syst. 27, 1071–1079
    (2014).
  volume: '27'
  year: '2014'
- unstructured: Hafner, D. et al. Learning latent dynamics for planning from pixels.
    Preprint at https://arxiv.org/abs/1811.04551 (2018).
- unstructured: Kaiser, L. et al. Model-based reinforcement learning for atari. Preprint
    at https://arxiv.org/abs/1903.00374 (2019).
- unstructured: Buesing, L. et al. Learning and querying fast generative models for
    reinforcement learning. Preprint at https://arxiv.org/abs/1802.03006 (2018).
- unstructured: 'Espeholt, L. et al. IMPALA: scalable distributed deep-RL with importance
    weighted actor-learner architectures. In Proc. International Conference on Machine
    Learning, ICML Vol. 80 (eds Dy, J. & Krause, A.) 1407–1416 (2018).'
- unstructured: Kapturowski, S., Ostrovski, G., Dabney, W., Quan, J. & Munos, R. Recurrent
    experience replay in distributed reinforcement learning. In International Conference
    on Learning Representations (2019).
- unstructured: Horgan, D. et al. Distributed prioritized experience replay. In International
    Conference on Learning Representations (2018).
- doi: 10.1002/9780470316887
  unstructured: 'Puterman, M. L. Markov Decision Processes: Discrete Stochastic Dynamic
    Programming 1st edn (John Wiley & Sons, 1994).'
- doi: 10.1007/978-3-540-75538-8_7
  unstructured: Coulom, R. Efficient selectivity and backup operators in Monte-Carlo
    tree search. In International Conference on Computers and Games 72–83 (Springer,
    2006).
- unstructured: 'Wahlström, N., Schön, T. B. & Deisenroth, M. P. From pixels to torques:
    policy learning with deep dynamical models. Preprint at http://arxiv.org/abs/1502.02251
    (2015).'
- unstructured: 'Watter, M., Springenberg, J. T., Boedecker, J. & Riedmiller, M. Embed
    to control: a locally linear latent dynamics model for control from raw images.
    In NIPS’15: Proc. 28th International Conference on Neural Information Processing
    Systems Vol. 2 (eds Cortes, C. et al.) 2746–2754 (MIT Press, 2015).'
- unstructured: 'Ha, D. & Schmidhuber, J. Recurrent world models facilitate policy
    evolution. In NIPS’18: Proc. 32nd International Conference on Neural Information
    Processing Systems (eds Bengio, S. et al.) 2455–2467 (Curran Associates, 2018).'
- unstructured: 'Gelada, C., Kumar, S., Buckman, J., Nachum, O. & Bellemare, M. G.
    DeepMDP: learning continuous latent space models for representation learning.
    Proc. 36th International Conference on Machine Learning: Volume 97 of Proc. Machine
    Learning Research (eds Chaudhuri, K. & Salakhutdinov, R.) 2170–2179 (PMLR, 2019).'
- unstructured: van Hasselt, H., Hessel, M. & Aslanides, J. When to use parametric
    models in reinforcement learning? Preprint at https://arxiv.org/abs/1906.05243
    (2019).
- author: A Tamar
  first-page: '2154'
  journal-title: Adv. Neural Inf. Process. Syst.
  unstructured: Tamar, A., Wu, Y., Thomas, G., Levine, S. & Abbeel, P. Value iteration
    networks. Adv. Neural Inf. Process. Syst. 29, 2154–2162 (2016).
  volume: '29'
  year: '2016'
- unstructured: 'Silver, D. et al. The predictron: end-to-end learning and planning.
    In Proc. 34th International Conference on Machine Learning Vol. 70 (eds Precup,
    D. & Teh, Y. W.) 3191–3199 (JMLR, 2017).'
- unstructured: 'Farahmand, A. M., Barreto, A. & Nikovski, D. Value-aware loss function
    for model-based reinforcement learning. In Proc. 20th International Conference
    on Artificial Intelligence and Statistics: Volume 54 of Proc. Machine Learning
    Research (eds Singh, A. & Zhu, J) 1486–1494 (PMLR, 2017).'
- author: A Farahmand
  first-page: '9090'
  journal-title: Adv. Neural Inf. Process. Syst.
  unstructured: Farahmand, A. Iterative value-aware model learning. Adv. Neural Inf.
    Process. Syst. 31, 9090–9101 (2018).
  volume: '31'
  year: '2018'
- unstructured: 'Farquhar, G., Rocktaeschel, T., Igl, M. & Whiteson, S. TreeQN and
    ATreeC: differentiable tree planning for deep reinforcement learning. In International
    Conference on Learning Representations (2018).'
- author: J Oh
  first-page: '6118'
  journal-title: Adv. Neural Inf. Process. Syst.
  unstructured: Oh, J., Singh, S. & Lee, H. Value prediction network. Adv. Neural
    Inf. Process. Syst. 30, 6118–6128 (2017).
  volume: '30'
  year: '2017'
- author: A Krizhevsky
  first-page: '1097'
  journal-title: Adv. Neural Inf. Process. Syst.
  unstructured: Krizhevsky, A., Sutskever, I. & Hinton, G. E. Imagenet classification
    with deep convolutional neural networks. Adv. Neural Inf. Process. Syst. 25, 1097–1105
    (2012).
  volume: '25'
  year: '2012'
- doi: 10.1007/978-3-319-46493-0_38
  unstructured: He, K., Zhang, X., Ren, S. & Sun, J. Identity mappings in deep residual
    networks. In 14th European Conference on Computer Vision 630–645 (2016).
- doi: 10.1609/aaai.v33i01.33013796
  unstructured: 'Hessel, M. et al. Rainbow: combining improvements in deep reinforcement
    learning. In Thirty-Second AAAI Conference on Artificial Intelligence (2018).'
- unstructured: Schmitt, S., Hessel, M. & Simonyan, K. Off-policy actor-critic with
    shared experience replay. Preprint at https://arxiv.org/abs/1909.11583 (2019).
- unstructured: Azizzadenesheli, K. et al. Surprising negative results for generative
    adversarial tree search. Preprint at http://arxiv.org/abs/1806.05780 (2018).
- author: V Mnih
  doi: 10.1038/nature14236
  first-page: '529'
  journal-title: Nature
  unstructured: Mnih, V. et al. Human-level control through deep reinforcement learning.
    Nature 518, 529–533 (2015).
  volume: '518'
  year: '2015'
- unstructured: Open, A. I. OpenAI five. OpenAI https://blog.openai.com/openai-five/
    (2018).
- author: O Vinyals
  doi: 10.1038/s41586-019-1724-z
  first-page: '350'
  journal-title: Nature
  unstructured: Vinyals, O. et al. Grandmaster level in StarCraft II using multi-agent
    reinforcement learning. Nature 575, 350–354 (2019).
  volume: '575'
  year: '2019'
- unstructured: Jaderberg, M. et al. Reinforcement learning with unsupervised auxiliary
    tasks. Preprint at https://arxiv.org/abs/1611.05397 (2016).
- author: D Silver
  doi: 10.1038/nature24270
  first-page: '354'
  journal-title: Nature
  unstructured: Silver, D. et al. Mastering the game of Go without human knowledge.
    Nature 550, 354–359 (2017).
  volume: '550'
  year: '2017'
- doi: 10.1007/11871842_29
  unstructured: Kocsis, L. & Szepesvári, C. Bandit based Monte-Carlo planning. In
    European Conference on Machine Learning 282–293 (Springer, 2006).
- author: CD Rosin
  doi: 10.1007/s10472-011-9258-6
  first-page: '203'
  journal-title: Ann. Math. Artif. Intell.
  unstructured: Rosin, C. D. Multi-armed bandits with episode context. Ann. Math.
    Artif. Intell. 61, 203–230 (2011).
  volume: '61'
  year: '2011'
- doi: 10.1007/978-3-540-87608-3_1
  unstructured: Schadd, M. P., Winands, M. H., Van Den Herik, H. J., Chaslot, G. M.-B.
    & Uiterwijk, J. W. Single-player Monte-Carlo tree search. In International Conference
    on Computers and Games 1–12 (Springer, 2008).
- unstructured: 'Pohlen, T. et al. Observe and look further: achieving consistent
    performance on Atari. Preprint at https://arxiv.org/abs/1805.11593 (2018).'
- unstructured: Schaul, T., Quan, J., Antonoglou, I. & Silver, D. Prioritized experience
    replay. In International Conference on Learning Representations (2016).
- unstructured: Cloud TPU. Google Cloud https://cloud.google.com/tpu/ (2019).
- doi: 10.1007/978-3-540-87608-3_11
  unstructured: 'Coulom, R. Whole-history rating: a Bayesian rating system for players
    of time-varying strength. In International Conference on Computers and Games 113–124
    (2008).'
- unstructured: Nair, A. et al. Massively parallel methods for deep reinforcement
    learning. Preprint at https://arxiv.org/abs/1507.04296 (2015).
- unstructured: 'Lanctot, M. et al. OpenSpiel: a framework for reinforcement learning
    in games. Preprint at http://arxiv.org/abs/1908.09453 (2019).'
doc_url: http://www.nature.com/articles/s41586-020-03051-4
doi: 10.1038/s41586-020-03051-4
files:
- schrittwieser-julian-and-antonoglou-ioannis-and-hubert-thomas-and-simonyan-karen-and-sifre-laurent-and-schmitt-simon-and-guez-arthur-and-lockha.data
- schrittwieser-julian-and-antonoglou-ioannis-and-hubert-thomas-and-simonyan-karen-and-sifre-laurent-and-schmitt-simon-and-guez-arthur-and-lockha-a.pdf
issue: '7839'
journal: Nature
language: en
month: 12
pages: 604--609
publisher: Springer Science and Business Media LLC
time-added: 2021-01-17-21:17:38
title: Mastering Atari, Go, chess and shogi by planning with a learned model
type: article
url: http://dx.doi.org/10.1038/s41586-020-03051-4
volume: '588'
year: 2020

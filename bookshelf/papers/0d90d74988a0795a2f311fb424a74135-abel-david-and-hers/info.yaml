abstract: The combinatorial explosion that plagues planning and reinforcement learning
  (RL) algorithms can be moderated using state abstraction. Prohibitively large task
  representations can be condensed such that essential information is preserved, and
  consequently, solutions are tractably computable. However, exact abstractions, which
  treat only fully-identical situations as equivalent, fail to present opportunities
  for abstraction in environments where no two situations are exactly alike. In this
  work, we investigate approximate state abstractions, which treat nearly-identical
  situations as equivalent. We present theoretical guarantees of the quality of behaviors
  derived from four types of approximate abstractions. Additionally, we empirically
  demonstrate that approximate abstractions lead to reduction in task complexity and
  bounded loss of optimality of behavior in a variety of environments.
archiveprefix: arXiv
author: Abel, David and Hershkowitz, D. Ellis and Littman, Michael L.
author_list:
- family: Abel
  given: David
- family: Hershkowitz
  given: D. Ellis
- family: Littman
  given: Michael L.
eprint: 1701.04113v1
file: 1701.04113v1.pdf
files:
- abel-david-and-hershkowitz-d.-ellis-and-littman-michael-l.near-optimal-behavior-via-approximate-state-abstraction2017.pdf
month: Jan
primaryclass: cs.LG
ref: 1701.04113v1
time-added: 2021-03-19-15:20:32
title: Near Optimal Behavior via Approximate State Abstraction
type: article
url: http://arxiv.org/abs/1701.04113v1
year: '2017'

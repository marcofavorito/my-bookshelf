abstract: Existing pre-trained models are generally geared towards a particular class
  of problems. To date, there seems to be still no consensus on what the right architecture
  and pre-training setup should be. This paper presents a unified framework for pre-training
  models that are universally effective across datasets and setups. We begin by disentangling
  architectural archetypes with pre-training objectives -- two concepts that are commonly
  conflated. Next, we present a generalized and unified perspective for self-supervision
  in NLP and show how different pre-training objectives can be cast as one another
  and how interpolating between different objectives can be effective. We then propose
  Mixture-of-Denoisers (MoD), a pre-training objective that combines diverse pre-training
  paradigms together. We furthermore introduce a notion of mode switching, wherein
  downstream fine-tuning is associated with specific pre-training schemes. We conduct
  extensive ablative experiments to compare multiple pre-training objectives and find
  that our method pushes the Pareto-frontier by outperforming T5 and/or GPT-like models
  across multiple diverse setups. Finally, by scaling our model up to 20B parameters,
  we achieve SOTA performance on 50 well-established supervised NLP tasks ranging
  from language generation (with automated and human evaluation), language understanding,
  text classification, question answering, commonsense reasoning, long text reasoning,
  structured knowledge grounding and information retrieval. Our model also achieve
  strong results at in-context learning, outperforming 175B GPT-3 on zero-shot SuperGLUE
  and tripling the performance of T5-XXL on one-shot summarization. We release Flax-based
  T5X model checkpoints for the 20B model at \url{https://github.com/google-research/google-research/tree/master/ul2}.
archiveprefix: arXiv
author: Tay, Yi and Dehghani, Mostafa and Tran, Vinh Q. and Garcia, Xavier and Bahri,
  Dara and Schuster, Tal and Zheng, Huaixiu Steven and Houlsby, Neil and Metzler,
  Donald
author_list:
- family: Tay
  given: Yi
- family: Dehghani
  given: Mostafa
- family: Tran
  given: Vinh Q.
- family: Garcia
  given: Xavier
- family: Bahri
  given: Dara
- family: Schuster
  given: Tal
- family: Zheng
  given: Huaixiu Steven
- family: Houlsby
  given: Neil
- family: Metzler
  given: Donald
eprint: 2205.05131v1
file: 2205.05131v1.pdf
files:
- tay-yi-and-dehghani-mostafa-and-tran-vinh-q.-and-garcia-xavier-and-bahri-dara-and-schuster-tal-and-zheng-huaixiu-steven-and-houlsby-neil-and-m.pdf
month: May
primaryclass: cs.CL
ref: 2205.05131v1
time-added: 2022-05-13-07:21:44
title: Unifying Language Learning Paradigms
type: article
url: http://arxiv.org/abs/2205.05131v1
year: '2022'

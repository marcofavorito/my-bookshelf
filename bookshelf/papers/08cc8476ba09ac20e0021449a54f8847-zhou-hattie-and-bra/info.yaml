abstract: 'Large language models exhibit surprising emergent generalization properties,
  yet also struggle on many simple reasoning tasks such as arithmetic and parity.
  This raises the question of if and when Transformer models can learn the true algorithm
  for solving a task. We study the scope of Transformers'' abilities in the specific
  setting of length generalization on algorithmic tasks. Here, we propose a unifying
  framework to understand when and how Transformers can exhibit strong length generalization
  on a given task. Specifically, we leverage RASP (Weiss et al., 2021) -- a programming
  language designed for the computational model of a Transformer -- and introduce
  the RASP-Generalization Conjecture: Transformers tend to length generalize on a
  task if the task can be solved by a short RASP program which works for all input
  lengths. This simple conjecture remarkably captures most known instances of length
  generalization on algorithmic tasks. Moreover, we leverage our insights to drastically
  improve generalization performance on traditionally hard tasks (such as parity and
  addition). On the theoretical side, we give a simple example where the "min-degree-interpolator"
  model of learning from Abbe et al. (2023) does not correctly predict Transformers''
  out-of-distribution behavior, but our conjecture does. Overall, our work provides
  a novel perspective on the mechanisms of compositional generalization and the algorithmic
  capabilities of Transformers.'
archiveprefix: arXiv
author: Zhou, Hattie and Bradley, Arwen and Littwin, Etai and Razin, Noam and Saremi,
  Omid and Susskind, Josh and Bengio, Samy and Nakkiran, Preetum
author_list:
- family: Zhou
  given: Hattie
- family: Bradley
  given: Arwen
- family: Littwin
  given: Etai
- family: Razin
  given: Noam
- family: Saremi
  given: Omid
- family: Susskind
  given: Josh
- family: Bengio
  given: Samy
- family: Nakkiran
  given: Preetum
eprint: 2310.16028v1
file: 2310.16028v1.pdf
files:
- zhou-hattie-and-bradley-arwen-and-littwin-etai-and-razin-noam-and-saremi-omid-and-susskind-josh-and-bengio-samy-and-nakkiran-preetumwhat-algor.pdf
month: Oct
primaryclass: cs.LG
ref: 2310.16028v1
time-added: 2023-10-26-09:50:30
title: What Algorithms can Transformers Learn? A Study in Length Generalization
type: article
url: http://arxiv.org/abs/2310.16028v1
year: '2023'

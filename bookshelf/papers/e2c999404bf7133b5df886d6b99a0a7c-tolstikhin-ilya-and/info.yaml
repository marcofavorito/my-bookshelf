abstract: 'Convolutional Neural Networks (CNNs) are the go-to model for computer vision.
  Recently, attention-based networks, such as the Vision Transformer, have also become
  popular. In this paper we show that while convolutions and attention are both sufficient
  for good performance, neither of them are necessary. We present MLP-Mixer, an architecture
  based exclusively on multi-layer perceptrons (MLPs). MLP-Mixer contains two types
  of layers: one with MLPs applied independently to image patches (i.e. "mixing" the
  per-location features), and one with MLPs applied across patches (i.e. "mixing"
  spatial information). When trained on large datasets, or with modern regularization
  schemes, MLP-Mixer attains competitive scores on image classification benchmarks,
  with pre-training and inference cost comparable to state-of-the-art models. We hope
  that these results spark further research beyond the realms of well established
  CNNs and Transformers.'
archiveprefix: arXiv
author: Tolstikhin, Ilya and Houlsby, Neil and Kolesnikov, Alexander and Beyer, Lucas
  and Zhai, Xiaohua and Unterthiner, Thomas and Yung, Jessica and Keysers, Daniel
  and Uszkoreit, Jakob and Lucic, Mario and Dosovitskiy, Alexey
author_list:
- family: Tolstikhin
  given: Ilya
- family: Houlsby
  given: Neil
- family: Kolesnikov
  given: Alexander
- family: Beyer
  given: Lucas
- family: Zhai
  given: Xiaohua
- family: Unterthiner
  given: Thomas
- family: Yung
  given: Jessica
- family: Keysers
  given: Daniel
- family: Uszkoreit
  given: Jakob
- family: Lucic
  given: Mario
- family: Dosovitskiy
  given: Alexey
eprint: 2105.01601v1
file: 2105.01601v1.pdf
files:
- tolstikhin-ilya-and-houlsby-neil-and-kolesnikov-alexander-and-beyer-lucas-and-zhai-xiaohua-and-unterthiner-thomas-and-yung-jessica-and-keysers.pdf
month: May
primaryclass: cs.CV
ref: 2105.01601v1
time-added: 2021-05-11-09:21:02
title: 'MLP-Mixer: An all-MLP Architecture for Vision'
type: article
url: http://arxiv.org/abs/2105.01601v1
year: '2021'

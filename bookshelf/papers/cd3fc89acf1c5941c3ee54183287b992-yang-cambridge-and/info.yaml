abstract: In recent years, researchers have made significant progress in devising
  reinforcement-learning algorithms for optimizing linear temporal logic (LTL) objectives
  and LTL-like objectives. Despite these advancements, there are fundamental limitations
  to how well this problem can be solved. Previous studies have alluded to this fact
  but have not examined it in depth. In this paper, we address the tractability of
  reinforcement learning for general LTL objectives from a theoretical perspective.
  We formalize the problem under the probably approximately correct learning in Markov
  decision processes (PAC-MDP) framework, a standard framework for measuring sample
  complexity in reinforcement learning. In this formalization, we prove that the optimal
  policy for any LTL formula is PAC-MDP-learnable if and only if the formula is in
  the most limited class in the LTL hierarchy, consisting of formulas that are decidable
  within a finite horizon. Practically, our result implies that it is impossible for
  a reinforcement-learning algorithm to obtain a PAC-MDP guarantee on the performance
  of its learned policy after finitely many interactions with an unconstrained environment
  for LTL objectives that are not decidable within a finite horizon.
archiveprefix: arXiv
author: Yang, Cambridge and Littman, Michael and Carbin, Michael
author_list:
- family: Yang
  given: Cambridge
- family: Littman
  given: Michael
- family: Carbin
  given: Michael
eprint: 2111.12679v3
file: 2111.12679v3.pdf
files:
- yang-cambridge-and-littman-michael-and-carbin-michaelon-the-in-tractability-of-reinforcement-learning-for-ltl-objectives2021.pdf
month: Nov
primaryclass: cs.AI
ref: 2111.12679v3
time-added: 2023-04-03-19:55:50
title: On the (In)Tractability of Reinforcement Learning for LTL Objectives
type: article
url: http://arxiv.org/abs/2111.12679v3
year: '2021'

abstract: Simple recurrent neural networks (RNNs) and their more advanced cousins
  LSTMs etc. have been very successful in sequence modeling. Their theoretical understanding,
  however, is lacking and has not kept pace with the progress for feedforward networks,
  where a reasonably complete understanding in the special case of highly overparametrized
  one-hidden-layer networks has emerged. In this paper, we make progress towards remedying
  this situation by proving that RNNs can learn functions of sequences. In contrast
  to the previous work that could only deal with functions of sequences that are sums
  of functions of individual tokens in the sequence, we allow general functions. Conceptually
  and technically, we introduce new ideas which enable us to extract information from
  the hidden state of the RNN in our proofs -- addressing a crucial weakness in previous
  work. We illustrate our results on some regular language recognition problems.
archiveprefix: arXiv
author: Panigrahi, Abhishek and Goyal, Navin
author_list:
- family: Panigrahi
  given: Abhishek
- family: Goyal
  given: Navin
eprint: 2106.00047v1
file: 2106.00047v1.pdf
files:
- panigrahi-abhishek-and-goyal-navinlearning-and-generalization-in-rnns2021.pdf
month: May
primaryclass: cs.LG
ref: 2106.00047v1
time-added: 2023-03-16-22:55:17
title: Learning and Generalization in RNNs
type: article
url: http://arxiv.org/abs/2106.00047v1
year: '2021'

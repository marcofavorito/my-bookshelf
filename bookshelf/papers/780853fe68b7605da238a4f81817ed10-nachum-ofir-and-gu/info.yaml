abstract: Hierarchical reinforcement learning (HRL) is a promising approach to extend
  traditional reinforcement learning (RL) methods to solve more complex tasks. Yet,
  the majority of current HRL methods require careful task-specific design and on-policy
  training, making them difficult to apply in real-world scenarios. In this paper,
  we study how we can develop HRL algorithms that are general, in that they do not
  make onerous additional assumptions beyond standard RL algorithms, and efficient,
  in the sense that they can be used with modest numbers of interaction samples, making
  them suitable for real-world problems such as robotic control. For generality, we
  develop a scheme where lower-level controllers are supervised with goals that are
  learned and proposed automatically by the higher-level controllers. To address efficiency,
  we propose to use off-policy experience for both higher and lower-level training.
  This poses a considerable challenge, since changes to the lower-level behaviors
  change the action space for the higher-level policy, and we introduce an off-policy
  correction to remedy this challenge. This allows us to take advantage of recent
  advances in off-policy model-free RL to learn both higher- and lower-level policies
  using substantially fewer environment interactions than on-policy algorithms. We
  term the resulting HRL agent HIRO and find that it is generally applicable and highly
  sample-efficient. Our experiments show that HIRO can be used to learn highly complex
  behaviors for simulated robots, such as pushing objects and utilizing them to reach
  target locations, learning from only a few million samples, equivalent to a few
  days of real-time interaction. In comparisons with a number of prior HRL methods,
  we find that our approach substantially outperforms previous state-of-the-art techniques.
archiveprefix: arXiv
author: Nachum, Ofir and Gu, Shixiang and Lee, Honglak and Levine, Sergey
author_list:
- family: Nachum
  given: Ofir
- family: Gu
  given: Shixiang
- family: Lee
  given: Honglak
- family: Levine
  given: Sergey
eprint: 1805.08296v4
file: 1805.08296v4.pdf
files:
- nachum-ofir-and-gu-shixiang-and-lee-honglak-and-levine-sergeydata-efficient-hierarchical-reinforcement-learning2018.pdf
month: May
primaryclass: cs.LG
ref: 1805.08296v4
time-added: 2020-12-01-19:45:09
title: Data-Efficient Hierarchical Reinforcement Learning
type: article
url: http://arxiv.org/abs/1805.08296v4
year: '2018'

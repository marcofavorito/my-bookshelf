abstract: We study empirical scaling laws for language model performance on the cross-entropy
  loss. The loss scales as a power-law with model size, dataset size, and the amount
  of compute used for training, with some trends spanning more than seven orders of
  magnitude. Other architectural details such as network width or depth have minimal
  effects within a wide range. Simple equations govern the dependence of overfitting
  on model/dataset size and the dependence of training speed on model size. These
  relationships allow us to determine the optimal allocation of a fixed compute budget.
  Larger models are significantly more sample-efficient, such that optimally compute-efficient
  training involves training very large models on a relatively modest amount of data
  and stopping significantly before convergence.
archiveprefix: arXiv
author: Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B. and
  Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey
  and Amodei, Dario
author_list:
- family: Kaplan
  given: Jared
- family: McCandlish
  given: Sam
- family: Henighan
  given: Tom
- family: Brown
  given: Tom B.
- family: Chess
  given: Benjamin
- family: Child
  given: Rewon
- family: Gray
  given: Scott
- family: Radford
  given: Alec
- family: Wu
  given: Jeffrey
- family: Amodei
  given: Dario
eprint: 2001.08361v1
file: 2001.08361v1.pdf
files:
- kaplan-jared-and-mccandlish-sam-and-henighan-tom-and-brown-tom-b.-and-chess-benjamin-and-child-rewon-and-gray-scott-and-radford-alec-and-wu-j.pdf
month: Jan
primaryclass: cs.LG
ref: 2001.08361v1
time-added: 2023-05-29-12:08:06
title: Scaling Laws for Neural Language Models
type: article
url: http://arxiv.org/abs/2001.08361v1
year: '2020'

abstract: Deep generative models have been wildly successful at learning coherent
  latent representations for continuous data such as video and audio. However, generative
  modeling of discrete data such as arithmetic expressions and molecular structures
  still poses significant challenges. Crucially, state-of-the-art methods often produce
  outputs that are not valid. We make the key observation that frequently, discrete
  data can be represented as a parse tree from a context-free grammar. We propose
  a variational autoencoder which encodes and decodes directly to and from these parse
  trees, ensuring the generated outputs are always valid. Surprisingly, we show that
  not only does our model more often generate valid outputs, it also learns a more
  coherent latent space in which nearby points decode to similar discrete outputs.
  We demonstrate the effectiveness of our learned models by showing their improved
  performance in Bayesian optimization for symbolic regression and molecular synthesis.
archiveprefix: arXiv
author: Kusner, Matt J. and Paige, Brooks and Hernández-Lobato, José Miguel
author_list:
- family: Kusner
  given: Matt J.
- family: Paige
  given: Brooks
- family: Hernández-Lobato
  given: José Miguel
eprint: 1703.01925v1
file: 1703.01925v1.pdf
files:
- kusner-matt-j.-and-paige-brooks-and-hernandez-lobato-jose-miguelgrammar-variational-autoencoder2017.pdf
month: Mar
primaryclass: stat.ML
ref: 1703.01925v1
time-added: 2020-07-12-00:52:33
title: Grammar Variational Autoencoder
type: article
url: http://arxiv.org/abs/1703.01925v1
year: '2017'

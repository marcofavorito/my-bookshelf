abstract: 'We show for the first time that large-scale generative pretrained transformer
  (GPT) family models can be pruned to at least 50% sparsity in one-shot, without
  any retraining, at minimal loss of accuracy. This is achieved via a new pruning
  method called SparseGPT, specifically designed to work efficiently and accurately
  on massive GPT-family models. We can execute SparseGPT on the largest available
  open-source models, OPT-175B and BLOOM-176B, in under 4.5 hours, and can reach 60%
  unstructured sparsity with negligible increase in perplexity: remarkably, more than
  100 billion weights from these models can be ignored at inference time. SparseGPT
  generalizes to semi-structured (2:4 and 4:8) patterns, and is compatible with weight
  quantization approaches. The code is available at: https://github.com/IST-DASLab/sparsegpt.'
archiveprefix: arXiv
author: Frantar, Elias and Alistarh, Dan
author_list:
- family: Frantar
  given: Elias
- family: Alistarh
  given: Dan
eprint: 2301.00774v3
file: 2301.00774v3.pdf
files:
- frantar-elias-and-alistarh-dansparsegpt-massive-language-models-can-be-accurately-pruned-in-one-shot2023.pdf
month: Jan
primaryclass: cs.LG
ref: 2301.00774v3
time-added: 2023-03-23-14:22:07
title: 'SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot'
type: article
url: http://arxiv.org/abs/2301.00774v3
year: '2023'

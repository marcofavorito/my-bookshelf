abstract: Reinforcement learning from human feedback (RLHF) has emerged as a powerful
  technique to make large language models (LLMs) easier to use and more effective.
  A core piece of the RLHF process is the training and utilization of a model of human
  preferences that acts as a reward function for optimization. This approach, which
  operates at the intersection of many stakeholders and academic disciplines, remains
  poorly understood. RLHF reward models are often cited as being central to achieving
  performance, yet very few descriptors of capabilities, evaluations, training methods,
  or open-source models exist. Given this lack of information, further study and transparency
  is needed for learned RLHF reward models. In this paper, we illustrate the complex
  history of optimizing preferences, and articulate lines of inquiry to understand
  the sociotechnical context of reward models. In particular, we highlight the ontological
  differences between costs, rewards, and preferences at stake in RLHF's foundations,
  related methodological tensions, and possible research directions to improve general
  understanding of how reward models function.
archiveprefix: arXiv
author: Lambert, Nathan and Gilbert, Thomas Krendl and Zick, Tom
author_list:
- family: Lambert
  given: Nathan
- family: Gilbert
  given: Thomas Krendl
- family: Zick
  given: Tom
eprint: 2310.13595v1
file: 2310.13595v1.pdf
files:
- lambert-nathan-and-gilbert-thomas-krendl-and-zick-tomentangled-preferences-the-history-and-risks-of-reinforcement-learning-and-human-feedback202.pdf
month: Oct
primaryclass: cs.CY
ref: 2310.13595v1
time-added: 2023-10-24-14:02:43
title: 'Entangled Preferences: The History and Risks of Reinforcement Learning   and
  Human Feedback'
type: article
url: http://arxiv.org/abs/2310.13595v1
year: '2023'

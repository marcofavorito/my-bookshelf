abstract: The aim of this paper is to introduce a new learning procedure for neural
  networks and to demonstrate that it works well enough on a few small problems to
  be worth further investigation. The Forward-Forward algorithm replaces the forward
  and backward passes of backpropagation by two forward passes, one with positive
  (i.e. real) data and the other with negative data which could be generated by the
  network itself. Each layer has its own objective function which is simply to have
  high goodness for positive data and low goodness for negative data. The sum of the
  squared activities in a layer can be used as the goodness but there are many other
  possibilities, including minus the sum of the squared activities. If the positive
  and negative passes could be separated in time, the negative passes could be done
  offline, which would make the learning much simpler in the positive pass and allow
  video to be pipelined through the network without ever storing activities or stopping
  to propagate derivatives.
archiveprefix: arXiv
author: Hinton, Geoffrey
author_list:
- family: Hinton
  given: Geoffrey
eprint: 2212.13345v1
file: 2212.13345v1.pdf
files:
- hinton-geoffreythe-forward-forward-algorithm-some-preliminary-investigations2022.pdf
month: Dec
primaryclass: cs.LG
ref: 2212.13345v1
time-added: 2023-03-08-13:49:38
title: 'The Forward-Forward Algorithm: Some Preliminary Investigations'
type: article
url: http://arxiv.org/abs/2212.13345v1
year: '2022'

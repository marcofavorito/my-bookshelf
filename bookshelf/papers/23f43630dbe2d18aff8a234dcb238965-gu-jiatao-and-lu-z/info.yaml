abstract: We address an important problem in sequence-to-sequence (Seq2Seq) learning
  referred to as copying, in which certain segments in the input sequence are selectively
  replicated in the output sequence. A similar phenomenon is observable in human language
  communication. For example, humans tend to repeat entity names or even long phrases
  in conversation. The challenge with regard to copying in Seq2Seq is that new machinery
  is needed to decide when to perform the operation. In this paper, we incorporate
  copying into neural network-based Seq2Seq learning and propose a new model called
  CopyNet with encoder-decoder structure. CopyNet can nicely integrate the regular
  way of word generation in the decoder with the new copying mechanism which can choose
  sub-sequences in the input sequence and put them at proper places in the output
  sequence. Our empirical study on both synthetic data sets and real world data sets
  demonstrates the efficacy of CopyNet. For example, CopyNet can outperform regular
  RNN-based model with remarkable margins on text summarization tasks.
archiveprefix: arXiv
author: Gu, Jiatao and Lu, Zhengdong and Li, Hang and Li, Victor O. K.
author_list:
- family: Gu
  given: Jiatao
- family: Lu
  given: Zhengdong
- family: Li
  given: Hang
- family: Li
  given: Victor O. K.
eprint: 1603.06393v3
file: 1603.06393v3.pdf
files:
- gu-jiatao-and-lu-zhengdong-and-li-hang-and-li-victor-o.-k.incorporating-copying-mechanism-in-sequence-to-sequence-learning2016.pdf
month: Mar
primaryclass: cs.CL
ref: 1603.06393v3
title: Incorporating Copying Mechanism in Sequence-to-Sequence Learning
type: article
url: http://arxiv.org/abs/1603.06393v3
year: '2016'

abstract: We present a series of long-context LLMs that support effective context
  windows of up to 32,768 tokens. Our model series are built through continual pretraining
  from Llama 2 with longer training sequences and on a dataset where long texts are
  upsampled. We perform extensive evaluation on language modeling, synthetic context
  probing tasks, and a wide range of research benchmarks. On research benchmarks,
  our models achieve consistent improvements on most regular tasks and significant
  improvements on long-context tasks over Llama 2. Notably, with a cost-effective
  instruction tuning procedure that does not require human-annotated long instruction
  data, the 70B variant can already surpass gpt-3.5-turbo-16k's overall performance
  on a suite of long-context tasks. Alongside these results, we provide an in-depth
  analysis on the individual components of our method. We delve into Llama's position
  encodings and discuss its limitation in modeling long dependencies. We also examine
  the impact of various design choices in the pretraining process, including the data
  mix and the training curriculum of sequence lengths -- our ablation experiments
  suggest that having abundant long texts in the pretrain dataset is not the key to
  achieving strong performance, and we empirically verify that long context continual
  pretraining is more efficient and similarly effective compared to pretraining from
  scratch with long sequences.
archiveprefix: arXiv
author: Xiong, Wenhan and Liu, Jingyu and Molybog, Igor and Zhang, Hejia and Bhargava,
  Prajjwal and Hou, Rui and Martin, Louis and Rungta, Rashi and Sankararaman, Karthik
  Abinav and Oguz, Barlas and Khabsa, Madian and Fang, Han and Mehdad, Yashar and
  Narang, Sharan and Malik, Kshitiz and Fan, Angela and Bhosale, Shruti and Edunov,
  Sergey and Lewis, Mike and Wang, Sinong and Ma, Hao
author_list:
- family: Xiong
  given: Wenhan
- family: Liu
  given: Jingyu
- family: Molybog
  given: Igor
- family: Zhang
  given: Hejia
- family: Bhargava
  given: Prajjwal
- family: Hou
  given: Rui
- family: Martin
  given: Louis
- family: Rungta
  given: Rashi
- family: Sankararaman
  given: Karthik Abinav
- family: Oguz
  given: Barlas
- family: Khabsa
  given: Madian
- family: Fang
  given: Han
- family: Mehdad
  given: Yashar
- family: Narang
  given: Sharan
- family: Malik
  given: Kshitiz
- family: Fan
  given: Angela
- family: Bhosale
  given: Shruti
- family: Edunov
  given: Sergey
- family: Lewis
  given: Mike
- family: Wang
  given: Sinong
- family: Ma
  given: Hao
eprint: 2309.16039v1
file: 2309.16039v1.pdf
files:
- xiong-wenhan-and-liu-jingyu-and-molybog-igor-and-zhang-hejia-and-bhargava-prajjwal-and-hou-rui-and-martin-louis-and-rungta-rashi-and-sankarara.pdf
month: Sep
primaryclass: cs.CL
ref: 2309.16039v1
time-added: 2023-09-30-21:02:14
title: Effective Long-Context Scaling of Foundation Models
type: article
url: http://arxiv.org/abs/2309.16039v1
year: '2023'

abstract: Boltzmann exploration is a classic strategy for sequential decision-making
  under uncertainty, and is one of the most standard tools in Reinforcement Learning
  (RL). Despite its widespread use, there is virtually no theoretical understanding
  about the limitations or the actual benefits of this exploration scheme. Does it
  drive exploration in a meaningful way? Is it prone to misidentifying the optimal
  actions or spending too much time exploring the suboptimal ones? What is the right
  tuning for the learning rate? In this paper, we address several of these questions
  in the classic setup of stochastic multi-armed bandits. One of our main results
  is showing that the Boltzmann exploration strategy with any monotone learning-rate
  sequence will induce suboptimal behavior. As a remedy, we offer a simple non-monotone
  schedule that guarantees near-optimal performance, albeit only when given prior
  access to key problem parameters that are typically not available in practical situations
  (like the time horizon $T$ and the suboptimality gap $\Delta$). More importantly,
  we propose a novel variant that uses different learning rates for different arms,
  and achieves a distribution-dependent regret bound of order $\frac{K\log^2 T}{\Delta}$
  and a distribution-independent bound of order $\sqrt{KT}\log K$ without requiring
  such prior knowledge. To demonstrate the flexibility of our technique, we also propose
  a variant that guarantees the same performance bounds even if the rewards are heavy-tailed.
archiveprefix: arXiv
author: Cesa-Bianchi, Nicolò and Gentile, Claudio and Lugosi, Gábor and Neu, Gergely
author_list:
- family: Cesa-Bianchi
  given: Nicolò
- family: Gentile
  given: Claudio
- family: Lugosi
  given: Gábor
- family: Neu
  given: Gergely
eprint: 1705.10257v2
file: 1705.10257v2.pdf
files:
- cesa-bianchi-nicolo-and-gentile-claudio-and-lugosi-gabor-and-neu-gergelyboltzmann-exploration-done-right2017.pdf
month: May
primaryclass: cs.LG
ref: 1705.10257v2
title: Boltzmann Exploration Done Right
type: article
url: http://arxiv.org/abs/1705.10257v2
year: '2017'

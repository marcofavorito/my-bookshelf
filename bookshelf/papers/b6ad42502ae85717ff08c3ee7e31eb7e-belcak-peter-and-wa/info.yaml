abstract: We break the linear link between the layer size and its inference cost by
  introducing the fast feedforward (FFF) architecture, a log-time alternative to feedforward
  networks. We demonstrate that FFFs are up to 220x faster than feedforward networks,
  up to 6x faster than mixture-of-experts networks, and exhibit better training properties
  than mixtures of experts thanks to noiseless conditional execution. Pushing FFFs
  to the limit, we show that they can use as little as 1% of layer neurons for inference
  in vision transformers while preserving 94.2% of predictive performance.
archiveprefix: arXiv
author: Belcak, Peter and Wattenhofer, Roger
author_list:
- family: Belcak
  given: Peter
- family: Wattenhofer
  given: Roger
eprint: 2308.14711v2
file: 2308.14711v2.pdf
files:
- belcak-peter-and-wattenhofer-rogerfast-feedforward-networks2023.pdf
month: Aug
primaryclass: cs.LG
ref: 2308.14711v2
time-added: 2023-09-21-19:21:38
title: Fast Feedforward Networks
type: article
url: http://arxiv.org/abs/2308.14711v2
year: '2023'

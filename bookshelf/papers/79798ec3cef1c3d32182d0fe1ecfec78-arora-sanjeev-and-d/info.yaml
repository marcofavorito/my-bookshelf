abstract: How well does a classic deep net architecture like AlexNet or VGG19 classify
  on a standard dataset such as CIFAR-10 when its width --- namely, number of channels
  in convolutional layers, and number of nodes in fully-connected internal layers
  --- is allowed to increase to infinity? Such questions have come to the forefront
  in the quest to theoretically understand deep learning and its mysteries about optimization
  and generalization. They also connect deep learning to notions such as Gaussian
  processes and kernels. A recent paper [Jacot et al., 2018] introduced the Neural
  Tangent Kernel (NTK) which captures the behavior of fully-connected deep nets in
  the infinite width limit trained by gradient descent; this object was implicit in
  some other recent papers. An attraction of such ideas is that a pure kernel-based
  method is used to capture the power of a fully-trained deep net of infinite width.   The
  current paper gives the first efficient exact algorithm for computing the extension
  of NTK to convolutional neural nets, which we call Convolutional NTK (CNTK), as
  well as an efficient GPU implementation of this algorithm. This results in a significant
  new benchmark for the performance of a pure kernel-based method on CIFAR-10, being
  $10\%$ higher than the methods reported in [Novak et al., 2019], and only $6\%$
  lower than the performance of the corresponding finite deep net architecture (once
  batch normalization, etc. are turned off). Theoretically, we also give the first
  non-asymptotic proof showing that a fully-trained sufficiently wide net is indeed
  equivalent to the kernel regression predictor using NTK.
archiveprefix: arXiv
author: Arora, Sanjeev and Du, Simon S. and Hu, Wei and Li, Zhiyuan and Salakhutdinov,
  Ruslan and Wang, Ruosong
author_list:
- family: Arora
  given: Sanjeev
- family: Du
  given: Simon S.
- family: Hu
  given: Wei
- family: Li
  given: Zhiyuan
- family: Salakhutdinov
  given: Ruslan
- family: Wang
  given: Ruosong
eprint: 1904.11955v2
file: 1904.11955v2.pdf
files:
- arora-sanjeev-and-du-simon-s.-and-hu-wei-and-li-zhiyuan-and-salakhutdinov-ruslan-and-wang-ruosongon-exact-computation-with-an-infinitely-wide-ne.pdf
month: Apr
primaryclass: cs.LG
ref: 1904.11955v2
time-added: 2020-06-21-12:51:26
title: On Exact Computation with an Infinitely Wide Neural Net
type: article
url: http://arxiv.org/abs/1904.11955v2
year: '2019'

abstract: Sequential data is being generated at an unprecedented pace in various forms,
  including text and genomic data. This creates the need for efficient compression
  mechanisms to enable better storage, transmission and processing of such data. To
  solve this problem, many of the existing compressors attempt to learn models for
  the data and perform prediction-based compression. Since neural networks are known
  as universal function approximators with the capability to learn arbitrarily complex
  mappings, and in practice show excellent performance in prediction tasks, we explore
  and devise methods to compress sequential data using neural network predictors.
  We combine recurrent neural network predictors with an arithmetic coder and losslessly
  compress a variety of synthetic, text and genomic datasets. The proposed compressor
  outperforms Gzip on the real datasets and achieves near-optimal compression for
  the synthetic datasets. The results also help understand why and where neural networks
  are good alternatives for traditional finite context models
archiveprefix: arXiv
author: Goyal, Mohit and Tatwawadi, Kedar and Chandak, Shubham and Ochoa, Idoia
author_list:
- family: Goyal
  given: Mohit
- family: Tatwawadi
  given: Kedar
- family: Chandak
  given: Shubham
- family: Ochoa
  given: Idoia
eprint: 1811.08162v1
file: 1811.08162v1.pdf
files:
- goyal-mohit-and-tatwawadi-kedar-and-chandak-shubham-and-ochoa-idoiadeepzip-lossless-data-compression-using-recurrent-neural-networks2018.pdf
month: Nov
primaryclass: cs.CL
ref: 1811.08162v1
time-added: 2020-06-05-21:49:44
title: 'DeepZip: Lossless Data Compression using Recurrent Neural Networks'
type: article
url: http://arxiv.org/abs/1811.08162v1
year: '2018'

abstract: We introduce a new family of deep neural network models. Instead of specifying
  a discrete sequence of hidden layers, we parameterize the derivative of the hidden
  state using a neural network. The output of the network is computed using a black-box
  differential equation solver. These continuous-depth models have constant memory
  cost, adapt their evaluation strategy to each input, and can explicitly trade numerical
  precision for speed. We demonstrate these properties in continuous-depth residual
  networks and continuous-time latent variable models. We also construct continuous
  normalizing flows, a generative model that can train by maximum likelihood, without
  partitioning or ordering the data dimensions. For training, we show how to scalably
  backpropagate through any ODE solver, without access to its internal operations.
  This allows end-to-end training of ODEs within larger models.
archiveprefix: arXiv
author: Chen, Ricky T. Q. and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud,
  David
author_list:
- family: Chen
  given: Ricky T. Q.
- family: Rubanova
  given: Yulia
- family: Bettencourt
  given: Jesse
- family: Duvenaud
  given: David
eprint: 1806.07366v5
file: 1806.07366v5.pdf
files:
- chen-ricky-t.-q.-and-rubanova-yulia-and-bettencourt-jesse-and-duvenaud-davidneural-ordinary-differential-equations2018.pdf
month: Jun
primaryclass: cs.LG
ref: 1806.07366v5
time-added: 2023-09-26-11:32:32
title: Neural Ordinary Differential Equations
type: article
url: http://arxiv.org/abs/1806.07366v5
year: '2018'

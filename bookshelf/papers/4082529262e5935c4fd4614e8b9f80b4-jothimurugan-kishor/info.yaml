abstract: 'Compositional reinforcement learning is a promising approach for training
  policies to perform complex long-horizon tasks. Typically, a high-level task is
  decomposed into a sequence of subtasks and a separate policy is trained to perform
  each subtask. In this paper, we focus on the problem of training subtask policies
  in a way that they can be used to perform any task; here, a task is given by a sequence
  of subtasks. We aim to maximize the worst-case performance over all tasks as opposed
  to the average-case performance. We formulate the problem as a two agent zero-sum
  game in which the adversary picks the sequence of subtasks. We propose two RL algorithms
  to solve this game: one is an adaptation of existing multi-agent RL algorithms to
  our setting and the other is an asynchronous version which enables parallel training
  of subtask policies. We evaluate our approach on two multi-task environments with
  continuous states and actions and demonstrate that our algorithms outperform state-of-the-art
  baselines.'
archiveprefix: arXiv
author: Jothimurugan, Kishor and Hsu, Steve and Bastani, Osbert and Alur, Rajeev
author_list:
- family: Jothimurugan
  given: Kishor
- family: Hsu
  given: Steve
- family: Bastani
  given: Osbert
- family: Alur
  given: Rajeev
eprint: 2302.02984v1
file: 2302.02984v1.pdf
files:
- jothimurugan-kishor-and-hsu-steve-and-bastani-osbert-and-alur-rajeevrobust-subtask-learning-for-compositional-generalization2023.pdf
month: Feb
primaryclass: cs.LG
ref: 2302.02984v1
time-added: 2023-02-09-10:40:57
title: Robust Subtask Learning for Compositional Generalization
type: article
url: http://arxiv.org/abs/2302.02984v1
year: '2023'

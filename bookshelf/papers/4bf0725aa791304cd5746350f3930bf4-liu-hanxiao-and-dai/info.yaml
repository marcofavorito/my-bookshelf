abstract: Transformers have become one of the most important architectural innovations
  in deep learning and have enabled many breakthroughs over the past few years. Here
  we propose a simple attention-free network architecture, gMLP, based solely on MLPs
  with gating, and show that it can perform as well as Transformers in key language
  and vision applications. Our comparisons show that self-attention is not critical
  for Vision Transformers, as gMLP can achieve the same accuracy. For BERT, our model
  achieves parity with Transformers on pretraining perplexity and is better on some
  downstream tasks. On finetuning tasks where gMLP performs worse, making the gMLP
  model substantially larger can close the gap with Transformers. In general, our
  experiments show that gMLP can scale as well as Transformers over increased data
  and compute.
archiveprefix: arXiv
author: Liu, Hanxiao and Dai, Zihang and So, David R. and Le, Quoc V.
author_list:
- family: Liu
  given: Hanxiao
- family: Dai
  given: Zihang
- family: So
  given: David R.
- family: Le
  given: Quoc V.
eprint: 2105.08050v1
file: 2105.08050v1.pdf
files:
- liu-hanxiao-and-dai-zihang-and-so-david-r.-and-le-quoc-v.pay-attention-to-mlps2021.pdf
month: May
primaryclass: cs.LG
ref: 2105.08050v1
time-added: 2021-05-25-18:07:34
title: Pay Attention to MLPs
type: article
url: http://arxiv.org/abs/2105.08050v1
year: '2021'

abstract: Deep reinforcement learning (DRL) has gained great success by learning directly
  from high-dimensional sensory inputs, yet is notorious for the lack of interpretability.
  Interpretability of the subtasks is critical in hierarchical decision-making as
  it increases the transparency of black-box-style DRL approach and helps the RL practitioners
  to understand the high-level behavior of the system better. In this paper, we introduce
  symbolic planning into DRL and propose a framework of Symbolic Deep Reinforcement
  Learning (SDRL) that can handle both high-dimensional sensory inputs and symbolic
  planning. The task-level interpretability is enabled by relating symbolic actions
  to options.This framework features a planner -- controller -- meta-controller architecture,
  which takes charge of subtask scheduling, data-driven subtask learning, and subtask
  evaluation, respectively. The three components cross-fertilize each other and eventually
  converge to an optimal symbolic plan along with the learned subtasks, bringing together
  the advantages of long-term planning capability with symbolic knowledge and end-to-end
  reinforcement learning directly from a high-dimensional sensory input. Experimental
  results validate the interpretability of subtasks, along with improved data efficiency
  compared with state-of-the-art approaches.
archiveprefix: arXiv
author: Lyu, Daoming and Yang, Fangkai and Liu, Bo and Gustafson, Steven
author_list:
- family: Lyu
  given: Daoming
- family: Yang
  given: Fangkai
- family: Liu
  given: Bo
- family: Gustafson
  given: Steven
eprint: 1811.00090v4
file: 1811.00090v4.pdf
files:
- lyu-daoming-and-yang-fangkai-and-liu-bo-and-gustafson-stevensdrl-interpretable-and-data-efficient-deep-reinforcement-learning-leveraging-symbol.pdf
month: Oct
primaryclass: cs.AI
ref: 1811.00090v4
time-added: 2021-04-11-09:52:02
title: 'SDRL: Interpretable and Data-efficient Deep Reinforcement Learning   Leveraging
  Symbolic Planning'
type: article
url: http://arxiv.org/abs/1811.00090v4
year: '2018'

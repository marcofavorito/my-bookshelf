abstract: While large language models (LLMs) have demonstrated impressive performance
  on a range of decision-making tasks, they rely on simple acting processes and fall
  short of broad deployment as autonomous agents. We introduce LATS (Language Agent
  Tree Search), a general framework that synergizes the capabilities of LLMs in planning,
  acting, and reasoning. Drawing inspiration from Monte Carlo tree search in model-based
  reinforcement learning, LATS employs LLMs as agents, value functions, and optimizers,
  repurposing their latent strengths for enhanced decision-making. What is crucial
  in this method is the use of an environment for external feedback, which offers
  a more deliberate and adaptive problem-solving mechanism that moves beyond the limitations
  of existing techniques. Our experimental evaluation across diverse domains, such
  as programming, HotPotQA, and WebShop, illustrates the applicability of LATS for
  both reasoning and acting. In particular, LATS achieves 94.4\% for programming on
  HumanEval with GPT-4 and an average score of 75.9 for web browsing on WebShop with
  GPT-3.5, demonstrating the effectiveness and generality of our method.
archiveprefix: arXiv
author: Zhou, Andy and Yan, Kai and Shlapentokh-Rothman, Michal and Wang, Haohan and
  Wang, Yu-Xiong
author_list:
- family: Zhou
  given: Andy
- family: Yan
  given: Kai
- family: Shlapentokh-Rothman
  given: Michal
- family: Wang
  given: Haohan
- family: Wang
  given: Yu-Xiong
eprint: 2310.04406v1
file: 2310.04406v1.pdf
files:
- zhou-andy-and-yan-kai-and-shlapentokh-rothman-michal-and-wang-haohan-and-wang-yu-xionglanguage-agent-tree-search-unifies-reasoning-acting-and-pla.pdf
month: Oct
primaryclass: cs.AI
ref: 2310.04406v1
time-added: 2023-10-09-18:03:07
title: Language Agent Tree Search Unifies Reasoning Acting and Planning in   Language
  Models
type: article
url: http://arxiv.org/abs/2310.04406v1
year: '2023'

abstract: An important paradigm of natural language processing consists of large-scale
  pre-training on general domain data and adaptation to particular tasks or domains.
  As we pre-train larger models, full fine-tuning, which retrains all model parameters,
  becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances
  of fine-tuned models, each with 175B parameters, is prohibitively expensive. We
  propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights
  and injects trainable rank decomposition matrices into each layer of the Transformer
  architecture, greatly reducing the number of trainable parameters for downstream
  tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of
  trainable parameters by 10,000 times and the GPU memory requirement by 3 times.
  LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa,
  GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput,
  and, unlike adapters, no additional inference latency. We also provide an empirical
  investigation into rank-deficiency in language model adaptation, which sheds light
  on the efficacy of LoRA. We release a package that facilitates the integration of
  LoRA with PyTorch models and provide our implementations and model checkpoints for
  RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.
archiveprefix: arXiv
author: Hu, Edward J. and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and
  Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu
author_list:
- family: Hu
  given: Edward J.
- family: Shen
  given: Yelong
- family: Wallis
  given: Phillip
- family: Allen-Zhu
  given: Zeyuan
- family: Li
  given: Yuanzhi
- family: Wang
  given: Shean
- family: Wang
  given: Lu
- family: Chen
  given: Weizhu
eprint: 2106.09685v2
file: 2106.09685v2.pdf
files:
- hu-edward-j.-and-shen-yelong-and-wallis-phillip-and-allen-zhu-zeyuan-and-li-yuanzhi-and-wang-shean-and-wang-lu-and-chen-weizhulora-low-rank-a.pdf
month: Jun
primaryclass: cs.CL
ref: 2106.09685v2
time-added: 2023-09-07-11:27:50
title: 'LoRA: Low-Rank Adaptation of Large Language Models'
type: article
url: http://arxiv.org/abs/2106.09685v2
year: '2021'

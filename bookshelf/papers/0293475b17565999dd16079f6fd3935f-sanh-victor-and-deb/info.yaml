abstract: As Transfer Learning from large-scale pre-trained models becomes more prevalent
  in Natural Language Processing (NLP), operating these large models in on-the-edge
  and/or under constrained computational training or inference budgets remains challenging.
  In this work, we propose a method to pre-train a smaller general-purpose language
  representation model, called DistilBERT, which can then be fine-tuned with good
  performances on a wide range of tasks like its larger counterparts. While most prior
  work investigated the use of distillation for building task-specific models, we
  leverage knowledge distillation during the pre-training phase and show that it is
  possible to reduce the size of a BERT model by 40%, while retaining 99% of its language
  understanding capabilities and being 60% faster. To leverage the inductive biases
  learned by larger models during pre-training, we introduce a triple loss combining
  language modeling, distillation and cosine-distance losses. Our smaller, faster
  and lighter model is cheaper to pre-train and we demonstrate its capabilities for
  on-device computations in a proof-of-concept experiment and a comparative on-device
  study.
archiveprefix: arXiv
author: Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas
author_list:
- family: Sanh
  given: Victor
- family: Debut
  given: Lysandre
- family: Chaumond
  given: Julien
- family: Wolf
  given: Thomas
eprint: 1910.01108v3
file: 1910.01108v3.pdf
files:
- sanh-victor-and-debut-lysandre-and-chaumond-julien-and-wolf-thomasdistilbert-a-distilled-version-of-bert-smaller-faster-cheaper-and-lighter2.pdf
month: Oct
primaryclass: cs.CL
ref: 1910.01108v3
title: 'DistilBERT, a distilled version of BERT: smaller, faster, cheaper and   lighter'
type: article
url: http://arxiv.org/abs/1910.01108v3
year: '2019'

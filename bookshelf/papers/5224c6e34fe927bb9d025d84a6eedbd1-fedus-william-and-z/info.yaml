abstract: In deep learning, models typically reuse the same parameters for all inputs.
  Mixture of Experts (MoE) defies this and instead selects different parameters for
  each incoming example. The result is a sparsely-activated model -- with outrageous
  numbers of parameters -- but a constant computational cost. However, despite several
  notable successes of MoE, widespread adoption has been hindered by complexity, communication
  costs and training instability -- we address these with the Switch Transformer.
  We simplify the MoE routing algorithm and design intuitive improved models with
  reduced communication and computational costs. Our proposed training techniques
  help wrangle the instabilities and we show large sparse models may be trained, for
  the first time, with lower precision (bfloat16) formats. We design models based
  off T5-Base and T5-Large to obtain up to 7x increases in pre-training speed with
  the same computational resources. These improvements extend into multilingual settings
  where we measure gains over the mT5-Base version across all 101 languages. Finally,
  we advance the current scale of language models by pre-training up to trillion parameter
  models on the "Colossal Clean Crawled Corpus" and achieve a 4x speedup over the
  T5-XXL model.
archiveprefix: arXiv
author: Fedus, William and Zoph, Barret and Shazeer, Noam
author_list:
- family: Fedus
  given: William
- family: Zoph
  given: Barret
- family: Shazeer
  given: Noam
eprint: 2101.03961v1
file: 2101.03961v1.pdf
files:
- fedus-william-and-zoph-barret-and-shazeer-noamswitch-transformers-scaling-to-trillion-parameter-models-with-simple-and-efficient-sparsity2021.pdf
month: Jan
primaryclass: cs.LG
ref: 2101.03961v1
time-added: 2021-01-20-10:59:10
title: 'Switch Transformers: Scaling to Trillion Parameter Models with Simple   and
  Efficient Sparsity'
type: article
url: http://arxiv.org/abs/2101.03961v1
year: '2021'

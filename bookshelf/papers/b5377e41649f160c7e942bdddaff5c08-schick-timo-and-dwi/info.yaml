abstract: Language models (LMs) exhibit remarkable abilities to solve new tasks from
  just a few examples or textual instructions, especially at scale. They also, paradoxically,
  struggle with basic functionality, such as arithmetic or factual lookup, where much
  simpler and smaller models excel. In this paper, we show that LMs can teach themselves
  to use external tools via simple APIs and achieve the best of both worlds. We introduce
  Toolformer, a model trained to decide which APIs to call, when to call them, what
  arguments to pass, and how to best incorporate the results into future token prediction.
  This is done in a self-supervised way, requiring nothing more than a handful of
  demonstrations for each API. We incorporate a range of tools, including a calculator,
  a Q\&A system, two different search engines, a translation system, and a calendar.
  Toolformer achieves substantially improved zero-shot performance across a variety
  of downstream tasks, often competitive with much larger models, without sacrificing
  its core language modeling abilities.
archiveprefix: arXiv
author: Schick, Timo and Dwivedi-Yu, Jane and Dessì, Roberto and Raileanu, Roberta
  and Lomeli, Maria and Zettlemoyer, Luke and Cancedda, Nicola and Scialom, Thomas
author_list:
- family: Schick
  given: Timo
- family: Dwivedi-Yu
  given: Jane
- family: Dessì
  given: Roberto
- family: Raileanu
  given: Roberta
- family: Lomeli
  given: Maria
- family: Zettlemoyer
  given: Luke
- family: Cancedda
  given: Nicola
- family: Scialom
  given: Thomas
eprint: 2302.04761v1
file: 2302.04761v1.pdf
files:
- schick-timo-and-dwivedi-yu-jane-and-dessi-roberto-and-raileanu-roberta-and-lomeli-maria-and-zettlemoyer-luke-and-cancedda-nicola-and-scialom-t.pdf
month: Feb
primaryclass: cs.CL
ref: 2302.04761v1
time-added: 2023-02-18-18:49:05
title: 'Toolformer: Language Models Can Teach Themselves to Use Tools'
type: article
url: http://arxiv.org/abs/2302.04761v1
year: '2023'

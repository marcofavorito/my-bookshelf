abstract: 'In this tutorial article, we aim to provide the reader with the conceptual
  tools needed to get started on research on offline reinforcement learning algorithms:
  reinforcement learning algorithms that utilize previously collected data, without
  additional online data collection. Offline reinforcement learning algorithms hold
  tremendous promise for making it possible to turn large datasets into powerful decision
  making engines. Effective offline reinforcement learning methods would be able to
  extract policies with the maximum possible utility out of the available data, thereby
  allowing automation of a wide range of decision-making domains, from healthcare
  and education to robotics. However, the limitations of current algorithms make this
  difficult. We will aim to provide the reader with an understanding of these challenges,
  particularly in the context of modern deep reinforcement learning methods, and describe
  some potential solutions that have been explored in recent work to mitigate these
  challenges, along with recent applications, and a discussion of perspectives on
  open problems in the field.'
archiveprefix: arXiv
author: Levine, Sergey and Kumar, Aviral and Tucker, George and Fu, Justin
author_list:
- family: Levine
  given: Sergey
- family: Kumar
  given: Aviral
- family: Tucker
  given: George
- family: Fu
  given: Justin
eprint: 2005.01643v3
file: 2005.01643v3.pdf
files:
- levine-sergey-and-kumar-aviral-and-tucker-george-and-fu-justinoffline-reinforcement-learning-tutorial-review-and-perspectives-on-open-problem.pdf
month: May
primaryclass: cs.LG
ref: 2005.01643v3
time-added: 2022-11-01-10:11:36
title: 'Offline Reinforcement Learning: Tutorial, Review, and Perspectives on   Open
  Problems'
type: article
url: http://arxiv.org/abs/2005.01643v3
year: '2020'

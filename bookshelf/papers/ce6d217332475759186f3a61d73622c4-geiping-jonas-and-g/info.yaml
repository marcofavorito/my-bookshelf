abstract: 'Recent trends in language modeling have focused on increasing performance
  through scaling, and have resulted in an environment where training language models
  is out of reach for most researchers and practitioners. While most in the community
  are asking how to push the limits of extreme computation, we ask the opposite question:
  How far can we get with a single GPU in just one day?   We investigate the downstream
  performance achievable with a transformer-based language model trained completely
  from scratch with masked language modeling for a single day on a single consumer
  GPU. Aside from re-analyzing nearly all components of the pretraining pipeline for
  this scenario and providing a modified pipeline with performance close to BERT,
  we investigate why scaling down is hard, and which modifications actually improve
  performance in this scenario. We provide evidence that even in this constrained
  setting, performance closely follows scaling laws observed in large-compute settings.
  Through the lens of scaling laws, we categorize a range of recent improvements to
  training and architecture and discuss their merit and practical applicability (or
  lack thereof) for the limited compute setting.'
archiveprefix: arXiv
author: Geiping, Jonas and Goldstein, Tom
author_list:
- family: Geiping
  given: Jonas
- family: Goldstein
  given: Tom
eprint: 2212.14034v1
file: 2212.14034v1.pdf
files:
- geiping-jonas-and-goldstein-tomcramming-training-a-language-model-on-a-single-gpu-in-one-day2022.pdf
month: Dec
primaryclass: cs.CL
ref: 2212.14034v1
time-added: 2023-01-03-14:14:18
title: 'Cramming: Training a Language Model on a Single GPU in One Day'
type: article
url: http://arxiv.org/abs/2212.14034v1
year: '2022'
